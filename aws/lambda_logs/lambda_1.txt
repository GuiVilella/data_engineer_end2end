{'data': {'data': {'video_id': {'0': 'rFwQDDbYTm4', '1': 'Nq3auVtvd9Q', '2': 'eyxmSmjmNS0', '3': 'yexR53My2O4', '4': 'GWt6Fu05voI', '5': 'rHQPBqMULXo', '6': 'Lg97gWXsiQ4', '7': 'U0mxx7AoNz0', '8': 'a4P8v8lGFPw', '9': 'Xp3jR-ttMfo', '10': 'w3knicSHx5s', '11': '1HEdXwEYrGM', '12': 'AJwnbSP_rq8', '13': 'vfBAUYpMCTU', '14': 'OUCwujwE7bA', '15': 'D6osiiEoV0w', '16': 'zcGOPqFZ4Tk', '17': 'qNfCVGbvnJc', '18': 'VQoyypYTz2U', '19': 'XHGh19Hbx48', '20': 'FNDVy_BR8aA', '21': 'C5sWbYwzKyg', '22': 'kl3aBni87jg', '23': '6dvcYx9hcbE', '24': 'MgJ3JsE3Tqo', '25': 'smxwT82o40Y', '26': 'Z3knUzwuIgo', '27': 'AvHLJqtmQkE', '28': '3ks2gpqAKY8', '29': 'z4lAlVRwbrc', '30': 'DdkenV-ZdJU', '31': 'C7mUYocWdG0', '32': 'ccBMRryxGog', '33': 'AIOE1l1W0Tw', '34': '16BsJI5I-Yw', '35': 'X4S8F3bwuuw', '36': '_7xpGve9QEE', '37': 'K-cXYoqHxBc', '38': 'gwI6g1pBD84', '39': 'qS-iYnp00uc', '40': 'af6WPqvzjjk', '41': 'YQ2QtKcK2dA', '42': 'WncUlZYpdq4', '43': 'dPsXxLyqpfs', '44': '_Z9ZP1eiKsI', '45': '-YiMVR3HEuY', '46': '56GW1IlWgMg', '47': 'agXIYMCICcc', '48': '_N_nFzMtWkA', '49': 'Qk4lJdp7ZAs', '50': '69IjNZaoeao', '51': 'kOy49NqZeqI', '52': 'BTLCdge7uSQ', '53': 'KXEEqcwXn8w', '54': 'We20YSAJZSE', '55': 'RrvC8YW0pT0', '56': 'EbFosdOi5SY', '57': 'U3zmekzQ8WQ', '58': 'awyuuJoHawo', '59': '8wkgDnNxiVs', '60': 'gbG1X8Xq-T8', '61': 'hg2Q_O5b9w4', '62': '-h1KB8ps11A', '63': 'pZyxlf6l0N8', '64': 'F5aaXrIMWyU', '65': 'PDRtyrVskMU', '66': 'to7vCdkLi4s', '67': 'tjbEVY5XIk0', '68': 'IiBFqnNu7A8', '69': 'Nfry2b4RFI4', '70': 'UjJU13GdL94', '71': 'HYEzHX6-fIA', '72': 'ml3Y1ljVSQ8', '73': 'rFwQDDbYTm4', '74': 'a4VvcmqnkhY', '75': 'v2GRWzIhaqQ', '76': '9-o2aAoN0rY', '77': 'vLTmnaMpQCs', '78': 'lFGnsdV-sR4', '79': 'O1b0cbgpRBw', '80': 'BhUWvQmLzSk', '81': 'o75ybZ-6Uu8', '82': 'kU-tWy_wr78', '83': '-buULmf7dec', '84': 'P38FZrbNHV4', '85': 'NJCLUzkn-sA', '86': 'U0mxx7AoNz0', '87': 'XHGh19Hbx48', '88': 'FNDVy_BR8aA', '89': '6dvcYx9hcbE', '90': 'NeGJAUSQEJI', '91': 'z4lAlVRwbrc', '92': 'povBDxUn1VQ', '93': '16BsJI5I-Yw', '94': 'Ru23eWAQ6_E', '95': 'X4S8F3bwuuw', '96': '3N3Bl5AA5QU', '97': 'ciNMc0Czmfc', '98': 'u1_qMdb0kYU', '99': '-9evrZnBorM', '100': '_PyusGsbBPY', '101': 'iDulhoQ2pro', '102': 'H5vpBCLo74U', '103': '-MCYbmU9kfg', '104': '69IjNZaoeao', '105': 'i4H0kjxrias', '106': 'tC01FRB0M7w', '107': 'p3sAF3gVMMA', '108': 'qeEO2GECQk0', '109': 'AU30czb4iQA', '110': '_8KNb5iqblE', '111': 'wTIPGoHLw_8', '112': 'cIUtRNhY6Rw', '113': 'G3pOvrKkFuk', '114': 'utuz7wBGjKM', '115': 'IIebBjbBevs', '116': 'SY5PvZrJhLE', '117': 'q7QP_lfqnQM', '118': 'nxEr4VNgYOE', '119': 'rl4nUngiR2k', '120': 'xTzFJIknh7E', '121': '-_2AF9Lhweo', '122': 'ZfDZRX3WiJg', '123': 'l12GXD0t_RE', '124': '1VdEw_mGjFk', '125': 'q6Kyvy1zLwQ', '126': 'hAooAOFRsYc', '127': 'yexR53My2O4', '128': 'WVPE62Gk3EM', '129': 'nv6oFDp6rNQ', '130': 'lj-LGrnh1oU', '131': 'vLTmnaMpQCs', '132': 'xJrKIPwVwGM', '133': 'NAJOZTNkhlI', '134': 'plK2WVdLTOY', '135': 'j4xgkjWlfL4', '136': 'T9XSU0pKX2E', '137': 'iAR8LkkMMIM', '138': 'zdb8MM94A5c', '139': 'm-zrcmRd7E4', '140': '_c6A33Fg5Ns', '141': '-Kgxv64aG3o', '142': '0JlB9gufTw8', '143': 'aX8phGhG8VQ', '144': 'kP-dXK9JEhY', '145': 'hgSGHusDx7M', '146': 'AJwnbSP_rq8', '147': 'OUCwujwE7bA', '148': 'qNfCVGbvnJc', '149': 's9UAOmyah1A', '150': '_EDr3ryrT_Y', '151': 'AvHLJqtmQkE', '152': 'gYxJEd3EUKs', '153': '3ks2gpqAKY8', '154': 'qlB0TPBQ7YY', '155': 'C7mUYocWdG0', '156': 'ccBMRryxGog', '157': 'Ru23eWAQ6_E', '158': 'X4S8F3bwuuw', '159': 'mIZLGBD99iU', '160': '_NMQyOu2HTo', '161': 'ZTs_mXwMCs8', '162': '0A8ljAkdFtg', '163': '64Izfm24FKA', '164': 'E5OnoYF2oAk', '165': '2zW33LfffPc', '166': '4Cclp6yPDuw', '167': 'ut5kp56wW_4', '168': 'x8pW19wKfXQ', '169': 'hMO6rbMAPew', '170': 'qeEO2GECQk0', '171': 'D-eg7k8YSfs', '172': 'plK2WVdLTOY', '173': 'k_hUdZJNzkU', '174': 'TFiZYA_JfJs', '175': 'l8JeokY5NsU', '176': 'ZAW9EyNo2fw', '177': 'rvr143crpuU', '178': 'ctCv_NRpqvM', '179': 'ZVVnvZdUMUk', '180': 'eYgPJ_7BkEw', '181': 'PZypP7PiKi0', '182': 'Ok44otx90D4', '183': 'D-eg7k8YSfs', '184': 'a0f07M2uj_A', '185': 'MpdbFLXOOIw', '186': 'fvctpYph8Pc', '187': 'jhCInVFE2sc', '188': 'k1GOF2jmX7c', '189': 'Cs_j-oNwGgg', '190': 'bFn2xcGi1TQ', '191': 'l5he9JNJqHA', '192': 'u5BkO8XMS2I', '193': 'a-VQfQqIMrE', '194': '3_qGrmD6iQY', '195': 'THcuTJbeD34', '196': 'O9kFX33nUcU', '197': 'cuyM63ugsxI', '198': 'Hdo81GtLC_4', '199': 'q7PjrmGNx5A', '200': '1VdEw_mGjFk', '201': 'z_3Qv4In2ac', '202': '3jT1qJ8ETzk', '203': 'Jqvb7jp4Nm8', '204': '5IRlUVrEVL8', '205': 'a6v92P0EbJc', '206': 'nv6oFDp6rNQ', '207': 'G2sr1g6rLdE', '208': 'EbHUU-gLyRA', '209': '3baFTP0uYOc', '210': 'DiNzQP7kK-s', '211': 'gch94ttuy5s', '212': 'LB4B5FYvtdI', '213': 'ahRPdiCop3E', '214': 'Z_kWZpgEZ7w', '215': 'Elxn8rS88bI', '216': '8Oy7o3Yu-Xo', '217': 'nQDZmf2Yb9k', '218': 'dND-7llwrpw', '219': 'EeMhj0sPrhE', '220': 'vVRC-0VKPrg', '221': 'W2UT8NjUqrk', '222': '1HEdXwEYrGM', '223': 'i-J4T3uLC9M', '224': 'MgJ3JsE3Tqo', '225': 'O_dJ31T01i8', '226': 'smxwT82o40Y', '227': 'AIOE1l1W0Tw', '228': 'jSdHmImyUjk', '229': '0PAiQ1jTN5k', '230': '3N3Bl5AA5QU', '231': '_okxGdHM5b8', '232': 'jltgNGt8Lpg', '233': 'u1_qMdb0kYU', '234': '-9evrZnBorM', '235': 'nPB0ppcnzZA', '236': 'iDulhoQ2pro', '237': 'agXIYMCICcc', '238': 'hMO6rbMAPew', '239': 'H5vpBCLo74U', '240': '1L83tM8nwHU', '241': 'nXGHJTtFYRU', '242': 'MIEA8azwu1k', '243': 'BTLCdge7uSQ', '244': 'We20YSAJZSE', '245': 'RrvC8YW0pT0', '246': 'i4H0kjxrias', '247': 'eCH0M4wzKJs', '248': 'Cs_j-oNwGgg', '249': 'T35ba_VXkMY', '250': 'Uumd2zOOz60', '251': '7DGlElSVYGo', '252': 'j4xgkjWlfL4', '253': 'hHZSA9z_abE', '254': 'cllFzkvrYmE', '255': 'Rk3MBx20z24', '256': 'rHQPBqMULXo', '257': '7OdhtAiPfWY', '258': 'hIoCn_9QTVU', '259': 'rR5_emVeyBk', '260': 'vxdcX0JTEr0', '261': 'Pm93D8CVlY8', '262': '2ethDz9KnLk', '263': '3N3Bl5AA5QU', '264': '0A8ljAkdFtg', '265': 'efPrtcLdcdM', '266': 'mIZLGBD99iU', '267': '64Izfm24FKA', '268': 'Hi6cbeBY2oQ', '269': 'ddG2fM9i4Kk', '270': 'jltgNGt8Lpg', '271': 'sbKaUc0tPaY', '272': 'OioFONrSETc', '273': '_PyusGsbBPY', '274': 'WYrvh50yu6s', '275': 'iDulhoQ2pro', '276': '1L83tM8nwHU', '277': 'H6Qiegq_36c', '278': 'wZWn7Hm8osA', '279': 'nXGHJTtFYRU', '280': 'Xc9Rkbg6IZA', '281': 'i4H0kjxrias', '282': '9Kec_7WFyp0', '283': 'p3sAF3gVMMA', '284': 'lmAj0SU_bW0', '285': '8wkgDnNxiVs', '286': 'klPuEHCKG9M', '287': 'ZVVnvZdUMUk', '288': 'eYgPJ_7BkEw', '289': 'Ok44otx90D4', '290': '_8KNb5iqblE', '291': '1aO-uHXbzmQ', '292': 'k1GOF2jmX7c', '293': 'l_3zj6HeWUE', '294': 'p-zOeQCoG9c', '295': 'T35ba_VXkMY', '296': 'q7QP_lfqnQM', '297': 'hQEnzdLkPj4', '298': '4GKCxJQSw-g', '299': 'WTB2p4bqtXU', '300': '-_2AF9Lhweo', '301': 'ZfDZRX3WiJg', '302': '8l-TDqpoUQs', '303': 'DLq1DUcMh1Q', '304': 'sEG8hD64c_Q', '305': 'Q5g3p9Zwjrk', '306': 'qSArFEIoSbo', '307': 'LMb5tvW-UoQ', '308': 'Hdo81GtLC_4', '309': 'eI8xTdcZ6VY', '310': 'V79rRI05Lj4', '311': 'DYBmD88vpiA', '312': '1VdEw_mGjFk', '313': 'qFRfnIRMNlk', '314': 'hAooAOFRsYc', '315': '3jT1qJ8ETzk', '316': 'Jqvb7jp4Nm8', '317': 'x6T1zMSE4Ts', '318': 'v-ZxzTSpmk4', '319': 'Nq3auVtvd9Q', '320': 'eyxmSmjmNS0', '321': 'GWt6Fu05voI', '322': 'a6v92P0EbJc', '323': 'WVPE62Gk3EM', '324': 'nv6oFDp6rNQ', '325': 'hv3UO3G0Ofo', '326': 'TrdevFK_am4', '327': '3qxJ2WD8p4w', '328': 'xJrKIPwVwGM', '329': 'IaS72aHrJKE', '330': 'iAR8LkkMMIM', '331': 'zdb8MM94A5c', '332': 'rNkHjZtH0RQ', '333': 'R5DiLFOMZrc', '334': 'RSSVWpBak6s', '335': 'cllFzkvrYmE', '336': 'P_xeshTnPZg', '337': '7K4Z8RqjWIk', '338': 'pH2jZun8MoY', '339': '2PYLNHqxd5A', '340': 'qgUegkefocg', '341': '2h4tRsQzipQ', '342': 'hgSGHusDx7M', '343': 'InhMx1h0N40', '344': 'Xp3jR-ttMfo', '345': 'w3knicSHx5s', '346': 'D6osiiEoV0w', '347': 'qNfCVGbvnJc', '348': 'X2k7n4FuI7c', '349': 'igBp5vUrc7o', '350': 'Z3knUzwuIgo', '351': 'ccBMRryxGog', '352': 'x8pW19wKfXQ', '353': 'RrBapqCPnmE', '354': 'G3pOvrKkFuk', '355': 'fvctpYph8Pc', '356': '1aO-uHXbzmQ', '357': 'cIUtRNhY6Rw', '358': 'to7vCdkLi4s', '359': 'tjbEVY5XIk0', '360': 'k1GOF2jmX7c', '361': 'Cs_j-oNwGgg', '362': 'l_3zj6HeWUE', '363': 'bFn2xcGi1TQ', '364': 'p-zOeQCoG9c', '365': 'l5he9JNJqHA', '366': 'u5BkO8XMS2I', '367': 'a-VQfQqIMrE', '368': 'IiBFqnNu7A8', '369': 'YrO1v7-KcXs', '370': 'Nfry2b4RFI4', '371': 'IIebBjbBevs', '372': 'UjJU13GdL94', '373': 'T35ba_VXkMY', '374': 'SY5PvZrJhLE', '375': 'q7QP_lfqnQM', '376': 'HYEzHX6-fIA', '377': '3_qGrmD6iQY', '378': 'hQEnzdLkPj4', '379': 'CA8JPbJ75tY', '380': '4GKCxJQSw-g', '381': 'rl4nUngiR2k', '382': 'xTzFJIknh7E', '383': 'WTB2p4bqtXU', '384': '-_2AF9Lhweo', '385': 'ZfDZRX3WiJg', '386': 'THcuTJbeD34', '387': 'O9kFX33nUcU', '388': 'cuyM63ugsxI', '389': 'ml3Y1ljVSQ8', '390': 'l12GXD0t_RE', '391': '8l-TDqpoUQs', '392': 'DLq1DUcMh1Q', '393': 'sEG8hD64c_Q', '394': 'YPfUiOMYOEE', '395': 'YBlNQK0Ao6g', '396': '2lkUNDZld-4', '397': 'Q5g3p9Zwjrk', '398': 'qSArFEIoSbo', '399': 'Uumd2zOOz60', '400': 'LMb5tvW-UoQ', '401': 'Hdo81GtLC_4', '402': 'eI8xTdcZ6VY', '403': 'V79rRI05Lj4', '404': 'q7PjrmGNx5A', '405': 'nxEr4VNgYOE', '406': 'DYBmD88vpiA', '407': '1VdEw_mGjFk', '408': 'q6Kyvy1zLwQ', '409': 'qFRfnIRMNlk', '410': 'hAooAOFRsYc', '411': '3jT1qJ8ETzk', '412': 'Jqvb7jp4Nm8', '413': 'x6T1zMSE4Ts', '414': 'v-ZxzTSpmk4', '415': '5IRlUVrEVL8', '416': 'rFwQDDbYTm4', '417': 'Nq3auVtvd9Q', '418': 'eyxmSmjmNS0', '419': 'yexR53My2O4', '420': 'GWt6Fu05voI', '421': 'a6v92P0EbJc', '422': 'a4VvcmqnkhY', '423': 'nv6oFDp6rNQ', '424': 'v2GRWzIhaqQ', '425': 'lj-LGrnh1oU', '426': '9-o2aAoN0rY', '427': 'G2sr1g6rLdE', '428': 'hv3UO3G0Ofo', '429': 'EbHUU-gLyRA', '430': 'vLTmnaMpQCs', '431': 'MQ89be_685o', '432': '3baFTP0uYOc', '433': 'TrdevFK_am4', '434': 'DiNzQP7kK-s', '435': '3qxJ2WD8p4w', '436': 'xJrKIPwVwGM', '437': 'NAJOZTNkhlI', '438': 'gch94ttuy5s', '439': 'IaS72aHrJKE', '440': 'LB4B5FYvtdI', '441': 'B9PL__gVxLI', '442': 'BhUWvQmLzSk', '443': 'plK2WVdLTOY', '444': 'j4xgkjWlfL4', '445': 'T9XSU0pKX2E', '446': 'iAR8LkkMMIM', '447': 'yFAuXmcGk2Y', '448': 'zdb8MM94A5c', '449': 'ahRPdiCop3E', '450': 'm-zrcmRd7E4', '451': 'rNkHjZtH0RQ', '452': 'R5DiLFOMZrc', '453': 'o75ybZ-6Uu8', '454': '_c6A33Fg5Ns', '455': 'RSSVWpBak6s', '456': 'cllFzkvrYmE', '457': 'Z_kWZpgEZ7w', '458': 'Ag1bw8MfHGQ', '459': 'Elxn8rS88bI', '460': 'VCUDo9umKEQ', '461': 'P_xeshTnPZg', '462': 'qtu0aSTDE2I', '463': 'CRlN-cYFxTk', '464': 'uwfVxckuq50', '465': 'h3ij3F3cPIk', '466': 'pH2jZun8MoY', '467': 'W-O7AZNzbzQ', '468': '2PYLNHqxd5A', '469': 'kU-tWy_wr78', '470': 'dmH1ZpcROMk', '471': '-buULmf7dec', '472': '8Oy7o3Yu-Xo', '473': 'P38FZrbNHV4', '474': 'g08NkNWmZTA', '475': 'k_hUdZJNzkU', '476': 'z15JLtAuwVI', '477': 'nQDZmf2Yb9k', '478': 'qgUegkefocg', '479': '-Kgxv64aG3o', '480': '0JlB9gufTw8', '481': 'pBau7umFhjQ', '482': 'aX8phGhG8VQ', '483': '19Q-vMd9bYg', '484': 'wTzvKB6D_34', '485': 'dND-7llwrpw', '486': 'kP-dXK9JEhY', '487': 'NJCLUzkn-sA', '488': '2h4tRsQzipQ', '489': 'EeMhj0sPrhE', '490': 'vVRC-0VKPrg', '491': 'W2UT8NjUqrk', '492': 'hgSGHusDx7M', '493': 'InhMx1h0N40', '494': 'Lg97gWXsiQ4', '495': 'gwI6g1pBD84', '496': 'U0mxx7AoNz0', '497': 'a4P8v8lGFPw', '498': 'Xp3jR-ttMfo', '499': 'w3knicSHx5s', '500': '1HEdXwEYrGM', '501': 'vfBAUYpMCTU', '502': 'OUCwujwE7bA', '503': 'D6osiiEoV0w', '504': 'zcGOPqFZ4Tk', '505': 'qNfCVGbvnJc', '506': 'XHGh19Hbx48', '507': 's9UAOmyah1A', '508': 'lvYVuOmUVs8', '509': '6dvcYx9hcbE', '510': 'i-J4T3uLC9M', '511': 'MgJ3JsE3Tqo', '512': 'O_dJ31T01i8', '513': 'X2k7n4FuI7c', '514': '_EDr3ryrT_Y', '515': 'gYxJEd3EUKs', '516': 'NeGJAUSQEJI', '517': 'qlB0TPBQ7YY', '518': 'povBDxUn1VQ', '519': 'Ru23eWAQ6_E', '520': 'qS-iYnp00uc', '521': 'oz5yZc9ULAc', '522': 'jSdHmImyUjk', '523': '3N3Bl5AA5QU', '524': '_okxGdHM5b8', '525': '_NMQyOu2HTo', '526': 'ZTs_mXwMCs8', '527': 'ciNMc0Czmfc', '528': 'E5OnoYF2oAk', '529': '2zW33LfffPc', '530': '4Cclp6yPDuw', '531': 'ut5kp56wW_4', '532': 'x8pW19wKfXQ', '533': 'WncUlZYpdq4', '534': '1waHlpKiNyY', '535': 'SjQyLhQIXSM', '536': 'C1N_PDHuJ6Q', '537': '6g0t3Phly2M', '538': 'NyG-7nRpsW8', '539': 'D8PJAL-MZv8', '540': 'ARq74QuavAo', '541': 'BOCLq2gpcGU', '542': 'FDCfw-YqWTE', '543': 'qhXZsFVxGKo', '544': 's2coXdufOzE', '545': 'y1xoI7mBtOc', '546': 'QrzApibhohY', '547': '4Ct3Yujl1dk', '548': '4qJaSmvhxi8', '549': '-_4Zi8fCZO4', '550': 'lAq96T8FkTw', '551': 'NxTFlzBjS-4', '552': 'lWzo8CajF5s', '553': 'k8fTYJPd3_I', '554': '_e-LFe_igno', '555': 'JXQT_vxqwIs', '556': 'QzulmoOg2JE', '557': 'AXDByU3D1hA', '558': 'cSoK_6Rkbfg', '559': 'wKkcBPp3F1Y', '560': 'tNIpEZLv_eg', '561': 'em6dfRxYkYU', '562': 'nUUqwaxLnWs', '563': '5qefnAek8OA', '564': 'LLux1SW--oM', '565': 'ueO_Ph0Pyqk', '566': 'fODpu1-lNTw', '567': 'S9ElPZupUsE', '568': 'CS4cs9xVecg', '569': 'n1l-9lIMW7E', '570': 'BYGpKPY9pO0', '571': 'xflCLdJh0n0', '572': 'ysnIDax71yY', '573': '7AZjh2VXD6E', '574': 'eqEc66RFY0I', '575': 'hjrYrynGWGA', '576': 'SHEPb1JHw5o', '577': 'uJryes5Vk1o', '578': 'GzphoJOVEcE', '579': '5H7M5Vd3-pk', '580': 'hCP1vGoCdYU', '581': 'nJyUyKN-XBQ', '582': 'z_xiwjEdAC4', '583': 'KKfZLXcF-aE', '584': 'qsIrQi0fzbY', '585': 'pYWASRauTzs', '586': 'okpqeEUdEkY', '587': '2BkqApHKwn0', '588': 'tKcLaGdvabM', '589': 'V2QlTmh6P2Y', '590': '0S9c7nHoDws', '591': 'k_S5fnKjO-4', '592': 'fXOsFF95ifk', '593': 'CcRkHl75Z-Y', '594': 'rMOdrD61IoU', '595': 'xy5MOQpx3aQ', '596': 'kkWRbIb42Ms', '597': 'Xvg00QnyaIY', '598': 'NkOv_k7r6no', '599': 'P7_jFxTtJEo', '600': '7bLEWDZng_M', '601': 'yXcQ4B-YSjQ', '602': '6by6Xas_Kho', '603': '2gw5tE2ziqA', '604': 'a8i2eJin0lY', '605': 'yslMo3hSbqE', '606': '5dWp1mw_XNk', '607': 'B7-iPbddhsw', '608': 'qzPQ8cEsVK8', '609': 'VTE2KlfoO3Q', '610': '2zgon7XfN4I', '611': '1waHlpKiNyY', '612': 'SjQyLhQIXSM', '613': 'C1N_PDHuJ6Q', '614': '6g0t3Phly2M', '615': 'NyG-7nRpsW8', '616': 'D8PJAL-MZv8', '617': 'ARq74QuavAo', '618': 'BOCLq2gpcGU', '619': 'FDCfw-YqWTE', '620': 'qhXZsFVxGKo', '621': 's2coXdufOzE', '622': 'y1xoI7mBtOc', '623': 'QrzApibhohY', '624': '4Ct3Yujl1dk', '625': '4qJaSmvhxi8', '626': '-_4Zi8fCZO4', '627': 'lAq96T8FkTw', '628': 'NxTFlzBjS-4', '629': 'lWzo8CajF5s', '630': 'k8fTYJPd3_I', '631': '_e-LFe_igno', '632': 'JXQT_vxqwIs', '633': 'QzulmoOg2JE', '634': 'AXDByU3D1hA', '635': 'cSoK_6Rkbfg', '636': 'wKkcBPp3F1Y', '637': 'tNIpEZLv_eg', '638': 'em6dfRxYkYU', '639': 'nUUqwaxLnWs', '640': '5qefnAek8OA', '641': 'LLux1SW--oM', '642': 'ueO_Ph0Pyqk', '643': 'fODpu1-lNTw', '644': 'S9ElPZupUsE', '645': 'dFX8k1kXhOw', '646': 'UEtvV1D6B3s', '647': 'sofffBNhVSo', '648': 'BH9mlmdXzzI', '649': 'M3qpIzy4MQk', '650': '_Fe5kKmFieg', '651': 'DFUqMbWs5d8', '652': 'J3HHOwcrkK8', '653': 'CZf3oo0fuh0', '654': 'NUmbgp1h64E', '655': 'dM0exrbVZ08', '656': 'zg26t-BH7ao', '657': 'JoAxZsdw_3w', '658': 'jyjJ-RpQ5zQ', '659': 'HfM8UIohGE0', '660': 'sfk5h0yC67o', '661': '2BH49JG_sTs', '662': 'sn_QSB7T1xo', '663': 'yofjFQddwHE', '664': 'UdXfsAr4Gjw', '665': 'ImUoubi_t7s', '666': 'l_-CUyEx_x4', '667': 'OT91E6_Qm1A', '668': 'dwFcodBz_2I', '669': 'dqwx-F7Eits', '670': 'oJFShOfCZiA', '671': 'LpAiPYNnxW0', '672': 'xxu4IqwKw0w', '673': 'JS12eb1cTLE', '674': 'eMh5YqKopjE', '675': 'y7RfAwltHTw', '676': '-eyhCTvrEtE', '677': '6joSL0CUNtA', '678': 'LWI3b5GtwVc', '679': 'wlQvPJHxfOE', '680': 'j2nGxw8sKYU', '681': '69dr4090Y-Q', '682': 'NKpuX_yzdYs', '683': '0jspaMLxBig', '684': 'qvT3NyaycoA', '685': 'cI6WLiujJDY', '686': '7VHsWChH27g', '687': 'nbfJ23FxvuI', '688': 'y2v1-u6t5eQ', '689': 'R2vIYVdMzPc', '690': '_YcAEBnbtPQ', '691': 'zGFKSQlef_0', '692': '6dQgPFaZMGQ', '693': 'oJ2FL5BsFuQ', '694': 'dvnsbOtAmYI', '695': '_i3aqgKVNQI', '696': 'Er2ucMxjdHE', '697': 'DejHQYAGb7Q', '698': 'SysgYptB198', '699': 'quoGRI-1l0A', '700': 'vm2SI8AJY0s', '701': 'ArPaAX_PhIs', '702': 'XuD4C8vJzEQ', '703': 'am36dePheDc', '704': 'smHa2442Ah4', '705': 'tQYZaDn_kSg', '706': 'KTB_OFoAQcc', '707': 'jPOAS7uCODQ', '708': '3PyJA9AfwSk', '709': '8oOgPUO-TBY', '710': 'bXJx7y51cl0', '711': 'ay3zYUeuyhU', '712': '-bvTzZCEOdM', '713': 'dZVkygnKh1M', '714': 'ZILIbUvp5lk', '715': 'RYth6EbBUqM', '716': 'c1RBQzKsDCk', '717': 'C86ZXvgpejM', '718': 'KfV8CJh7hE0', '719': 'cFFu__mcoIw', '720': 'FQM13HkEfBk', '721': 'JI8saFjK84o', '722': 'c3zw6KI6dLc', '723': 'GSwYGkTfOKk', '724': 'rRB9iymNy1w', '725': '5e5pjeojznk', '726': 'XdsmlBGOK-k', '727': 'ANIzQ5G-XPE', '728': 'VAo84c1hQX8', '729': 'RTlwl2bv0Tg', '730': '9s_FpMpdYW8', '731': '6ykvU9WuIws', '732': '-FfMVnwXrZ0', '733': '96b_weTZb2w', '734': '6jfw8MuKwpI', '735': 'd2XB5-tuCWU', '736': '0NSLgoEtdnw', '737': 'R39tWYYKNcI', '738': 'ChoV5h7tw5A', '739': 'xY-DMAJpIP4', '740': 'b1I5X3UfEYI', '741': 'QgkLfjfGul8', '742': 'Cn8AtS-9Nwc', '743': 'H343JRrncfc', '744': 'DffGdrfY9gI', '745': 'PiF2Aln-L3w', '746': 'KGI7K_ehHsU', '747': 'NgWujOrCZFo', '748': 'e69ZWbbsGng', '749': 'YJsRD_hU4tc', '750': '1zhmudvZAs4', '751': 'UyEtTyeahus', '752': 'ErNp43wcudY', '753': 'hq_XyP9y0xg', '754': '79UqdjnPEZ0', '755': 'lHXd2hBnlJk', '756': 'oBB47VrQucA', '757': 'fiDmWKh_WeQ', '758': 'O5mqR4EFBQk', '759': '8Covj8F-NNc', '760': 'quEHyoA94rw', '761': 'BdZ6bjcixhk', '762': 'BlxnbyvHTyI', '763': 'o4je1lSpyaw', '764': 'k3UYUmp3Bi4', '765': 'uot5sbPz1NQ', '766': 'foCIxwn7VpI', '767': 'O9ZrPXPLmWg', '768': 'DTd7TyY7a-0', '769': 'A2bnWAIpLIo', '770': 'qOEeK1SNF3k', '771': '0aDhjrs8FMw', '772': 'mzv1mkJRA10', '773': 's5qFpEPNXEY', '774': 'f5sN3xAEAWQ', '775': 'a-oCxdzFapE', '776': 'eW546hpa744', '777': 'Ny970B12IQk', '778': 'qt9tXjtlQt4', '779': 'gz-44N3MMOA', '780': 'hbqxEJisBHo', '781': 'mFD5hUZubTI', '782': 'UEMMOdFbT94', '783': '43CZ0HjIC7U', '784': 'opWrnW5v25w', '785': 'CEBwVqRdKWc', '786': '9p7WWapTrpA'}, 'channelTitle': {'0': 'Yannic Kilcher', '1': 'Yannic Kilcher', '2': 'Yannic Kilcher', '3': 'Yannic Kilcher', '4': 'Yannic Kilcher', '5': 'Yannic Kilcher', '6': 'Yannic Kilcher', '7': 'Yannic Kilcher', '8': 'Yannic Kilcher', '9': 'Yannic Kilcher', '10': 'Yannic Kilcher', '11': 'Yannic Kilcher', '12': 'Yannic Kilcher', '13': 'Yannic Kilcher', '14': 'Yannic Kilcher', '15': 'Yannic Kilcher', '16': 'Yannic Kilcher', '17': 'Yannic Kilcher', '18': 'Yannic Kilcher', '19': 'Yannic Kilcher', '20': 'Yannic Kilcher', '21': 'Yannic Kilcher', '22': 'Yannic Kilcher', '23': 'Yannic Kilcher', '24': 'Yannic Kilcher', '25': 'Yannic Kilcher', '26': 'Yannic Kilcher', '27': 'Yannic Kilcher', '28': 'Yannic Kilcher', '29': 'Yannic Kilcher', '30': 'Yannic Kilcher', '31': 'Yannic Kilcher', '32': 'Yannic Kilcher', '33': 'Yannic Kilcher', '34': 'Yannic Kilcher', '35': 'Yannic Kilcher', '36': 'Yannic Kilcher', '37': 'Yannic Kilcher', '38': 'Yannic Kilcher', '39': 'Yannic Kilcher', '40': 'Yannic Kilcher', '41': 'Yannic Kilcher', '42': 'Yannic Kilcher', '43': 'Yannic Kilcher', '44': 'Yannic Kilcher', '45': 'Yannic Kilcher', '46': 'Yannic Kilcher', '47': 'Yannic Kilcher', '48': 'Yannic Kilcher', '49': 'Yannic Kilcher', '50': 'Yannic Kilcher', '51': 'Yannic Kilcher', '52': 'Yannic Kilcher', '53': 'Yannic Kilcher', '54': 'Yannic Kilcher', '55': 'Yannic Kilcher', '56': 'Yannic Kilcher', '57': 'Yannic Kilcher', '58': 'Yannic Kilcher', '59': 'Yannic Kilcher', '60': 'Yannic Kilcher', '61': 'Yannic Kilcher', '62': 'Yannic Kilcher', '63': 'Yannic Kilcher', '64': 'Yannic Kilcher', '65': 'Yannic Kilcher', '66': 'Yannic Kilcher', '67': 'Yannic Kilcher', '68': 'Yannic Kilcher', '69': 'Yannic Kilcher', '70': 'Yannic Kilcher', '71': 'Yannic Kilcher', '72': 'Yannic Kilcher', '73': 'Yannic Kilcher', '74': 'Yannic Kilcher', '75': 'Yannic Kilcher', '76': 'Yannic Kilcher', '77': 'Yannic Kilcher', '78': 'Flat Sabbath', '79': 'Yannic Kilcher', '80': 'Yannic Kilcher', '81': 'Yannic Kilcher', '82': 'Yannic Kilcher', '83': 'Yannic Kilcher', '84': 'Yannic Kilcher', '85': 'Yannic Kilcher', '86': 'Yannic Kilcher', '87': 'Yannic Kilcher', '88': 'Yannic Kilcher', '89': 'Yannic Kilcher', '90': 'Yannic Kilcher', '91': 'Yannic Kilcher', '92': 'Yannic Kilcher', '93': 'Yannic Kilcher', '94': 'Yannic Kilcher', '95': 'Yannic Kilcher', '96': 'Yannic Kilcher', '97': 'Yannic Kilcher', '98': 'Yannic Kilcher', '99': 'Yannic Kilcher', '100': 'Yannic Kilcher', '101': 'Yannic Kilcher', '102': 'Yannic Kilcher', '103': 'Yannic Kilcher', '104': 'Yannic Kilcher', '105': 'Yannic Kilcher', '106': 'Yannic Kilcher', '107': 'Yannic Kilcher', '108': 'Yannic Kilcher', '109': 'Yannic Kilcher', '110': 'Yannic Kilcher', '111': 'Yannic Kilcher', '112': 'Yannic Kilcher', '113': 'Yannic Kilcher', '114': 'Yannic Kilcher', '115': 'Yannic Kilcher', '116': 'Yannic Kilcher', '117': 'Yannic Kilcher', '118': 'Yannic Kilcher', '119': 'Yannic Kilcher', '120': 'Yannic Kilcher', '121': 'Yannic Kilcher', '122': 'Yannic Kilcher', '123': 'Yannic Kilcher', '124': 'Yannic Kilcher', '125': 'Yannic Kilcher', '126': 'Yannic Kilcher', '127': 'Yannic Kilcher', '128': 'Yannic Kilcher', '129': 'Yannic Kilcher', '130': 'Yannic Kilcher', '131': 'Yannic Kilcher', '132': 'Yannic Kilcher', '133': 'Yannic Kilcher', '134': 'Yannic Kilcher', '135': 'Yannic Kilcher', '136': 'Yannic Kilcher', '137': 'Yannic Kilcher', '138': 'Yannic Kilcher', '139': 'Yannic Kilcher', '140': 'Yannic Kilcher', '141': 'Yannic Kilcher', '142': 'Yannic Kilcher', '143': 'Yannic Kilcher', '144': 'Yannic Kilcher', '145': 'Yannic Kilcher', '146': 'Yannic Kilcher', '147': 'Yannic Kilcher', '148': 'Yannic Kilcher', '149': 'Yannic Kilcher', '150': 'Yannic Kilcher', '151': 'Yannic Kilcher', '152': 'Yannic Kilcher', '153': 'Yannic Kilcher', '154': 'Yannic Kilcher', '155': 'Yannic Kilcher', '156': 'Yannic Kilcher', '157': 'Yannic Kilcher', '158': 'Yannic Kilcher', '159': 'Yannic Kilcher', '160': 'Yannic Kilcher', '161': 'Yannic Kilcher', '162': 'Yannic Kilcher', '163': 'Yannic Kilcher', '164': 'Yannic Kilcher', '165': 'Yannic Kilcher', '166': 'Yannic Kilcher', '167': 'Yannic Kilcher', '168': 'Yannic Kilcher', '169': 'Yannic Kilcher', '170': 'Yannic Kilcher', '171': 'Yannic Kilcher', '172': 'Yannic Kilcher', '173': 'Yannic Kilcher', '174': 'Yannic Kilcher', '175': 'Yannic Kilcher', '176': 'Yannic Kilcher', '177': 'Yannic Kilcher', '178': 'Yannic Kilcher', '179': 'Yannic Kilcher', '180': 'Yannic Kilcher', '181': 'Yannic Kilcher', '182': 'Yannic Kilcher', '183': 'Yannic Kilcher', '184': 'Yannic Kilcher', '185': 'Yannic Kilcher', '186': 'Yannic Kilcher', '187': 'Yannic Kilcher', '188': 'Yannic Kilcher', '189': 'Yannic Kilcher', '190': 'Yannic Kilcher', '191': 'Yannic Kilcher', '192': 'Yannic Kilcher', '193': 'Yannic Kilcher', '194': 'Yannic Kilcher', '195': 'Yannic Kilcher', '196': 'Yannic Kilcher', '197': 'Yannic Kilcher', '198': 'Yannic Kilcher', '199': 'Yannic Kilcher', '200': 'Yannic Kilcher', '201': 'Yannic Kilcher', '202': 'Yannic Kilcher', '203': 'Yannic Kilcher', '204': 'Yannic Kilcher', '205': 'Yannic Kilcher', '206': 'Yannic Kilcher', '207': 'Yannic Kilcher', '208': 'Yannic Kilcher', '209': 'Yannic Kilcher', '210': 'Yannic Kilcher', '211': 'Yannic Kilcher', '212': 'Yannic Kilcher', '213': 'Yannic Kilcher', '214': 'Yannic Kilcher', '215': 'Yannic Kilcher', '216': 'Yannic Kilcher', '217': 'Yannic Kilcher', '218': 'Yannic Kilcher', '219': 'Yannic Kilcher', '220': 'Yannic Kilcher', '221': 'Yannic Kilcher', '222': 'Yannic Kilcher', '223': 'Yannic Kilcher', '224': 'Yannic Kilcher', '225': 'Yannic Kilcher', '226': 'Yannic Kilcher', '227': 'Yannic Kilcher', '228': 'Yannic Kilcher', '229': 'Yannic Kilcher', '230': 'Yannic Kilcher', '231': 'Yannic Kilcher', '232': 'Yannic Kilcher', '233': 'Yannic Kilcher', '234': 'Yannic Kilcher', '235': 'Yannic Kilcher', '236': 'Yannic Kilcher', '237': 'Yannic Kilcher', '238': 'Yannic Kilcher', '239': 'Yannic Kilcher', '240': 'Yannic Kilcher', '241': 'Yannic Kilcher', '242': 'Yannic Kilcher', '243': 'Yannic Kilcher', '244': 'Yannic Kilcher', '245': 'Yannic Kilcher', '246': 'Yannic Kilcher', '247': 'Yannic Kilcher', '248': 'Yannic Kilcher', '249': 'Yannic Kilcher', '250': 'Yannic Kilcher', '251': 'Yannic Kilcher', '252': 'Yannic Kilcher', '253': 'Yannic Kilcher', '254': 'Yannic Kilcher', '255': 'Yannic Kilcher', '256': 'Yannic Kilcher', '257': 'Yannic Kilcher', '258': 'Yannic Kilcher', '259': 'Yannic Kilcher', '260': 'Yannic Kilcher', '261': 'Yannic Kilcher', '262': 'Yannic Kilcher', '263': 'Yannic Kilcher', '264': 'Yannic Kilcher', '265': 'Yannic Kilcher', '266': 'Yannic Kilcher', '267': 'Yannic Kilcher', '268': 'Yannic Kilcher', '269': 'Yannic Kilcher', '270': 'Yannic Kilcher', '271': 'Yannic Kilcher', '272': 'Yannic Kilcher', '273': 'Yannic Kilcher', '274': 'Yannic Kilcher', '275': 'Yannic Kilcher', '276': 'Yannic Kilcher', '277': 'Yannic Kilcher', '278': 'Yannic Kilcher', '279': 'Yannic Kilcher', '280': 'Yannic Kilcher', '281': 'Yannic Kilcher', '282': 'Yannic Kilcher', '283': 'Yannic Kilcher', '284': 'Yannic Kilcher', '285': 'Yannic Kilcher', '286': 'Yannic Kilcher', '287': 'Yannic Kilcher', '288': 'Yannic Kilcher', '289': 'Yannic Kilcher', '290': 'Yannic Kilcher', '291': 'Yannic Kilcher', '292': 'Yannic Kilcher', '293': 'Yannic Kilcher', '294': 'Yannic Kilcher', '295': 'Yannic Kilcher', '296': 'Yannic Kilcher', '297': 'Yannic Kilcher', '298': 'Yannic Kilcher', '299': 'Yannic Kilcher', '300': 'Yannic Kilcher', '301': 'Yannic Kilcher', '302': 'Yannic Kilcher', '303': 'Yannic Kilcher', '304': 'Yannic Kilcher', '305': 'Yannic Kilcher', '306': 'Yannic Kilcher', '307': 'Yannic Kilcher', '308': 'Yannic Kilcher', '309': 'Yannic Kilcher', '310': 'Yannic Kilcher', '311': 'Yannic Kilcher', '312': 'Yannic Kilcher', '313': 'Yannic Kilcher', '314': 'Yannic Kilcher', '315': 'Yannic Kilcher', '316': 'Yannic Kilcher', '317': 'Yannic Kilcher', '318': 'Yannic Kilcher', '319': 'Yannic Kilcher', '320': 'Yannic Kilcher', '321': 'Yannic Kilcher', '322': 'Yannic Kilcher', '323': 'Yannic Kilcher', '324': 'Yannic Kilcher', '325': 'Yannic Kilcher', '326': 'Yannic Kilcher', '327': 'Yannic Kilcher', '328': 'Yannic Kilcher', '329': 'Yannic Kilcher', '330': 'Yannic Kilcher', '331': 'Yannic Kilcher', '332': 'Yannic Kilcher', '333': 'Yannic Kilcher', '334': 'Yannic Kilcher', '335': 'Yannic Kilcher', '336': 'Yannic Kilcher', '337': 'Yannic Kilcher', '338': 'Yannic Kilcher', '339': 'Yannic Kilcher', '340': 'Yannic Kilcher', '341': 'Yannic Kilcher', '342': 'Yannic Kilcher', '343': 'Yannic Kilcher', '344': 'Yannic Kilcher', '345': 'Yannic Kilcher', '346': 'Yannic Kilcher', '347': 'Yannic Kilcher', '348': 'Yannic Kilcher', '349': 'Turkey Tom', '350': 'Yannic Kilcher', '351': 'Yannic Kilcher', '352': 'Yannic Kilcher', '353': 'Yannic Kilcher', '354': 'Yannic Kilcher', '355': 'Yannic Kilcher', '356': 'Yannic Kilcher', '357': 'Yannic Kilcher', '358': 'Yannic Kilcher', '359': 'Yannic Kilcher', '360': 'Yannic Kilcher', '361': 'Yannic Kilcher', '362': 'Yannic Kilcher', '363': 'Yannic Kilcher', '364': 'Yannic Kilcher', '365': 'Yannic Kilcher', '366': 'Yannic Kilcher', '367': 'Yannic Kilcher', '368': 'Yannic Kilcher', '369': 'Yannic Kilcher', '370': 'Yannic Kilcher', '371': 'Yannic Kilcher', '372': 'Yannic Kilcher', '373': 'Yannic Kilcher', '374': 'Yannic Kilcher', '375': 'Yannic Kilcher', '376': 'Yannic Kilcher', '377': 'Yannic Kilcher', '378': 'Yannic Kilcher', '379': 'Yannic Kilcher', '380': 'Yannic Kilcher', '381': 'Yannic Kilcher', '382': 'Yannic Kilcher', '383': 'Yannic Kilcher', '384': 'Yannic Kilcher', '385': 'Yannic Kilcher', '386': 'Yannic Kilcher', '387': 'Yannic Kilcher', '388': 'Yannic Kilcher', '389': 'Yannic Kilcher', '390': 'Yannic Kilcher', '391': 'Yannic Kilcher', '392': 'Yannic Kilcher', '393': 'Yannic Kilcher', '394': 'Yannic Kilcher', '395': 'Yannic Kilcher', '396': 'Yannic Kilcher', '397': 'Yannic Kilcher', '398': 'Yannic Kilcher', '399': 'Yannic Kilcher', '400': 'Yannic Kilcher', '401': 'Yannic Kilcher', '402': 'Yannic Kilcher', '403': 'Yannic Kilcher', '404': 'Yannic Kilcher', '405': 'Yannic Kilcher', '406': 'Yannic Kilcher', '407': 'Yannic Kilcher', '408': 'Yannic Kilcher', '409': 'Yannic Kilcher', '410': 'Yannic Kilcher', '411': 'Yannic Kilcher', '412': 'Yannic Kilcher', '413': 'Yannic Kilcher', '414': 'Yannic Kilcher', '415': 'Yannic Kilcher', '416': 'Yannic Kilcher', '417': 'Yannic Kilcher', '418': 'Yannic Kilcher', '419': 'Yannic Kilcher', '420': 'Yannic Kilcher', '421': 'Yannic Kilcher', '422': 'Yannic Kilcher', '423': 'Yannic Kilcher', '424': 'Yannic Kilcher', '425': 'Yannic Kilcher', '426': 'Yannic Kilcher', '427': 'Yannic Kilcher', '428': 'Yannic Kilcher', '429': 'Yannic Kilcher', '430': 'Yannic Kilcher', '431': 'Yannic Kilcher', '432': 'Yannic Kilcher', '433': 'Yannic Kilcher', '434': 'Yannic Kilcher', '435': 'Yannic Kilcher', '436': 'Yannic Kilcher', '437': 'Yannic Kilcher', '438': 'Yannic Kilcher', '439': 'Yannic Kilcher', '440': 'Yannic Kilcher', '441': 'Yannic Kilcher', '442': 'Yannic Kilcher', '443': 'Yannic Kilcher', '444': 'Yannic Kilcher', '445': 'Yannic Kilcher', '446': 'Yannic Kilcher', '447': 'Yannic Kilcher', '448': 'Yannic Kilcher', '449': 'Yannic Kilcher', '450': 'Yannic Kilcher', '451': 'Yannic Kilcher', '452': 'Yannic Kilcher', '453': 'Yannic Kilcher', '454': 'Yannic Kilcher', '455': 'Yannic Kilcher', '456': 'Yannic Kilcher', '457': 'Yannic Kilcher', '458': 'Yannic Kilcher', '459': 'Yannic Kilcher', '460': 'CNCF [Cloud Native Computing Foundation]', '461': 'Yannic Kilcher', '462': 'Yannic Kilcher', '463': 'Yannic Kilcher', '464': 'Yannic Kilcher', '465': 'Yannic Kilcher', '466': 'Yannic Kilcher', '467': 'Yannic Kilcher', '468': 'Yannic Kilcher', '469': 'Yannic Kilcher', '470': 'Yannic Kilcher', '471': 'Yannic Kilcher', '472': 'Yannic Kilcher', '473': 'Yannic Kilcher', '474': 'Yannic Kilcher', '475': 'Yannic Kilcher', '476': 'Yannic Kilcher', '477': 'Yannic Kilcher', '478': 'Yannic Kilcher', '479': 'Yannic Kilcher', '480': 'Yannic Kilcher', '481': 'Yannic Kilcher', '482': 'Yannic Kilcher', '483': 'Yannic Kilcher', '484': 'Yannic Kilcher', '485': 'Yannic Kilcher', '486': 'Yannic Kilcher', '487': 'Yannic Kilcher', '488': 'Yannic Kilcher', '489': 'Yannic Kilcher', '490': 'Yannic Kilcher', '491': 'Yannic Kilcher', '492': 'Yannic Kilcher', '493': 'Yannic Kilcher', '494': 'Yannic Kilcher', '495': 'Yannic Kilcher', '496': 'Yannic Kilcher', '497': 'Yannic Kilcher', '498': 'Yannic Kilcher', '499': 'Yannic Kilcher', '500': 'Yannic Kilcher', '501': 'Yannic Kilcher', '502': 'Yannic Kilcher', '503': 'Yannic Kilcher', '504': 'Yannic Kilcher', '505': 'Yannic Kilcher', '506': 'Yannic Kilcher', '507': 'Yannic Kilcher', '508': 'Yannic Kilcher', '509': 'Yannic Kilcher', '510': 'Yannic Kilcher', '511': 'Yannic Kilcher', '512': 'Yannic Kilcher', '513': 'Yannic Kilcher', '514': 'Yannic Kilcher', '515': 'Yannic Kilcher', '516': 'Yannic Kilcher', '517': 'Yannic Kilcher', '518': 'Yannic Kilcher', '519': 'Yannic Kilcher', '520': 'Yannic Kilcher', '521': 'Yannic Kilcher', '522': 'Yannic Kilcher', '523': 'Yannic Kilcher', '524': 'Yannic Kilcher', '525': 'Yannic Kilcher', '526': 'Yannic Kilcher', '527': 'Yannic Kilcher', '528': 'Yannic Kilcher', '529': 'Yannic Kilcher', '530': 'Yannic Kilcher', '531': 'Yannic Kilcher', '532': 'Yannic Kilcher', '533': 'Yannic Kilcher', '534': 'DeepLearningAI', '535': 'DeepLearningAI', '536': 'DeepLearningAI', '537': 'DeepLearningAI', '538': 'DeepLearningAI', '539': 'DeepLearningAI', '540': 'DeepLearningAI', '541': 'DeepLearningAI', '542': 'DeepLearningAI', '543': 'DeepLearningAI', '544': 'DeepLearningAI', '545': 'DeepLearningAI', '546': 'DeepLearningAI', '547': 'DeepLearningAI', '548': 'DeepLearningAI', '549': 'DeepLearningAI', '550': 'DeepLearningAI', '551': 'DeepLearningAI', '552': 'DeepLearningAI', '553': 'DeepLearningAI', '554': 'DeepLearningAI', '555': 'DeepLearningAI', '556': 'DeepLearningAI', '557': 'DeepLearningAI', '558': 'DeepLearningAI', '559': 'DeepLearningAI', '560': 'DeepLearningAI', '561': 'DeepLearningAI', '562': 'DeepLearningAI', '563': 'DeepLearningAI', '564': 'DeepLearningAI', '565': 'DeepLearningAI', '566': 'DeepLearningAI', '567': 'DeepLearningAI', '568': 'DeepLearningAI', '569': 'DeepLearningAI', '570': 'DeepLearningAI', '571': 'DeepLearningAI', '572': 'DeepLearningAI', '573': 'DeepLearningAI', '574': 'DeepLearningAI', '575': 'DeepLearningAI', '576': 'DeepLearningAI', '577': 'DeepLearningAI', '578': 'DeepLearningAI', '579': 'DeepLearningAI', '580': 'DeepLearningAI', '581': 'DeepLearningAI', '582': 'DeepLearningAI', '583': 'DeepLearningAI', '584': 'DeepLearningAI', '585': 'DeepLearningAI', '586': 'DeepLearningAI', '587': 'DeepLearningAI', '588': 'DeepLearningAI', '589': 'DeepLearningAI', '590': 'DeepLearningAI', '591': 'DeepLearningAI', '592': 'DeepLearningAI', '593': 'DeepLearningAI', '594': 'DeepLearningAI', '595': 'DeepLearningAI', '596': 'DeepLearningAI', '597': 'DeepLearningAI', '598': 'DeepLearningAI', '599': 'DeepLearningAI', '600': 'DeepLearningAI', '601': 'DeepLearningAI', '602': 'DeepLearningAI', '603': 'DeepLearningAI', '604': 'DeepLearningAI', '605': 'DeepLearningAI', '606': 'DeepLearningAI', '607': 'DeepLearningAI', '608': 'DeepLearningAI', '609': 'DeepLearningAI', '610': 'DeepLearningAI', '611': 'DeepLearningAI', '612': 'DeepLearningAI', '613': 'DeepLearningAI', '614': 'DeepLearningAI', '615': 'DeepLearningAI', '616': 'DeepLearningAI', '617': 'DeepLearningAI', '618': 'DeepLearningAI', '619': 'DeepLearningAI', '620': 'DeepLearningAI', '621': 'DeepLearningAI', '622': 'DeepLearningAI', '623': 'DeepLearningAI', '624': 'DeepLearningAI', '625': 'DeepLearningAI', '626': 'DeepLearningAI', '627': 'DeepLearningAI', '628': 'DeepLearningAI', '629': 'DeepLearningAI', '630': 'DeepLearningAI', '631': 'DeepLearningAI', '632': 'DeepLearningAI', '633': 'DeepLearningAI', '634': 'DeepLearningAI', '635': 'DeepLearningAI', '636': 'DeepLearningAI', '637': 'DeepLearningAI', '638': 'DeepLearningAI', '639': 'DeepLearningAI', '640': 'DeepLearningAI', '641': 'DeepLearningAI', '642': 'DeepLearningAI', '643': 'DeepLearningAI', '644': 'DeepLearningAI', '645': 'DeepLearningAI', '646': 'DeepLearningAI', '647': 'DeepLearningAI', '648': 'DeepLearningAI', '649': 'DeepLearningAI', '650': 'DeepLearningAI', '651': 'DeepLearningAI', '652': 'DeepLearningAI', '653': 'DeepLearningAI', '654': 'DeepLearningAI', '655': 'DeepLearningAI', '656': 'DeepLearningAI', '657': 'DeepLearningAI', '658': 'DeepLearningAI', '659': 'DeepLearningAI', '660': 'DeepLearningAI', '661': 'DeepLearningAI', '662': 'DeepLearningAI', '663': 'DeepLearningAI', '664': 'DeepLearningAI', '665': 'DeepLearningAI', '666': 'DeepLearningAI', '667': 'DeepLearningAI', '668': 'DeepLearningAI', '669': 'DeepLearningAI', '670': 'DeepLearningAI', '671': 'DeepLearningAI', '672': 'DeepLearningAI', '673': 'DeepLearningAI', '674': 'DeepLearningAI', '675': 'DeepLearningAI', '676': 'Preserve Knowledge', '677': 'DeepLearningAI', '678': 'DeepLearningAI', '679': 'DeepLearningAI', '680': 'DeepLearningAI', '681': 'DeepLearningAI', '682': 'The Artificial Intelligence Channel', '683': 'Lex Fridman', '684': 'DeepLearningAI', '685': 'DeepLearningAI', '686': 'DeepLearningAI', '687': 'DeepLearningAI', '688': 'DeepLearningAI', '689': 'DeepLearningAI', '690': 'DeepLearningAI', '691': 'DeepLearningAI', '692': 'DeepLearningAI', '693': 'DeepLearningAI', '694': 'DeepLearningAI', '695': 'DeepLearningAI', '696': 'DeepLearningAI', '697': 'DeepLearningAI', '698': 'DeepLearningAI', '699': 'DeepLearningAI', '700': 'DeepLearningAI', '701': 'DeepLearningAI', '702': 'DeepLearningAI', '703': 'DeepLearningAI', '704': 'DeepLearningAI', '705': 'DeepLearningAI', '706': 'DeepLearningAI', '707': 'DeepLearningAI', '708': 'DeepLearningAI', '709': 'DeepLearningAI', '710': 'DeepLearningAI', '711': 'DeepLearningAI', '712': 'DeepLearningAI', '713': 'DeepLearningAI', '714': 'DeepLearningAI', '715': 'DeepLearningAI', '716': 'DeepLearningAI', '717': 'DeepLearningAI', '718': 'DeepLearningAI', '719': 'DeepLearningAI', '720': 'DeepLearningAI', '721': 'DeepLearningAI', '722': 'DeepLearningAI', '723': 'DeepLearningAI', '724': 'DeepLearningAI', '725': 'DeepLearningAI', '726': 'DeepLearningAI', '727': 'DeepLearningAI', '728': 'DeepLearningAI', '729': 'DeepLearningAI', '730': 'DeepLearningAI', '731': 'DeepLearningAI', '732': 'DeepLearningAI', '733': 'DeepLearningAI', '734': 'DeepLearningAI', '735': 'DeepLearningAI', '736': 'DeepLearningAI', '737': 'DeepLearningAI', '738': 'DeepLearningAI', '739': 'DeepLearningAI', '740': 'DeepLearningAI', '741': 'DeepLearningAI', '742': 'DeepLearningAI', '743': 'DeepLearningAI', '744': 'DeepLearningAI', '745': 'DeepLearningAI', '746': 'DeepLearningAI', '747': 'DeepLearningAI', '748': 'DeepLearningAI', '749': 'DeepLearningAI', '750': 'DeepLearningAI', '751': 'DeepLearningAI', '752': 'DeepLearningAI', '753': 'DeepLearningAI', '754': 'DeepLearningAI', '755': 'DeepLearningAI', '756': 'DeepLearningAI', '757': 'DeepLearningAI', '758': 'DeepLearningAI', '759': 'DeepLearningAI', '760': 'DeepLearningAI', '761': 'DeepLearningAI', '762': 'DeepLearningAI', '763': 'DeepLearningAI', '764': 'DeepLearningAI', '765': 'DeepLearningAI', '766': 'DeepLearningAI', '767': 'DeepLearningAI', '768': 'DeepLearningAI', '769': 'DeepLearningAI', '770': 'DeepLearningAI', '771': 'DeepLearningAI', '772': 'DeepLearningAI', '773': 'DeepLearningAI', '774': 'DeepLearningAI', '775': 'DeepLearningAI', '776': 'DeepLearningAI', '777': 'DeepLearningAI', '778': 'DeepLearningAI', '779': 'DeepLearningAI', '780': 'DeepLearningAI', '781': 'DeepLearningAI', '782': 'DeepLearningAI', '783': 'DeepLearningAI', '784': 'DeepLearningAI', '785': 'DeepLearningAI', '786': 'DeepLearningAI'}, 'title': {'0': '[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)', '1': '[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)', '2': '[Classic] Generative Adversarial Networks (Paper Explained)', '3': '[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality', '4': '[Classic] Deep Residual Learning for Image Recognition (Paper Explained)', '5': 'Machine Learning PhD Survival Guide 2021 | Advice on Topic Selection, Papers, Conferences & more!', '6': 'Resolution-robust Large Mask Inpainting with Fourier Convolutions (w/ Author Interview)', '7': 'Player of Games: All the games, one algorithm! (w/ author Martin Schmid)', '8': 'This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)', '9': 'Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)', '10': 'Dynamic Inference with Neural Interpreters (w/ author interview)', '11': 'Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)', '12': 'GPT-NeoX-20B - Open-Source huge language model by EleutherAI (Interview w/ co-founder Connor Leahy)', '13': 'Unsupervised Brain Models - How does Deep Learning inform Neuroscience? (w/ Patrick Mineault)', '14': 'Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)', '15': 'HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)', '16': 'AI against Censorship: Genetic Algorithms, The Geneva Project, ML in Security, and more!', '17': 'CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)', '18': 'All about AI Accelerators: GPU, TPU, Dataflow, Near-Memory, Optical, Neuromorphic & more (w/ Author)', '19': 'Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)', '20': 'Can Wikipedia Help Offline Reinforcement Learning? (Author Interview)', '21': 'AlphaCode - with the authors!', '22': 'First Author Interview: AI & formal math (Formal Mathematics Statement Curriculum Learning)', '23': 'Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents', '24': "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis", '25': 'Active Dendrites avoid catastrophic forgetting - Interview with the Authors', '26': 'One Model For All The Tasks - BLIP (Author Interview)', '27': 'Author Interview - Typical Decoding for Natural Language Generation', '28': 'Author Interview - Memory-assisted prompt editing to improve GPT-3 after deployment', '29': 'Author Interview - Improving Intrinsic Exploration with Language Abstractions', '30': 'The Weird and Wonderful World of AI Art (w/ Author Jack Morris)', '31': 'Author Interview - Transformer Memory as a Differentiable Search Index', '32': 'Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)', '33': 'LAION-5B: 5 billion image-text-pairs dataset (with the authors)', '34': 'Author Interview - ACCEL: Evolving Curricula with Regret-Based Environment Design', '35': 'Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', '36': 'The Future of AI is Self-Organizing and Self-Assembling (w/ Prof. Sebastian Risi)', '37': 'More Is Different for AI - Scaling Up, Emergence, and Paperclip Maximizers (w/ Jacob Steinhardt)', '38': 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models', '39': 'Parti - Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Paper Explained)', '40': '[ML News] Text-to-Image models are taking over! (Imagen, DALL-E 2, Midjourney, CogView 2 & more)', '41': 'The Man behind Stable Diffusion', '42': 'Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust (Explained)', '43': 'World Models', '44': 'Curiosity-driven Exploration by Self-supervised Prediction', '45': 'Reinforcement Learning with Unsupervised Auxiliary Tasks', '46': 'Learning model-based planning from scratch', '47': 'Imagination-Augmented Agents for Deep Reinforcement Learning', '48': 'Reinforcement Learning, Fast and Slow', '49': 'Learning World Graphs to Accelerate Hierarchical Reinforcement Learning', '50': 'LeDeepChef 👨\u200d🍳 Deep Reinforcement Learning Agent for Families of Text-Based Games', '51': 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures', '52': 'AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning', '53': 'A neurally plausible model learns successor representations in partially observable environments', '54': 'MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model', '55': "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions", '56': 'Go-Explore: a New Approach for Hard-Exploration Problems', '57': 'Agent57: Outperforming the Atari Human Benchmark', '58': 'Dream to Control: Learning Behaviors by Latent Imagination', '59': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Solutions', '60': 'Enhanced POET: Open-Ended RL through Unbounded Invention of Learning Challenges and their Solutions', '61': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning', '62': 'Datasets for Data-Driven Reinforcement Learning', '63': 'Thinking While Moving: Deep Reinforcement Learning with Concurrent Control', '64': 'The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies (Paper Explained)', '65': 'Chip Placement with Deep Reinforcement Learning (Paper Explained)', '66': 'Reinforcement Learning with Augmented Data (Paper Explained)', '67': 'Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning (Paper Explained)', '68': 'Planning to Explore via Self-Supervised World Models (Paper Explained)', '69': 'Investigating Human Priors for Playing Video Games (Paper & Demo)', '70': 'Regularizing Trajectory Optimization with Denoising Autoencoders (Paper Explained)', '71': 'Dynamics-Aware Unsupervised Discovery of Skills (Paper Explained)', '72': 'PCGRL: Procedural Content Generation via Reinforcement Learning (Paper Explained)', '73': '[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)', '74': 'What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (Paper Explained)', '75': 'Meta-Learning through Hebbian Plasticity in Random Networks (Paper Explained)', '76': 'Fast reinforcement learning with generalized policy updates (Paper Explained)', '77': 'Learning to summarize from human feedback (Paper Explained)', '78': 'The XX   Intro HQ', '79': 'Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess (Paper Explained)', '80': 'ReBeL - Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (Explained)', '81': 'Dreamer v2: Mastering Atari with Discrete World Models (Machine Learning Research Paper Explained)', '82': 'Fast and Slow Learning of Recurrent Independent Mechanisms (Machine Learning Paper Explained)', '83': 'Decision Transformer: Reinforcement Learning via Sequence Modeling (Research Paper Explained)', '84': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control (Paper Explained)', '85': 'EfficientZero: Mastering Atari Games with Limited Data (Machine Learning Research Paper Explained)', '86': 'Player of Games: All the games, one algorithm! (w/ author Martin Schmid)', '87': 'Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)', '88': 'Can Wikipedia Help Offline Reinforcement Learning? (Author Interview)', '89': 'Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents', '90': 'Improving Intrinsic Exploration with Language Abstractions (Machine Learning Paper Explained)', '91': 'Author Interview - Improving Intrinsic Exploration with Language Abstractions', '92': 'ACCEL: Evolving Curricula with Regret-Based Environment Design (Paper Review)', '93': 'Author Interview - ACCEL: Evolving Curricula with Regret-Based Environment Design', '94': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)', '95': 'Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', '96': 'This is a game changer! (AlphaTensor by DeepMind explained)', '97': 'CICERO: An AI agent that negotiates, persuades, and cooperates with people', '98': 'GPT-2: Language Models are Unsupervised Multitask Learners', '99': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', '100': 'Stochastic RNNs without Teacher-Forcing', '101': 'Attention Is All You Need', '102': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', '103': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach', '104': 'LeDeepChef 👨\u200d🍳 Deep Reinforcement Learning Agent for Families of Text-Based Games', '105': 'Reformer: The Efficient Transformer', '106': 'Turing-NLG, DeepSpeed and the ZeRO optimizer', '107': 'Deep Learning for Symbolic Mathematics', '108': 'Evaluating NLP Models via Contrast Sets', '109': 'Imputer: Sequence Modelling via Imputation and Dynamic Programming', '110': 'Longformer: The Long-Document Transformer', '111': 'I talk to the new Facebook Blender Chatbot', '112': 'TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)', '113': '[Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)', '114': '[News] OpenAI Model Generates Python Code', '115': 'When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)', '116': 'GPT-3: Language Models are Few-Shot Learners (Paper Explained)', '117': 'Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)', '118': 'Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)', '119': 'BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)', '120': 'TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)', '121': 'Linformer: Self-Attention with Linear Complexity (Paper Explained)', '122': 'VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)', '123': 'Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)', '124': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)', '125': 'BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)', '126': 'Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)', '127': '[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality', '128': 'Big Bird: Transformers for Longer Sequences (Paper Explained)', '129': 'Hopfield Networks is All You Need (Paper Explained)', '130': 'REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)', '131': 'Learning to summarize from human feedback (Paper Explained)', '132': 'Rethinking Attention with Performers (Paper Explained)', '133': 'Language Models are Open Knowledge Graphs (Paper Explained)', '134': 'Extracting Training Data from Large Language Models (Paper Explained)', '135': 'OpenAI DALL·E: Creating Images from Text (Blog Post Explained)', '136': 'OpenAI CLIP: ConnectingText and Images (Paper Explained)', '137': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity', '138': 'Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)', '139': 'Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention (AI Paper Explained)', '140': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)', '141': 'ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation', '142': '∞-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)', '143': 'Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset', '144': 'Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)', '145': 'Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained', '146': 'GPT-NeoX-20B - Open-Source huge language model by EleutherAI (Interview w/ co-founder Connor Leahy)', '147': 'Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)', '148': 'CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)', '149': 'Competition-Level Code Generation with AlphaCode (Paper Review)', '150': 'Typical Decoding for Natural Language Generation (Get more human-like outputs from language models!)', '151': 'Author Interview - Typical Decoding for Natural Language Generation', '152': 'Memory-assisted prompt editing to improve GPT-3 after deployment (Machine Learning Paper Explained)', '153': 'Author Interview - Memory-assisted prompt editing to improve GPT-3 after deployment', '154': 'Transformer Memory as a Differentiable Search Index (Machine Learning Research Paper Explained)', '155': 'Author Interview - Transformer Memory as a Differentiable Search Index', '156': 'Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)', '157': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)', '158': 'Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', '159': "Did Google's LaMDA chatbot just become sentient?", '160': 'ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)', '161': 'Galactica: A Large Language Model for Science (Drama & Paper Review)', '162': 'ChatGPT: This AI has a JAILBREAK?! (Unbelievable AI Progress)', '163': "OpenAssistant - ChatGPT's Open Alternative (We need your help!)", '164': 'LLaMA: Open and Efficient Foundation Language Models (Paper Explained)', '165': 'GPT-4 is here! What we know so far (Full Analysis)', '166': 'Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)', '167': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Full Paper Review)', '168': 'RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)', '169': 'Adversarial Examples Are Not Bugs, They Are Features', '170': 'Evaluating NLP Models via Contrast Sets', '171': 'Shortcut Learning in Deep Neural Networks', '172': 'Extracting Training Data from Large Language Models (Paper Explained)', '173': 'The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Research Paper Explained)', '174': 'Population-Based Search and Open-Ended Algorithms', '175': 'Conversation about Population-Based Methods (Re-upload)', '176': 'Reconciling modern machine learning and the bias-variance trade-off', '177': 'Accelerating Deep Learning by Focusing on the Biggest Losers', '178': 'The Visual Task Adaptation Benchmark', '179': 'The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks', '180': 'FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence', '181': 'Gradient Surgery for Multi-Task Learning', '182': 'Feature Visualization & The OpenAI microscope', '183': 'Shortcut Learning in Deep Neural Networks', '184': 'Backpropagation and the brain', '185': 'Supervised Contrastive Learning', '186': 'Do ImageNet Classifiers Generalize to ImageNet? (Paper Explained)', '187': 'Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Paper Explained)', '188': 'Big Transfer (BiT): General Visual Representation Learning (Paper Explained)', '189': 'Concept Learning with Energy-Based Models (Paper Explained)', '190': 'Faster Neural Network Training with Data Echoing (Paper Explained)', '191': 'A critical analysis of self-supervision, or what we can learn from a single image (Paper Explained)', '192': 'iMAML: Meta-Learning with Implicit Gradients (Paper Explained)', '193': 'mixup: Beyond Empirical Risk Minimization (Paper Explained)', '194': 'On the Measure of Intelligence by François Chollet - Part 1: Foundations (Paper Explained)', '195': 'On the Measure of Intelligence by François Chollet - Part 2: Human Priors (Paper Explained)', '196': 'On the Measure of Intelligence by François Chollet - Part 4: The ARC Challenge (Paper Explained)', '197': 'On the Measure of Intelligence by François Chollet - Part 3: The Math (Paper Explained)', '198': 'Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)', '199': 'Self-training with Noisy Student improves ImageNet classification (Paper Explained)', '200': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)', '201': '[Live Machine Learning Research] Plain Self-Ensembles (I actually DISCOVER SOMETHING) - Part 1', '202': 'SupSup: Supermasks in Superposition (Paper Explained)', '203': 'Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)', '204': 'Deep Ensembles: A Loss Landscape Perspective (Paper Explained)', '205': 'Neural Architecture Search without Training (Paper Explained)', '206': 'Hopfield Networks is All You Need (Paper Explained)', '207': 'Radioactive data: tracing through training (Paper Explained)', '208': 'Self-classifying MNIST Digits (Paper Explained)', '209': 'Training more effective learned optimizers, and using them to train themselves (Paper Explained)', '210': 'Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Paper Explained)', '211': 'Underspecification Presents Challenges for Credibility in Modern Machine Learning (Paper Explained)', '212': 'Predictive Coding Approximates Backprop along Arbitrary Computation Graphs (Paper Explained)', '213': 'Deep Networks Are Kernel Machines (Paper Explained)', '214': 'Multimodal Neurons in Artificial Neural Networks (w/ OpenAI Microscope, Research Paper Explained)', '215': 'Pretrained Transformers as Universal Computation Engines (Machine Learning Research Paper Explained)', '216': 'Efficient and Modular Implicit Differentiation (Machine Learning Research Paper Explained)', '217': 'PonderNet: Learning to Ponder (Machine Learning Research Paper Explained)', '218': 'Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)', '219': 'Gradients are Not All You Need (Machine Learning Research Paper Explained)', '220': 'Learning Rate Grafting: Transferability of Optimizer Tuning (Machine Learning Research Paper Review)', '221': 'Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Paper Explained)', '222': 'Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)', '223': "VOS: Learning What You Don't Know by Virtual Outlier Synthesis (Paper Explained)", '224': "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis", '225': 'Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments (Review)', '226': 'Active Dendrites avoid catastrophic forgetting - Interview with the Authors', '227': 'LAION-5B: 5 billion image-text-pairs dataset (with the authors)', '228': 'JEPA - A Path Towards Autonomous Machine Intelligence (Paper Explained)', '229': 'How to make your CPU as fast as a GPU - Advances in Sparsity w/ Nir Shavit', '230': 'This is a game changer! (AlphaTensor by DeepMind explained)', '231': 'Neural Networks are Decision Trees (w/ Alexander Mattick)', '232': 'Neural Ordinary Differential Equations', '233': 'GPT-2: Language Models are Unsupervised Multitask Learners', '234': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', '235': 'What’s in a name? The need to nip NIPS', '236': 'Attention Is All You Need', '237': 'Imagination-Augmented Agents for Deep Reinforcement Learning', '238': 'Adversarial Examples Are Not Bugs, They Are Features', '239': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding', '240': 'Manifold Mixup: Better Representations by Interpolating Hidden States', '241': 'Dynamic Routing Between Capsules', '242': 'DEEP LEARNING MEME REVIEW - Episode 1', '243': 'AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning', '244': 'MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model', '245': "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions", '246': 'Reformer: The Efficient Transformer', '247': 'WHO ARE YOU? 10k Subscribers Special (w/ Channel Analytics)', '248': 'Concept Learning with Energy-Based Models (Paper Explained)', '249': 'DETR: End-to-End Object Detection with Transformers (Paper Explained)', '250': "How I Read a Paper: Facebook's DETR (Video Tutorial)", '251': 'MEMES IS ALL YOU NEED - Deep Learning Meme Review - Episode 2 (Part 1 of 2)', '252': 'OpenAI DALL·E: Creating Images from Text (Blog Post Explained)', '253': 'STOCHASTIC MEME DESCENT - Deep Learning Meme Review - Episode 2 (Part 2 of 2)', '254': "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)", '255': "Apple or iPod??? Easy Fix for Adversarial Textual Attacks on OpenAI's CLIP Model! #Shorts", '256': 'Machine Learning PhD Survival Guide 2021 | Advice on Topic Selection, Papers, Conferences & more!', '257': 'I BUILT A NEURAL NETWORK IN MINECRAFT | Analog Redstone Network w/ Backprop & Optimizer (NO MODS)', '258': "I COOKED A RECIPE MADE BY A.I. | Cooking with GPT-3 (Don't try this at home)", '259': "AI made this music video | What happens when OpenAI's CLIP meets BigGAN?", '260': 'I took a Swiss train and it was awesome! Train Seat Review - SBB InterCity 1 - Geneva to St. Gallen', '261': 'This A.I. creates infinite NFTs', '262': 'The hidden dangers of loading open-source AI models (ARBITRARY CODE EXPLOIT!)', '263': 'This is a game changer! (AlphaTensor by DeepMind explained)', '264': 'ChatGPT: This AI has a JAILBREAK?! (Unbelievable AI Progress)', '265': 'GPT-4chan: This is the worst AI ever', '266': "Did Google's LaMDA chatbot just become sentient?", '267': "OpenAssistant - ChatGPT's Open Alternative (We need your help!)", '268': 'OpenAssistant First Models are here! (Open-Source ChatGPT)', '269': "OpenAssistant RELEASED! The world's best open-source Chat AI!", '270': 'Neural Ordinary Differential Equations', '271': 'The Odds are Odd: A Statistical Test for Detecting Adversarial Examples', '272': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', '273': 'Stochastic RNNs without Teacher-Forcing', '274': 'Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations', '275': 'Attention Is All You Need', '276': 'Manifold Mixup: Better Representations by Interpolating Hidden States', '277': 'Processing Megapixel Images with Deep Attention-Sampling Models', '278': 'Gauge Equivariant Convolutional Networks and the Icosahedral CNN', '279': 'Dynamic Routing Between Capsules', '280': 'SinGAN: Learning a Generative Model from a Single Natural Image', '281': 'Reformer: The Efficient Transformer', '282': 'Growing Neural Cellular Automata', '283': 'Deep Learning for Symbolic Mathematics', '284': 'Axial Attention & MetNet: A Neural Weather Model for Precipitation Forecasting', '285': 'POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Solutions', '286': 'Evolving Normalization-Activation Layers', '287': 'The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks', '288': 'FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence', '289': 'Feature Visualization & The OpenAI microscope', '290': 'Longformer: The Long-Document Transformer', '291': 'Jukebox: A Generative Model for Music (Paper Explained)', '292': 'Big Transfer (BiT): General Visual Representation Learning (Paper Explained)', '293': 'Group Normalization (Paper Explained)', '294': 'Weight Standardization (Paper Explained)', '295': 'DETR: End-to-End Object Detection with Transformers (Paper Explained)', '296': 'Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)', '297': 'Learning To Classify Images Without Labels (Paper Explained)', '298': 'Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search (Paper Explained)', '299': 'End-to-End Adversarial Text-to-Speech (Paper Explained)', '300': 'Linformer: Self-Attention with Linear Complexity (Paper Explained)', '301': 'VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)', '302': 'SynFlow: Pruning neural networks without any data by iteratively conserving synaptic flow', '303': 'A bio-inspired bistable recurrent cell allows for long-lasting memory (Paper Explained)', '304': 'TUNIT: Rethinking the Truly Unsupervised Image-to-Image Translation (Paper Explained)', '305': 'SIREN: Implicit Neural Representations with Periodic Activation Functions (Paper Explained)', '306': 'RepNet: Counting Out Time - Class Agnostic Video Repetition Counting in the Wild (Paper Explained)', '307': 'Discovering Symbolic Models from Deep Learning with Inductive Biases (Paper Explained)', '308': 'Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)', '309': 'Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection (Paper Explained)', '310': 'Set Distribution Networks: a Generative Model for Sets of Images (Paper Explained)', '311': 'Object-Centric Learning with Slot Attention (Paper Explained)', '312': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)', '313': 'SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization (Paper Explained)', '314': 'Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)', '315': 'SupSup: Supermasks in Superposition (Paper Explained)', '316': 'Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)', '317': 'NVAE: A Deep Hierarchical Variational Autoencoder (Paper Explained)', '318': 'Gradient Origin Networks (Paper Explained w/ Live Coding)', '319': '[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)', '320': '[Classic] Generative Adversarial Networks (Paper Explained)', '321': '[Classic] Deep Residual Learning for Image Recognition (Paper Explained)', '322': 'Neural Architecture Search without Training (Paper Explained)', '323': 'Big Bird: Transformers for Longer Sequences (Paper Explained)', '324': 'Hopfield Networks is All You Need (Paper Explained)', '325': 'Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation (Paper Explained)', '326': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)', '327': 'LambdaNetworks: Modeling long-range Interactions without Attention (Paper Explained)', '328': 'Rethinking Attention with Performers (Paper Explained)', '329': 'Fourier Neural Operator for Parametric Partial Differential Equations (Paper Explained)', '330': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity', '331': 'Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)', '332': 'NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)', '333': 'TransGAN: Two Transformers Can Make One Strong GAN (Machine Learning Research Paper Explained)', '334': 'Linear Transformers Are Secretly Fast Weight Memory Systems (Machine Learning Paper Explained)', '335': "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)", '336': 'Perceiver: General Perception with Iterative Attention (Google DeepMind Research Paper Explained)', '337': 'MLP-Mixer: An all-MLP Architecture for Vision (Machine Learning Research Paper Explained)', '338': 'Involution: Inverting the Inherence of Convolution for Visual Recognition (Research Paper Explained)', '339': 'Expire-Span: Not All Memories are Created Equal: Learning to Forget by Expiring (Paper Explained)', '340': 'Fastformer: Additive Attention Can Be All You Need (Machine Learning Research Paper Explained)', '341': 'Autoregressive Diffusion Models (Machine Learning Research Paper Explained)', '342': 'Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained', '343': 'NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion (ML Research Paper Explained)', '344': 'Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)', '345': 'Dynamic Inference with Neural Interpreters (w/ author interview)', '346': 'HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)', '347': 'CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)', '348': 'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding&Generation', '349': 'The Pewdiepie Story: Maintaining A YouTube Empire', '350': 'One Model For All The Tasks - BLIP (Author Interview)', '351': 'Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)', '352': 'RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)', '353': '[ML Coding Tips] Separate Computation & Plotting using locals', '354': '[Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)', '355': 'Do ImageNet Classifiers Generalize to ImageNet? (Paper Explained)', '356': 'Jukebox: A Generative Model for Music (Paper Explained)', '357': 'TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)', '358': 'Reinforcement Learning with Augmented Data (Paper Explained)', '359': 'Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning (Paper Explained)', '360': 'Big Transfer (BiT): General Visual Representation Learning (Paper Explained)', '361': 'Concept Learning with Energy-Based Models (Paper Explained)', '362': 'Group Normalization (Paper Explained)', '363': 'Faster Neural Network Training with Data Echoing (Paper Explained)', '364': 'Weight Standardization (Paper Explained)', '365': 'A critical analysis of self-supervision, or what we can learn from a single image (Paper Explained)', '366': 'iMAML: Meta-Learning with Implicit Gradients (Paper Explained)', '367': 'mixup: Beyond Empirical Risk Minimization (Paper Explained)', '368': 'Planning to Explore via Self-Supervised World Models (Paper Explained)', '369': 'Deep image reconstruction from human brain activity (Paper Explained)', '370': 'Investigating Human Priors for Playing Video Games (Paper & Demo)', '371': 'When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)', '372': 'Regularizing Trajectory Optimization with Denoising Autoencoders (Paper Explained)', '373': 'DETR: End-to-End Object Detection with Transformers (Paper Explained)', '374': 'GPT-3: Language Models are Few-Shot Learners (Paper Explained)', '375': 'Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)', '376': 'Dynamics-Aware Unsupervised Discovery of Skills (Paper Explained)', '377': 'On the Measure of Intelligence by François Chollet - Part 1: Foundations (Paper Explained)', '378': 'Learning To Classify Images Without Labels (Paper Explained)', '379': 'CornerNet: Detecting Objects as Paired Keypoints (Paper Explained)', '380': 'Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search (Paper Explained)', '381': 'BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)', '382': 'TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)', '383': 'End-to-End Adversarial Text-to-Speech (Paper Explained)', '384': 'Linformer: Self-Attention with Linear Complexity (Paper Explained)', '385': 'VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)', '386': 'On the Measure of Intelligence by François Chollet - Part 2: Human Priors (Paper Explained)', '387': 'On the Measure of Intelligence by François Chollet - Part 4: The ARC Challenge (Paper Explained)', '388': 'On the Measure of Intelligence by François Chollet - Part 3: The Math (Paper Explained)', '389': 'PCGRL: Procedural Content Generation via Reinforcement Learning (Paper Explained)', '390': 'Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)', '391': 'SynFlow: Pruning neural networks without any data by iteratively conserving synaptic flow', '392': 'A bio-inspired bistable recurrent cell allows for long-lasting memory (Paper Explained)', '393': 'TUNIT: Rethinking the Truly Unsupervised Image-to-Image Translation (Paper Explained)', '394': 'BYOL: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (Paper Explained)', '395': 'Image GPT: Generative Pretraining from Pixels (Paper Explained)', '396': 'Big Self-Supervised Models are Strong Semi-Supervised Learners (Paper Explained)', '397': 'SIREN: Implicit Neural Representations with Periodic Activation Functions (Paper Explained)', '398': 'RepNet: Counting Out Time - Class Agnostic Video Repetition Counting in the Wild (Paper Explained)', '399': "How I Read a Paper: Facebook's DETR (Video Tutorial)", '400': 'Discovering Symbolic Models from Deep Learning with Inductive Biases (Paper Explained)', '401': 'Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)', '402': 'Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection (Paper Explained)', '403': 'Set Distribution Networks: a Generative Model for Sets of Images (Paper Explained)', '404': 'Self-training with Noisy Student improves ImageNet classification (Paper Explained)', '405': 'Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)', '406': 'Object-Centric Learning with Slot Attention (Paper Explained)', '407': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)', '408': 'BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)', '409': 'SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization (Paper Explained)', '410': 'Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)', '411': 'SupSup: Supermasks in Superposition (Paper Explained)', '412': 'Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)', '413': 'NVAE: A Deep Hierarchical Variational Autoencoder (Paper Explained)', '414': 'Gradient Origin Networks (Paper Explained w/ Live Coding)', '415': 'Deep Ensembles: A Loss Landscape Perspective (Paper Explained)', '416': '[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)', '417': '[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)', '418': '[Classic] Generative Adversarial Networks (Paper Explained)', '419': '[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality', '420': '[Classic] Deep Residual Learning for Image Recognition (Paper Explained)', '421': 'Neural Architecture Search without Training (Paper Explained)', '422': 'What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (Paper Explained)', '423': 'Hopfield Networks is All You Need (Paper Explained)', '424': 'Meta-Learning through Hebbian Plasticity in Random Networks (Paper Explained)', '425': 'REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)', '426': 'Fast reinforcement learning with generalized policy updates (Paper Explained)', '427': 'Radioactive data: tracing through training (Paper Explained)', '428': 'Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation (Paper Explained)', '429': 'Self-classifying MNIST Digits (Paper Explained)', '430': 'Learning to summarize from human feedback (Paper Explained)', '431': 'The Hardware Lottery (Paper Explained)', '432': 'Training more effective learned optimizers, and using them to train themselves (Paper Explained)', '433': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)', '434': 'Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Paper Explained)', '435': 'LambdaNetworks: Modeling long-range Interactions without Attention (Paper Explained)', '436': 'Rethinking Attention with Performers (Paper Explained)', '437': 'Language Models are Open Knowledge Graphs (Paper Explained)', '438': 'Underspecification Presents Challenges for Credibility in Modern Machine Learning (Paper Explained)', '439': 'Fourier Neural Operator for Parametric Partial Differential Equations (Paper Explained)', '440': 'Predictive Coding Approximates Backprop along Arbitrary Computation Graphs (Paper Explained)', '441': "DeepMind's AlphaFold 2 Explained! AI Breakthrough in Protein Folding! What we know (& what we don't)", '442': 'ReBeL - Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (Explained)', '443': 'Extracting Training Data from Large Language Models (Paper Explained)', '444': 'OpenAI DALL·E: Creating Images from Text (Blog Post Explained)', '445': 'OpenAI CLIP: ConnectingText and Images (Paper Explained)', '446': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity', '447': 'SingularityNET - A Decentralized, Open Market and Network for AIs (Whitepaper Explained)', '448': 'Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)', '449': 'Deep Networks Are Kernel Machines (Paper Explained)', '450': 'Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention (AI Paper Explained)', '451': 'NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)', '452': 'TransGAN: Two Transformers Can Make One Strong GAN (Machine Learning Research Paper Explained)', '453': 'Dreamer v2: Mastering Atari with Discrete World Models (Machine Learning Research Paper Explained)', '454': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)', '455': 'Linear Transformers Are Secretly Fast Weight Memory Systems (Machine Learning Paper Explained)', '456': "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)", '457': 'Multimodal Neurons in Artificial Neural Networks (w/ OpenAI Microscope, Research Paper Explained)', '458': 'Yann LeCun - Self-Supervised Learning: The Dark Matter of Intelligence (FAIR Blog Post Explained)', '459': 'Pretrained Transformers as Universal Computation Engines (Machine Learning Research Paper Explained)', '460': 'Webinar: MLOps automation with Git Based CI/CD for ML', '461': 'Perceiver: General Perception with Iterative Attention (Google DeepMind Research Paper Explained)', '462': 'DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning', '463': 'NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ML Research Paper Explained)', '464': 'Why AI is Harder Than We Think (Machine Learning Research Paper Explained)', '465': 'DINO: Emerging Properties in Self-Supervised Vision Transformers (Facebook AI Research Explained)', '466': 'Involution: Inverting the Inherence of Convolution for Visual Recognition (Research Paper Explained)', '467': 'DDPM - Diffusion Models Beat GANs on Image Synthesis (Machine Learning Research Paper Explained)', '468': 'Expire-Span: Not All Memories are Created Equal: Learning to Forget by Expiring (Paper Explained)', '469': 'Fast and Slow Learning of Recurrent Independent Mechanisms (Machine Learning Paper Explained)', '470': 'Reward Is Enough (Machine Learning Research Paper Explained)', '471': 'Decision Transformer: Reinforcement Learning via Sequence Modeling (Research Paper Explained)', '472': 'Efficient and Modular Implicit Differentiation (Machine Learning Research Paper Explained)', '473': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control (Paper Explained)', '474': 'XCiT: Cross-Covariance Image Transformers (Facebook AI Machine Learning Research Paper Explained)', '475': 'The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Research Paper Explained)', '476': 'How Apple scans your phone (and how to evade it) - NeuralHash CSAM Detection Algorithm Explained', '477': 'PonderNet: Learning to Ponder (Machine Learning Research Paper Explained)', '478': 'Fastformer: Additive Attention Can Be All You Need (Machine Learning Research Paper Explained)', '479': 'ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation', '480': '∞-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)', '481': 'Topographic VAEs learn Equivariant Capsules (Machine Learning Research Paper Explained)', '482': 'Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset', '483': 'Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment (Paper Explained)', '484': "How far can we scale up? Deep Learning's Diminishing Returns (Article Review)", '485': 'Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)', '486': 'Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)', '487': 'EfficientZero: Mastering Atari Games with Limited Data (Machine Learning Research Paper Explained)', '488': 'Autoregressive Diffusion Models (Machine Learning Research Paper Explained)', '489': 'Gradients are Not All You Need (Machine Learning Research Paper Explained)', '490': 'Learning Rate Grafting: Transferability of Optimizer Tuning (Machine Learning Research Paper Review)', '491': 'Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Paper Explained)', '492': 'Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained', '493': 'NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion (ML Research Paper Explained)', '494': 'Resolution-robust Large Mask Inpainting with Fourier Convolutions (w/ Author Interview)', '495': 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models', '496': 'Player of Games: All the games, one algorithm! (w/ author Martin Schmid)', '497': 'This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)', '498': 'Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)', '499': 'Dynamic Inference with Neural Interpreters (w/ author interview)', '500': 'Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)', '501': 'Unsupervised Brain Models - How does Deep Learning inform Neuroscience? (w/ Patrick Mineault)', '502': 'Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)', '503': 'HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)', '504': 'AI against Censorship: Genetic Algorithms, The Geneva Project, ML in Security, and more!', '505': 'CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)', '506': 'Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)', '507': 'Competition-Level Code Generation with AlphaCode (Paper Review)', '508': 'OpenAI tackles Math - Formal Mathematics Statement Curriculum Learning (Paper Explained)', '509': 'Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents', '510': "VOS: Learning What You Don't Know by Virtual Outlier Synthesis (Paper Explained)", '511': "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis", '512': 'Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments (Review)', '513': 'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding&Generation', '514': 'Typical Decoding for Natural Language Generation (Get more human-like outputs from language models!)', '515': 'Memory-assisted prompt editing to improve GPT-3 after deployment (Machine Learning Paper Explained)', '516': 'Improving Intrinsic Exploration with Language Abstractions (Machine Learning Paper Explained)', '517': 'Transformer Memory as a Differentiable Search Index (Machine Learning Research Paper Explained)', '518': 'ACCEL: Evolving Curricula with Regret-Based Environment Design (Paper Review)', '519': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)', '520': 'Parti - Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Paper Explained)', '521': 'Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Paper Explained)', '522': 'JEPA - A Path Towards Autonomous Machine Intelligence (Paper Explained)', '523': 'This is a game changer! (AlphaTensor by DeepMind explained)', '524': 'Neural Networks are Decision Trees (w/ Alexander Mattick)', '525': 'ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)', '526': 'Galactica: A Large Language Model for Science (Drama & Paper Review)', '527': 'CICERO: An AI agent that negotiates, persuades, and cooperates with people', '528': 'LLaMA: Open and Efficient Foundation Language Models (Paper Explained)', '529': 'GPT-4 is here! What we know so far (Full Analysis)', '530': 'Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)', '531': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Full Paper Review)', '532': 'RWKV: Reinventing RNNs for the Transformer Era (Paper Explained)', '533': 'Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust (Explained)', '534': 'Train/Dev/Test Sets (C2W1L01)', '535': 'Bias/Variance (C2W1L02)', '536': 'Basic Recipe for Machine Learning (C2W1L03)', '537': 'Regularization (C2W1L04)', '538': 'Why Regularization Reduces Overfitting (C2W1L05)', '539': 'Dropout Regularization (C2W1L06)', '540': 'Understanding Dropout (C2W1L07)', '541': 'Other Regularization Methods (C2W1L08)', '542': 'Normalizing Inputs (C2W1L09)', '543': 'Vanishing/Exploding Gradients (C2W1L10)', '544': 'Weight Initialization in a Deep Network (C2W1L11)', '545': 'Numerical Approximations of Gradients (C2W1L12)', '546': 'Gradient Checking (C2W1L13)', '547': 'Gradient Checking Implementation Notes (C2W1L14)', '548': 'Mini Batch Gradient Descent (C2W2L01)', '549': 'Understanding Mini-Batch Gradient Dexcent (C2W2L02)', '550': 'Exponentially Weighted Averages (C2W2L03)', '551': 'Understanding Exponentially Weighted Averages (C2W2L04)', '552': 'Bias Correction of Exponentially Weighted Averages (C2W2L05)', '553': 'Gradient Descent With Momentum (C2W2L06)', '554': 'RMSProp (C2W2L07)', '555': 'Adam Optimization Algorithm (C2W2L08)', '556': 'Learning Rate Decay (C2W2L09)', '557': 'Tuning Process (C2W3L01)', '558': 'Using an Appropriate Scale (C2W3L02)', '559': 'Hyperparameter Tuning in Practice (C2W3L03)', '560': 'Normalizing Activations in a Network (C2W3L04)', '561': 'Fitting Batch Norm Into Neural Networks (C2W3L05)', '562': 'Why Does Batch Norm Work? (C2W3L06)', '563': 'Batch Norm At Test Time (C2W3L07)', '564': 'Softmax Regression (C2W3L08)', '565': 'Training Softmax Classifier (C2W3L09)', '566': 'The Problem of Local Optima (C2W3L10)', '567': 'TensorFlow (C2W3L11)', '568': 'Welcome (Deep Learning Specialization C1W1L01)', '569': 'What is a Neural Network? (C1W1L02)', '570': 'Supervised Learning with a Neural Network (C1W1L03)', '571': 'Why is deep learning taking off? (C1W1L04)', '572': 'About This Course (C1W1L05)', '573': 'Course Resources (C1W1L06)', '574': 'Binary Classification (C1W2L01)', '575': 'Logistic Regression (C1W2L02)', '576': 'Logistic Regression Cost Function (C1W2L03)', '577': 'Gradient Descent (C1W2L04)', '578': 'Derivatives (C1W2L05)', '579': 'More Derivative Examples (C1W2L06)', '580': 'Computation Graph (C1W2L07)', '581': 'Derivatives With Computation Graphs (C1W2L08)', '582': 'Logistic Regression Gradient Descent (C1W2L09)', '583': 'Gradient Descent on m Examples (C1W2L10)', '584': 'Vectorization (C1W2L11)', '585': 'More Vectorization Examples (C1W2L12)', '586': 'Vectorizing Logistic Regression (C1W2L13)', '587': "Vectorizing Logistic Regression's Gradient Computation (C1W2L14)", '588': 'Broadcasting in Python (C1W2L15)', '589': 'A Note on Python/Numpy Vectors (C1W2L16)', '590': 'Quick Tour of Jupyter/iPython Notebooks (C1W2L17)', '591': "Explanation of Logistic Regression's Cost Function (C1W2L18)", '592': 'Neural Network Overview (C1W3L01)', '593': 'Neural Network Representations (C1W3L02)', '594': 'Computing Neural Network Output (C1W3L03)', '595': 'Vectorizing Across Multiple Examples (C1W3L04)', '596': 'Explanation For Vectorized Implementation (C1W3L05)', '597': 'Activation Functions (C1W3L06)', '598': 'Why Non-linear Activation Functions (C1W3L07)', '599': 'Derivatives Of Activation Functions (C1W3L08)', '600': 'Gradient Descent For Neural Networks (C1W3L09)', '601': 'Backpropagation Intuition (C1W3L10)', '602': 'Random Initialization (C1W3L11)', '603': 'Deep L-Layer Neural Network (C1W4L01)', '604': 'Forward Propagation in a Deep Network (C1W4L02)', '605': 'Getting Matrix Dimensions Right (C1W4L03)', '606': 'Why Deep Representations? (C1W4L04)', '607': 'Building Blocks of a Deep Neural Network (C1W4L05)', '608': 'Forward and Backward Propagation (C1W4L06)', '609': 'Parameters vs Hyperparameters (C1W4L07)', '610': 'What does this have to do with the brain? (C1W4L08)', '611': 'Train/Dev/Test Sets (C2W1L01)', '612': 'Bias/Variance (C2W1L02)', '613': 'Basic Recipe for Machine Learning (C2W1L03)', '614': 'Regularization (C2W1L04)', '615': 'Why Regularization Reduces Overfitting (C2W1L05)', '616': 'Dropout Regularization (C2W1L06)', '617': 'Understanding Dropout (C2W1L07)', '618': 'Other Regularization Methods (C2W1L08)', '619': 'Normalizing Inputs (C2W1L09)', '620': 'Vanishing/Exploding Gradients (C2W1L10)', '621': 'Weight Initialization in a Deep Network (C2W1L11)', '622': 'Numerical Approximations of Gradients (C2W1L12)', '623': 'Gradient Checking (C2W1L13)', '624': 'Gradient Checking Implementation Notes (C2W1L14)', '625': 'Mini Batch Gradient Descent (C2W2L01)', '626': 'Understanding Mini-Batch Gradient Dexcent (C2W2L02)', '627': 'Exponentially Weighted Averages (C2W2L03)', '628': 'Understanding Exponentially Weighted Averages (C2W2L04)', '629': 'Bias Correction of Exponentially Weighted Averages (C2W2L05)', '630': 'Gradient Descent With Momentum (C2W2L06)', '631': 'RMSProp (C2W2L07)', '632': 'Adam Optimization Algorithm (C2W2L08)', '633': 'Learning Rate Decay (C2W2L09)', '634': 'Tuning Process (C2W3L01)', '635': 'Using an Appropriate Scale (C2W3L02)', '636': 'Hyperparameter Tuning in Practice (C2W3L03)', '637': 'Normalizing Activations in a Network (C2W3L04)', '638': 'Fitting Batch Norm Into Neural Networks (C2W3L05)', '639': 'Why Does Batch Norm Work? (C2W3L06)', '640': 'Batch Norm At Test Time (C2W3L07)', '641': 'Softmax Regression (C2W3L08)', '642': 'Training Softmax Classifier (C2W3L09)', '643': 'The Problem of Local Optima (C2W3L10)', '644': 'TensorFlow (C2W3L11)', '645': 'Improving Model Performance (C3W1L01)', '646': 'Orthogonalization (C3W1L02 )', '647': 'Single Number Evaluation Metric (C3W1L03)', '648': 'Satisficing and Optimizing Metrics (C3W1L04)', '649': 'Train/Dev/Test Set Distributions (C3W1L05)', '650': 'Sizeof Dev and Test Sets (C3W1L06)', '651': 'When to Change Dev/Test Sets (C3W1L07)', '652': 'C3W1L08 WhyHumanLevelPerformance', '653': 'Avoidable Bias (C3W1L09)', '654': 'Understanding Human-Level Performance? (C3W1L10)', '655': 'Surpassing Human-Level Performance (C3W1L11)', '656': 'Improving Model Performance (C3W1L12)', '657': 'Carrying Out Error Analysis (C3W2L01)', '658': 'Cleaning Up Incorrectly Labelled Data (C3W2L02)', '659': 'Build First System Quickly, Then Iterate (C3W2L03)', '660': 'Training and Testing on Different Distributions (C3W2L04)', '661': 'Bias and Variance With Mismatched Data (C3W2L05)', '662': 'Addressing Data Mismatch (C3W2L06)', '663': 'Transfer Learning (C3W2L07)', '664': 'Multitask Learning (C3W2L08)', '665': 'What is end-to-end deep learning? (C3W2L09)', '666': 'Whether to Use End-To-End Deep Learning (C3W2L10)', '667': "deeplearning.ai's Heroes of Deep Learning: Ruslan Salakhutdinov", '668': "deeplearning.ai's Heroes of Deep Learning: Yuanqing Lin", '669': "deeplearning.ai's Heroes of Deep Learning: Ian Goodfellow", '670': "deeplearning.ai's Heroes of Deep Learning: Yoshua Bengio", '671': "deeplearning.ai's Heroes of Deep Learning: Pieter Abbeel", '672': "deeplearning.ai's Heroes of Deep Learning: Andrej Karpathy", '673': "deeplearning.ai's Heroes of Deep Learning: Yann LeCun", '674': "deeplearning.ai's Heroes of Deep Learning: Dawn Song", '675': 'Heroes of Deep Learning: Pieter Abbeel and Andrej Karpathy on Getting Started', '676': 'Heroes of Deep Learning: Andrew Ng interviews Geoffrey Hinton', '677': 'Heroes of Deep Learning: Yann LeCun and Ruslan Salakhutdinov on Getting Started', '678': 'Pie & AI Bogotá', '679': 'Pie & AI Medellín: A Discussion with Andrew Ng and Helmuth Trefftz', '680': 'Andrew Ng at Amazon re:MARS 2019', '681': 'Pie & AI: TensorFlow Specialization Launch @ Google HQ', '682': 'Andrew Ng - The State of Artificial Intelligence', '683': 'Andrew Ng: Deep Learning, Education, and Real-World AI | Lex Fridman Podcast #73', '684': 'Why Computer Vision is a Hard Problem (TensorFlow in Practice)', '685': 'What AI Can and Cannot Do (AI For Everyone)', '686': 'Training in the Browser (TensorFlow: Data and Deployment)', '687': 'Coming soon: a new deeplearning.ai Specialization', '688': 'How to pick AI projects (AI For Everyone)', '689': 'Four advanced deployment scenarios (TensorFlow: Data and Deployment)', '690': 'Augmenting Data (TensorFlow in Practice)', '691': 'New AI For Medicine Specialization coming soon!', '692': 'Practical Challenges Training with Medical Data (AI For Medicine)', '693': 'AI For Prognosis (AI For Medicine)', '694': 'How deep learning can detect cancerous tissue (AI For Medicine)', '695': 'C5W3L01 Basic Models', '696': 'C5W3L02 Picking the most likely sentence', '697': 'C5W3L06 Bleu Score (Optional)', '698': 'C5W3L07 Attention Model Intuition', '699': 'C5W3L08 Attention Model', '700': 'C5W3L09 SpeechRecog', '701': 'C4W1L01 Computer Vision', '702': 'C4W1L02 Edge Detection Examples', '703': 'C4W1L03 More Edge Detection', '704': 'C4W1L04 Padding', '705': 'C4W1L05 Strided Convolutions', '706': 'C4W1L06 Convolutions Over Volumes', '707': 'C4W1L07 One Layer of a Convolutional Net', '708': 'C4W1L08 Simple Convolutional Network Example', '709': 'C4W1L09 Pooling Layers', '710': 'C4W1L10 CNN Example', '711': 'C4W1L11 Why Convolutions', '712': 'C4W2L01 Why look at case studies?', '713': 'C4W2L02 Classic Network', '714': 'C4W2L03 Resnets', '715': 'C4W2L04 Why ResNets Work', '716': 'C4W2L05 Network In Network', '717': 'C4W2L06 Inception Network Motivation', '718': 'C4W2L07 Inception Network', '719': 'C4W2L08 Using Open Source Implementation', '720': 'C4W2L09 Transfer Learning', '721': 'C4W2L10 Data Augmentation', '722': 'C4W2L11 State of Computer Vision', '723': 'C4W3L01 Object Localization', '724': 'C4W3L02 Landmark Detection', '725': 'C4W3L03 Object Detection', '726': 'C4W3L04 Convolutional Implementation Sliding Windows', '727': 'C4W3L06 Intersection Over Union', '728': 'C4W3L07 Nonmax Suppression', '729': 'C4W3L08 Anchor Boxes', '730': 'C4W3L09 YOLO Algorithm', '731': 'C4W3L10 Region Proposals', '732': 'C4W4L01 What is face recognition', '733': 'C4W4L02 One Shot Learning', '734': 'C4W4L03 Siamese Network', '735': 'C4W4L04 Triplet loss', '736': 'C4W4L05 Face Verification', '737': 'C4W4L06 What is neural style transfer?', '738': 'C4W4L07 What are deep CNs learning?', '739': 'C4W4L08 Cost Function', '740': 'C4W4L09 Content Cost Function', '741': 'C4W4L10 Style Cost Function', '742': 'C4W4L11 1D and 3D Generalizations', '743': 'Heroes of NLP: Chris Manning', '744': 'Heroes of NLP: Kathleen McKeown', '745': 'Heroes of NLP: Oren Etzioni', '746': 'Heroes of NLP: Quoc Le', '747': '#1 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 1]', '748': '#2 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 2]', '749': '#3 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 3]', '750': '#4 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 4]', '751': '#5 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 5]', '752': '#6 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 6]', '753': '#7 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 7]', '754': '#8 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 8]', '755': '#9 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 1]', '756': '#10 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 2]', '757': '#11 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 3]', '758': '#12 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 4]', '759': '#13 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 5]', '760': '#14 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 6]', '761': '#15 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 7]', '762': '#16 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 8]', '763': '#17 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 9]', '764': '#18 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 10]', '765': '#19 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 11]', '766': '#20 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 12]', '767': '#21 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 13]', '768': '#22 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 14]', '769': '#23 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 15]', '770': '#24 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 2, Lesson 16]', '771': '#25 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 1]', '772': '#26 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3 Lesson 2]', '773': '#27 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 3]', '774': '#28 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 4]', '775': '#29 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 5]', '776': '#30 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 6]', '777': '#31 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 7]', '778': '#32 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 8]', '779': '#33 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 9]', '780': '#34 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 10]', '781': '#35 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 1, Lesson 11]', '782': '#36 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 12]', '783': '#37 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 13]', '784': '#38 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 14]', '785': '#39 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 15]', '786': '#40 Machine Learning Engineering for Production (MLOps) Specialization [Course 1, Week 3, Lesson 16]'}, 'description': {'0': '#ai #dqn #deepmind\n\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:50 - Arcade Learning Environment\n4:25 - Deep Reinforcement Learning\n9:20 - Deep Q-Learning\n26:30 - Experience Replay\n32:25 - Network Architecture\n33:50 - Experiments\n37:45 - Conclusion\n\nPaper: https://arxiv.org/abs/1312.5602\n\nAbstract:\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\n\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '1': '#ai #research #alexnet\n\nAlexNet was the start of the deep learning revolution. Up until 2012, the best computer vision systems relied on hand-crafted features and highly specialized algorithms to perform object classification. This paper was the first to successfully train a deep convolutional neural network on not one, but two GPUs and managed to outperform the competition on ImageNet by an order of magnitude.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:00 - The necessity of larger models\n6:20 - Why CNNs?\n11:05 - ImageNet\n12:05 - Model Architecture Overview\n14:35 - ReLU Nonlinearities\n18:45 - Multi-GPU training\n21:30 - Classification Results\n24:30 - Local Response Normalization\n28:05 - Overlapping Pooling\n32:25 - Data Augmentation\n38:30 - Dropout\n40:30 - More Results\n43:50 - Conclusion\n\nPaper: http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\n\nAbstract:\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n\nAuthors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '2': '#ai #deeplearning #gan\n\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:50 - Motivation\n8:40 - Minimax Loss Function\n13:20 - Intuition Behind the Loss\n19:30 - GAN Algorithm\n22:05 - Theoretical Analysis\n27:00 - Experiments\n33:10 - Advantages & Disadvantages\n35:00 - Conclusion\n\nPaper: https://arxiv.org/abs/1406.2661\n\nAbstract:\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '3': '#ai #research #word2vec\n\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\n\nOUTLINE:\n0:00 - Intro & Outline\n1:50 - Distributed Word Representations\n5:40 - Skip-Gram Model\n12:00 - Hierarchical Softmax\n14:55 - Negative Sampling\n22:30 - Mysterious 3/4 Power\n25:50 - Frequent Words Subsampling\n28:15 - Empirical Results\n29:45 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1310.4546\nCode: https://code.google.com/archive/p/word2vec/\n\nAbstract:\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n\nAuthors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '4': "#ai #research #resnet\n\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper's invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n1:45 - The Problem with Depth\n3:15 - VGG-Style Networks\n6:00 - Overfitting is Not the Problem\n7:25 - Motivation for Residual Connections\n10:25 - Residual Blocks\n12:10 - From VGG to ResNet\n18:50 - Experimental Results\n23:30 - Bottleneck Blocks\n24:40 - Deeper ResNets\n28:15 - More Results\n29:50 - Conclusion & Comments\n\nPaper: https://arxiv.org/abs/1512.03385\n\nAbstract:\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '5': "#machinelearning #phd #howto\n\nThis video is advice for new PhD students in the field of Machine Learning in 2021 and after. The field has shifted dramatically in the last few years and navigating grad school can be very hard, especially when you're as clueless as I was when I started. The video is a personal recount of my mistakes and what I've learned from them. If you already have several published papers and know what to do, this video is not for you. However, if you are not even sure where to start, how to select a topic, or what goes in a paper, you might benefit from this video, because that's exactly how I felt.\n\nMain Takeaways:\n- Select niche topics rather than hype topics\n- Write papers that can't be rejected\n- Don't be discouraged by bad reviews\n- Take reviewing & teaching seriously\n- Keep up your focus\n- Conferences are for networking\n- Internships are great opportunities\n- Team up with complementary skills\n- Don't work too hard\n\nOUTLINE:\n0:00 - Intro & Overview\n1:25 - Thesis Topic Selection\n4:25 - How To Publish Papers\n5:35 - Dealing With Reviewers\n6:30 - How To Be A Reviewer\n7:40 - Take Teaching Seriously\n8:30 - Maintain Focus\n10:20 - Navigating Conferences\n12:40 - Internships\n13:40 - Collaborations\n14:55 - Don't Forget To Enjoy\n\nTranscript: https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae\n\nCredits to Lanz for editing\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher\nParler: https://parler.com/profile/YannicKilcher\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\nBiliBili: https://space.bilibili.com/1824646584\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '6': '#lama #inpainting #deeplearning\n\nAt the end of the video is an interview with the paper authors!\nLaMa is a system that is amazing at removing foreground objects from images, especially when those objects cover a large part of the image itself. LaMa is specifically trained to reconstruct large masked areas and includes global information throughout its forward propagation by using Fourier Convolutions in its layers. This makes it incredibly effective at reconstructing periodic structures with long-range consistency, compared to regular convolutions.\n\nOUTLINE:\n0:00 - Intro\n0:45 - Sponsor: ClearML\n3:30 - Inpainting Examples\n5:05 - Live Demo\n6:40 - Locality as a weakness of convolutions\n10:30 - Using Fourier Transforms for global information\n12:55 - Model architecture overview\n14:35 - Fourier convolution layer\n21:15 - Loss function\n24:25 - Mask generation algorithm\n25:40 - Experimental results\n28:25 - Interview with the authors\n\nPaper: https://arxiv.org/abs/2109.07161\nCode: https://github.com/saic-mdal/lama\nOnline Demo: https://cleanup.pictures/\n\nSponsor: ClearML\nhttps://clear.ml\n\nAbstract:\nModern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter&time costs than the competitive baselines. The code is available at \\url{this https URL}.\n\nAuthors: Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '7': "#playerofgames #deepmind #alphazero\n\nSpecial Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\nGames have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\n\nOUTLINE:\n0:00 - Introduction\n2:50 - What games can Player of Games be trained on?\n4:00 - Tree search algorithms (AlphaZero)\n8:00 - What is different in imperfect information games?\n15:40 - Counterfactual Value- and Policy-Networks\n18:50 - The Player of Games search procedure\n28:30 - How to train the network?\n34:40 - Experimental Results\n47:20 - Discussion & Outlook\n\nPaper: https://arxiv.org/abs/2112.03178\n\nAbstract:\nGames have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.\n\nAuthors: Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '8': "#minerl #minecraft #deeplearning\n\nThe MineRL BASALT challenge has no reward functions or technical descriptions of what's to be achieved. Instead, the goal of each task is given as a short natural language string, and the agent is evaluated by a team of human judges who rate both how well the goal has been fulfilled, as well as how human-like the agent behaved. In this video, I interview KAIROS, the winning team of the 2021 challenge, and discuss how they used a combination of machine learning, efficient data collection, hand engineering, and a bit of knowledge about Minecraft to beat all other teams.\n\nOUTLINE:\n0:00 - Introduction\n4:10 - Paper Overview\n11:15 - Start of Interview\n17:05 - First Approach\n20:30 - State Machine\n26:45 - Efficient Label Collection\n30:00 - Navigation Policy\n38:15 - Odometry Estimation\n46:00 - Pain Points & Learnings\n50:40 - Live Run Commentary\n58:50 - What other tasks can be solved?\n1:01:55 - What made the difference?\n1:07:30 - Recommendations & Conclusion\n1:11:10 - Full Runs: Waterfall\n1:12:40 - Full Runs: Build House\n1:17:45 - Full Runs: Animal Pen\n1:20:50 - Full Runs: Find Cave\n\nPaper: https://arxiv.org/abs/2112.03482\nCode: https://github.com/viniciusguigo/kairos_minerl_basalt\nChallenge Website: https://minerl.io/basalt/\n\nPaper Title: Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft\n\nAbstract:\nReal-world tasks of interest are generally poorly defined by human-readable descriptions and have no pre-defined reward signals unless it is defined by a human designer. Conversely, data-driven algorithms are often designed to solve a specific, narrowly defined, task with performance metrics that drives the agent's learning. In this work, we present the solution that won first place and was awarded the most human-like agent in the 2021 NeurIPS Competition MineRL BASALT Challenge: Learning from Human Feedback in Minecraft, which challenged participants to use human data to solve four tasks defined only by a natural language description and no reward function. Our approach uses the available human demonstration data to train an imitation learning policy for navigation and additional human feedback to train an image classifier. These modules, together with an estimated odometry map, are then combined into a state-machine designed based on human knowledge of the tasks that breaks them down in a natural hierarchy and controls which macro behavior the learning agent should follow at any instant. We compare this hybrid intelligence approach to both end-to-end machine learning and pure engineered solutions, which are then judged by human evaluators. Codebase is available at this https URL.\n\nAuthors: Vinicius G. Goecks, Nicholas Waytowich, David Watkins, Bharat Prakash\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '9': "#deeplearning #noether #symmetries\n\nThis video includes an interview with first author Ferran Alet!\nEncoding inductive biases has been a long established methods to provide deep networks with the ability to learn from less data. Especially useful are encodings of symmetry properties of the data, such as the convolution's translation invariance. But such symmetries are often hard to program explicitly, and can only be encoded exactly when done in a direct fashion. Noether Networks use Noether's theorem connecting symmetries to conserved quantities and are able to dynamically and approximately enforce symmetry properties upon deep neural networks.\n\nOUTLINE:\n0:00 - Intro & Overview\n18:10 - Interview Start\n21:20 - Symmetry priors vs conserved quantities\n23:25 - Example: Pendulum\n27:45 - Noether Network Model Overview\n35:35 - Optimizing the Noether Loss\n41:00 - Is the computation graph stable?\n46:30 - Increasing the inference time computation\n48:45 - Why dynamically modify the model?\n55:30 - Experimental Results & Discussion\n\nPaper: https://arxiv.org/abs/2112.03321\nWebsite: https://dylandoblar.github.io/noether-networks/\nCode: https://github.com/dylandoblar/noether-networks\n\nAbstract:\nProgress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether's theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.\n\nAuthors: Ferran Alet, Dylan Doblar, Allan Zhou, Joshua Tenenbaum, Kenji Kawaguchi, Chelsea Finn\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '10': "#deeplearning #neuralinterpreter #ai\n\nThis video includes an interview with the paper's authors!\nWhat if we treated deep networks like modular programs? Neural Interpreters divide computation into small modules and route data to them via a dynamic type inference system. The resulting model combines recurrent elements, weight sharing, attention, and more to tackle both abstract reasoning, as well as computer vision tasks.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:00 - Model Overview\n7:00 - Interpreter weights and function code\n9:40 - Routing data to functions via neural type inference\n14:55 - ModLin layers\n18:25 - Experiments\n21:35 - Interview Start\n24:50 - General Model Structure\n30:10 - Function code and signature\n40:30 - Explaining Modulated Layers\n49:50 - A closer look at weight sharing\n58:30 - Experimental Results\n\nPaper: https://arxiv.org/abs/2110.06399\n\nGuests:\nNasim Rahaman: https://twitter.com/nasim_rahaman\nFrancesco Locatello: https://twitter.com/FrancescoLocat8\nWaleed Gondal: https://twitter.com/Wallii_gondal\n\nAbstract:\nModern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \\emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization\n\nAuthors: Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, Bernhard Schölkopf\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '11': "#deeplearning #symbolic #research\n\nThis video includes an interview with first author Stéphane d'Ascoli (https://sdascoli.github.io/).\nDeep neural networks are typically excellent at numeric regression, but using them for symbolic computation has largely been ignored so far. This paper uses transformers to do symbolic regression on integer and floating point number sequences, which means that given the start of a sequence of numbers, the model has to not only predict the correct continuation, but also predict the data generating formula behind the sequence. Through clever encoding of the input space and a well constructed training data generation process, this paper's model can learn and represent many of the sequences in the OEIS, the online encyclopedia of integer sequences and it also features an interactive demo if you want to try it by yourself. \n\nOUTLINE:\n0:00 - Introduction\n2:20 - Summary of the Paper\n16:10 - Start of Interview\n17:15 - Why this research direction?\n20:45 - Overview of the method\n30:10 - Embedding space of input tokens\n33:00 - Data generation process\n42:40 - Why are transformers useful here?\n46:40 - Beyond number sequences, where is this useful?\n48:45 - Success cases and failure cases\n58:10 - Experimental Results\n1:06:30 - How did you overcome difficulties?\n1:09:25 - Interactive demo\n\nPaper: https://arxiv.org/abs/2201.04600\nInteractive demo: https://symbolicregression.metademolab.com/\n\nAbstract:\nSymbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. bessel0(x)≈sin(x)+cos(x)πx√ and 1.644934≈π2/6. An interactive demonstration of our models is provided at this https URL.\n\nAuthors: Stéphane d'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, François Charton\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '12': "#eleuther #gptneo #gptj\n\nEleutherAI announces GPT-NeoX-20B, a 20 billion parameter open-source language model, inspired by GPT-3. Connor joins me to discuss the process of training, how the group got their hands on the necessary hardware, what the new model can do, and how anyone can try it out!\n\nOUTLINE:\n0:00 - Intro\n1:00 - Start of interview\n2:00 - How did you get all the hardware?\n3:50 - What's the scale of this model?\n6:00 - A look into the experimental results\n11:15 - Why are there GPT-Neo, GPT-J, and GPT-NeoX?\n14:15 - How difficult is training these big models?\n17:00 - Try out the model on GooseAI\n19:00 - Final thoughts\n\nRead the announcement: https://blog.eleuther.ai/announcing-20b/\nTry out the model: https://goose.ai/\nCheck out EleutherAI: https://www.eleuther.ai/\nRead the code: https://github.com/EleutherAI/gpt-neox\nHardware sponsor: https://www.coreweave.com/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '13': '#deeplearning #brain #neuroscience\n\nOriginally, Deep Learning sprang into existence inspired by how the brain processes information, but the two fields have diverged ever since. However, given that deep models can solve many perception tasks with remarkable accuracy, is it possible that we might be able to learn something about how the brain works by inspecting our models? I speak to Patrick Mineault about his blog post "2021 in review: unsupervised brain models" and we explore why neuroscientists are taking interest in unsupervised and self-supervised deep neural networks in order to explain how the brain works. We discuss a series of influential papers that have appeared last year, and we go into the more general questions of connecting neuroscience and machine learning.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:35 - Start of Interview\n10:30 - Visual processing in the brain\n12:50 - How does deep learning inform neuroscience?\n21:15 - Unsupervised training explains the ventral stream\n30:50 - Predicting own motion parameters explains the dorsal stream\n42:20 - Why are there two different visual streams?\n49:45 - Concept cells and representation learning\n56:20 - Challenging the manifold theory\n1:08:30 - What are current questions in the field?\n1:13:40 - Should the brain inform deep learning?\n1:18:50 - Neuromatch Academy and other endeavours\n\nBlog Post: https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/\nPatrick\'s Blog: https://xcorr.net/\nTwitter: https://twitter.com/patrickmineault\nNeuromatch Academy: https://academy.neuromatch.io/\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '14': '#gpt3 #embodied #planning\n\nIn this video: Paper explanation, followed by first author interview with Wenlong Huang.\nLarge language models contain extraordinary amounts of world knowledge that can be queried in various ways. But their output format is largely uncontrollable. This paper investigates the VirtualHome environment, which expects a particular set of actions, objects, and verbs to be used. Turns out, with proper techniques and only using pre-trained models (no fine-tuning), one can translate unstructured language model outputs into the structured grammar of the environment. This is potentially very useful anywhere where the models\' world knowledge needs to be provided in a particular structured format.\n\nOUTLINE:\n0:00 - Intro & Overview\n2:45 - The VirtualHome environment\n6:25 - The problem of plan evaluation\n8:40 - Contributions of this paper\n16:40 - Start of interview\n24:00 - How to use language models with environments?\n34:00 - What does model size matter?\n40:00 - How to fix the large models\' outputs?\n55:00 - Possible improvements to the translation procedure\n59:00 - Why does Codex perform so well?\n1:02:15 - Diving into experimental results\n1:14:15 - Future outlook\n\nPaper: https://arxiv.org/abs/2201.07207\nWebsite: https://wenlong.page/language-planner/\nCode: https://github.com/huangwl18/language-planner\nWenlong\'s Twitter: https://twitter.com/wenlong_huang\n\nAbstract:\nCan world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into low-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at this https URL\n\nAuthors: Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '15': '#hypertransformer #metalearning #deeplearning\n\nThis video contains a paper explanation and an interview with author Andrey Zhmoginov!\nFew-shot learning is an interesting sub-field in meta-learning, with wide applications, such as creating personalized models based on just a handful of data points. Traditionally, approaches have followed the BERT approach where a large model is pre-trained and then fine-tuned. However, this couples the size of the final model to the size of the model that has been pre-trained. Similar problems exist with "true" meta-learners, such as MaML. HyperTransformer fundamentally decouples the meta-learner from the size of the final model by directly predicting the weights of the final model. The HyperTransformer takes the few-shot dataset as a whole into its context and predicts either one or multiple layers of a (small) ConvNet, meaning its output are the weights of the convolution filters. Interestingly, and with the correct engineering care, this actually appears to deliver promising results and can be extended in many ways.\n\nOUTLINE:\n0:00 - Intro & Overview\n3:05 - Weight-generation vs Fine-tuning for few-shot learning\n10:10 - HyperTransformer model architecture overview\n22:30 - Why the self-attention mechanism is useful here\n34:45 - Start of Interview\n39:45 - Can neural networks even produce weights of other networks?\n47:00 - How complex does the computational graph get?\n49:45 - Why are transformers particularly good here?\n58:30 - What can the attention maps tell us about the algorithm?\n1:07:00 - How could we produce larger weights?\n1:09:30 - Diving into experimental results\n1:14:30 - What questions remain open?\n\nPaper: https://arxiv.org/abs/2201.04182\n\nERRATA: I introduce Max Vladymyrov as Mark Vladymyrov\n\nAbstract:\nIn this work we propose a HyperTransformer, a transformer-based model for few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable. Finally, we extend our approach to a semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance.\n\nAuthors: Andrey Zhmoginov, Mark Sandler, Max Vladymyrov\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '16': "#security #censorship #ai\n\nMost of us conceive the internet as a free and open space where we are able to send traffic between any two nodes, but for large parts of the world this is not the case. Entire nations have large machinery in place to survey all internet traffic and automated procedures to block any undesirable connections. Evading such censorship has been largely a cat-and-mouse game between security researchers and government actors. A new system, called Geneva, uses a Genetic Algorithm in combination with Evolutionary Search in order to dynamically evade such censorship and adjust itself in real-time to any potential response by its adversaries. In this video, I talk to Security researcher Kevin Bock, who is one of Geneva's main contributors and member of the Breakerspace project. We talk about the evolution of internet censorship, how to evade it, how to mess with the censors' infrastructure, as well as the broader emerging connections between AI and Security.\n\nOUTLINE:\n0:00 - Intro\n3:30 - What is automated censorship in networks?\n7:20 - The evolution of censorship vs evasion\n12:40 - Why do we need a dynamic, evolving system?\n16:30 - The building blocks of Geneva\n23:15 - Introducing evolution\n28:30 - What's the censors' response?\n31:45 - How was Geneva's media reception?\n33:15 - Where do we go from here?\n37:30 - Can we deliberately attack the censors?\n47:00 - On responsible disclosure\n49:40 - Breakerspace: Security research for undergrads\n50:40 - How often do you get into trouble?\n52:10 - How can I get started in security?\n\nLearn more at:\n- Geneva (& more) project page: https://censorship.ai\n- Open Observatory of Network Interference: https://ooni.org\n- Censored Planet: https://censoredplanet.org\n- Breakerspace: https://breakerspace.cs.umd.edu\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '17': "#cm3 #languagemodel #transformer\n\nThis video contains a paper explanation and an incredibly informative interview with first author Armen Aghajanyan.\nAutoregressive Transformers have come to dominate many fields in Machine Learning, from text generation to image creation and many more. However, there are two problems. First, the collected data is usually scraped from the web and uni- or bi-modal and throws away a lot of structure of the original websites, and second, language modelling losses are uni-directional. CM3 addresses both problems: It directly operates on HTML and includes text, hyperlinks, and even images (via VQGAN tokenization) and can therefore be used in plenty of ways: Text generation, captioning, image creation, entity linking, and much more. It also introduces a new training strategy called Causally Masked Language Modelling, which brings a level of bi-directionality into autoregressive language modelling. In the interview after the paper explanation, Armen and I go deep into the how and why of these giant models, we go over the stunning results and we make sense of what they mean for the future of universal models.\n\nOUTLINE:\n0:00 - Intro & Overview\n6:30 - Directly learning the structure of HTML\n12:30 - Causally Masked Language Modelling\n18:50 - A short look at how to use this model\n23:20 - Start of interview\n25:30 - Feeding language models with HTML\n29:45 - How to get bi-directionality into decoder-only Transformers?\n37:00 - Images are just tokens\n41:15 - How does one train such giant models?\n45:40 - CM3 results are amazing\n58:20 - Large-scale dataset collection and content filtering\n1:04:40 - More experimental results\n1:12:15 - Why don't we use raw HTML?\n1:18:20 - Does this paper contain too many things?\n\nPaper: https://arxiv.org/abs/2201.07520\n\nAbstract:\nWe introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.\n\nAuthors: Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer\n\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '18': '#ai #gpu #tpu\n\nThis video is an interview with Adi Fuchs, author of a series called "AI Accelerators", and an expert in modern AI acceleration technology.\nAccelerators like GPUs and TPUs are an integral part of today\'s AI landscape. Deep Neural Network training can be sped up by orders of magnitudes by making good use of these specialized pieces of hardware. However, GPUs and TPUs are only the beginning of a vast landscape of emerging technologies and companies that build accelerators for the next generation of AI models. In this interview, we go over many aspects of building hardware for AI, including why GPUs have been so successful, what the most promising approaches look like, how they work, and what the main challenges are.\n\nOUTLINE:\n0:00 - Intro\n5:10 - What does it mean to make hardware for AI?\n8:20 - Why were GPUs so successful?\n16:25 - What is "dark silicon"?\n20:00 - Beyond GPUs: How can we get even faster AI compute?\n28:00 - A look at today\'s accelerator landscape\n30:00 - Systolic Arrays and VLIW\n35:30 - Reconfigurable dataflow hardware\n40:50 - The failure of Wave Computing\n42:30 - What is near-memory compute?\n46:50 - Optical and Neuromorphic Computing\n49:50 - Hardware as enabler and limiter\n55:20 - Everything old is new again\n1:00:00 - Where to go to dive deeper?\n\nRead the full blog series here:\nPart I: https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4\nPart II: https://medium.com/@adi.fu7/ai-accelerators-part-ii-transistors-and-pizza-or-why-do-we-need-accelerators-75738642fdaa\nPart III: https://medium.com/@adi.fu7/ai-accelerators-part-iii-architectural-foundations-3f1f73d61f1f\nPart  IV: https://medium.com/@adi.fu7/ai-accelerators-part-iv-the-very-rich-landscape-17481be80917\nPart V: https://medium.com/@adi.fu7/ai-accelerators-part-v-final-thoughts-94eae9dbfafb\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '19': '#wikipedia #reinforcementlearning #languagemodels\n\nTransformers have come to overtake many domain-targeted custom models in a wide variety of fields, such as Natural Language Processing, Computer Vision, Generative Modelling, and recently also Reinforcement Learning. This paper looks at the Decision Transformer and shows that, surprisingly, pre-training the model on a language-modelling task significantly boosts its performance on Offline Reinforcement Learning. The resulting model achieves higher scores, can get away with less parameters, and exhibits superior scaling properties. This raises many questions about the fundamental connection between the domains of language and RL.\n\nOUTLINE:\n0:00 - Intro\n1:35 - Paper Overview\n7:35 - Offline Reinforcement Learning as Sequence Modelling\n12:00 - Input Embedding Alignment & other additions\n16:50 - Main experimental results\n20:45 - Analysis of the attention patterns across models\n32:25 - More experimental results (scaling properties, ablations, etc.)\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '20': "#wikipedia #reinforcementlearning #languagemodels\n\nOriginal paper review here: https://youtu.be/XHGh19Hbx48\n\nMachel Reid and Yutaro Yamada join me to discuss their recent paper on langauge model pre-training for decision transformers in offline reinforcement learning.\n\nOUTLINE:\n0:00 - Intro\n1:00 - Brief paper, setup & idea recap\n7:30 - Main experimental results & high standard deviations\n10:00 - Why is there no clear winner?\n13:00 - Why are bigger models not a lot better?\n14:30 - What’s behind the name ChibiT?\n15:30 - Why is iGPT underperforming?\n19:15 - How are tokens distributed in Reinforcement Learning?\n22:00 - What other domains could have good properties to transfer?\n24:20 - A deeper dive into the models' attention patterns\n33:30 - Codebase, model sizes, and compute requirements\n37:30 - Scaling behavior of pre-trained models\n40:05 - What did not work out in this project?\n42:00 - How can people get started and where to go next?\n\nPaper: https://arxiv.org/abs/2201.12122\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\n\nAbstract:\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\n\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '21': "#ai #alphacode #deepmind\n\nAn interview with the creators of AlphaCode!\nPaper review video here: https://youtu.be/s9UAOmyah1A\n\nOUTLINE:\n0:00 - Intro\n1:10 - Media Reception\n5:10 - How did the project go from start to finish?\n9:15 - Does the model understand its own code?\n14:45 - Are there plans to reduce the number of samples?\n16:15 - Could one do smarter filtering of samples?\n18:55 - How crucial are the public test cases?\n21:55 - Could we imagine an adversarial method?\n24:45 - How are coding problems even made?\n27:40 - Does AlphaCode evaluate a solution's asymptotic complexity?\n33:15 - Are our sampling procedures inappropriate for diversity?\n36:30 - Are all generated solutions as instructive as the example?\n41:30 - How are synthetic examples created during training?\n42:30 - What were high and low points during this research?\n45:25 - What was the most valid criticism after publication?\n47:40 - What are applications in the real world?\n51:00 - Where do we go from here?\n\nPaper: https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\nCode: https://github.com/deepmind/code_contests\n\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. Evaluated on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in programming competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\n\nAuthors: Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu and Oriol Vinyals\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '22': '#openai #math #imo\n\nThis is an interview with Stanislas Polu, research engineer at OpenAI and first author of the paper "Formal Mathematics Statement Curriculum Learning".\nWatch the paper review here: https://youtu.be/lvYVuOmUVs8\n\nOUTLINE:\n0:00 - Intro\n2:00 - How do you explain the big public reaction?\n4:00 - What\'s the history behind the paper?\n6:15 - How does algorithmic formal math work?\n13:10 - How does expert iteration replace self-play?\n22:30 - How is the language model trained and used?\n30:50 - Why is every model fine-tuned on the initial state?\n33:05 - What if we want to prove something we don\'t know already?\n40:35 - How can machines and humans work together?\n43:40 - Aren\'t most produced statements useless?\n46:20 - A deeper look at the experimental results\n50:10 - What were the high and low points during the research?\n54:25 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2202.01344\nminiF2F benchmark: https://github.com/openai/miniF2F\nFollow Stan here: https://twitter.com/spolu\n\nAbstract:\nWe explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.\n\nAuthors: Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '23': '#deepmind #rl #society\n\nThis is an in-depth paper review, followed by an interview with the papers\' authors!\nSociety is ruled by norms, and most of these norms are very useful, such as washing your hands before cooking. However, there also exist plenty of social norms which are essentially arbitrary, such as what hairstyles are acceptable, or what words are rude. These are called "silly rules". This paper uses multi-agent reinforcement learning to investigate why such silly rules exist. Their results indicate a plausible mechanism, by which the existence of silly rules drastically speeds up the agents\' acquisition of the skill of enforcing rules, which generalizes well, and therefore a society that has silly rules will be better at enforcing rules in general, leading to faster adaptation in the face of genuinely useful norms.\n\nOUTLINE:\n0:00 - Intro\n3:00 - Paper Overview\n5:20 - Why are some social norms arbitrary?\n11:50 - Reinforcement learning environment setup\n20:00 - What happens if we introduce a "silly" rule?\n25:00 - Experimental Results: how silly rules help society\n30:10 - Isolated probing experiments\n34:30 - Discussion of the results\n37:30 - Start of Interview\n39:30 - Where does the research idea come from?\n44:00 - What is the purpose behind this research?\n49:20 - Short recap of the mechanics of the environment\n53:00 - How much does such a closed system tell us about the real world?\n56:00 - What do the results tell us about silly rules?\n1:01:00 - What are these agents really learning?\n1:08:00 - How many silly rules are optimal?\n1:11:30 - Why do you have separate weights for each agent?\n1:13:45 - What features could be added next?\n1:16:00 - How sensitive is the system to hyperparameters?\n1:17:20 - How to avoid confirmation bias?\n1:23:15 - How does this play into progress towards AGI?\n1:29:30 - Can we make real-world recommendations based on this?\n1:32:50 - Where do we go from here?\n\nPaper: https://www.pnas.org/doi/10.1073/pnas.2106028118\nBlog: https://deepmind.com/research/publications/2021/Spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents\n\nAbstract:\nThe fact that humans enforce and comply with norms is an important reason why humans enjoy higher levels of cooperation and welfare than other animals. Some norms are relatively easy to explain; they may prohibit obviously harmful or uncooperative actions. But many norms are not easy to explain. For example, most cultures prohibit eating certain kinds of foods and almost all societies have rules about what constitutes appropriate clothing, language, and gestures. Using a computational model focused on learning shows that apparently pointless rules can have an indirect effect on welfare. They can help agents learn how to enforce and comply with norms in general, improving the group’s ability to enforce norms that have a direct effect on welfare.\n\nAuthors: Raphael Köster, Dylan Hadfield-Menell, Richard Everett, Laura Weidinger, Gillian K. Hadfield, Joel Z. Leibo\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '24': '#deeplearning #objectdetection #outliers\n\nAn interview with the authors of "Virtual Outlier Synthesis".\nWatch the paper review video here: https://youtu.be/i-J4T3uLC9M\n\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\n\nOUTLINE:\n0:00 - Intro\n2:20 - What was the motivation behind this paper?\n5:30 - Why object detection?\n11:05 - What\'s the connection to energy-based models?\n12:15 - Is a Gaussian mixture model appropriate for high-dimensional data?\n16:15 - What are the most important components of the method?\n18:30 - What are the downstream effects of the regularizer?\n22:00 - Are there severe trade-offs to outlier detection?\n23:55 - Main experimental takeaways?\n26:10 - Why do outlier detection in the last layer?\n30:20 - What does it take to finish a research projects successfully?\n\nPaper: https://arxiv.org/abs/2202.01197\nCode: https://github.com/deeplearning-wisc/vos\n\nAbstract:\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model\'s decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\n\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '25': "#multitasklearning #biology #neuralnetworks\n\nThis is an interview with the paper's authors: Abhiram Iyer, Karan Grewal, and Akash Velu!\nPaper Review Video: https://youtu.be/O_dJ31T01i8\n\nCheck out Zak's course on Graph Neural Networks (discount with this link): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\n\nOUTLINE:\n0:00 - Intro\n0:55 - Sponsor: GNN Course\n2:30 - How did the idea come to be?\n7:05 - What roles do the different parts of the method play?\n8:50 - What was missing in the paper review?\n10:35 - Are biological concepts viable if we still have backprop?\n11:50 - How many dendrites are necessary?\n14:10 - Why is there a plateau in the sparsity plot?\n20:50 - How does task difficulty play into the algorithm?\n24:10 - Why are there different setups in the experiments?\n30:00 - Is there a place for unsupervised pre-training?\n32:50 - How can we apply the online prototyping to more difficult tasks?\n37:00 - What did not work out during the project?\n41:30 - How do you debug a project like this?\n47:10 - How is this related to other architectures?\n51:10 - What other things from neuroscience are to be included?\n55:50 - Don't miss the awesome ending :)\n\nPaper: https://arxiv.org/abs/2201.00042\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\n\nLink to the GNN course (with discount): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nAbstract:\nA key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\n\nAuthors: Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '26': "#blip #interview #salesforce\n\nPaper Review Video: https://youtu.be/X2k7n4FuI7c\nSponsor: Assembly AI\nhttps://www.assemblyai.com/?utm_source=youtube&utm_medium=social&utm_campaign=yannic2\n\nThis is an interview with Junnan Li and Dongxu Li, authors of BLIP and members of Salesforce research.\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\n\nOUTLINE:\n0:00 - Intro\n0:40 - Sponsor: Assembly AI\n1:30 - Start of Interview\n2:30 - What's the pitch?\n4:40 - How did data bootstrapping come into the project?\n7:10 - How big of a problem is data quality?\n11:10 - Are the captioning & filtering models biased towards COCO data?\n14:40 - Could the data bootstrapping be done multiple times?\n16:20 - What was the evolution of the BLIP architecture?\n21:15 - Are there additional benefits to adding language modelling?\n23:50 - Can we imagine a modular future for pre-training?\n29:45 - Diving into the experimental results\n42:40 - What did and did not work out during the research?\n45:00 - How is research life at Salesforce?\n46:45 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2201.12086\nCode: https://github.com/salesforce/BLIP\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\n\nAbstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\n\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '27': '#deeplearning #nlp #sampling\n\nThis is an interview with first author Clara Meister.\nPaper review video hereé https://youtu.be/_EDr3ryrT_Y\n\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\n\nSponsor: Introduction to Graph Neural Networks Course\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES&affcode=999036_lzknae-d\n\nOUTLINE:\n0:00 - Intro\n0:35 - Sponsor: Introduction to GNNs Course (link in description)\n1:30 - Why does sampling matter?\n5:40 - What is a "typical" message?\n8:35 - How do humans communicate?\n10:25 - Why don\'t we just sample from the model\'s distribution?\n15:30 - What happens if we condition on the information to transmit?\n17:35 - Does typical sampling really represent human outputs?\n20:55 - What do the plots mean?\n31:00 - Diving into the experimental results\n39:15 - Are our training objectives wrong?\n41:30 - Comparing typical sampling to top-k and nucleus sampling\n44:50 - Explaining arbitrary engineering choices\n47:20 - How can people get started with this?\n\nPaper: https://arxiv.org/abs/2202.00666\nCode: https://github.com/cimeister/typical-sampling/blob/3e676cfd88fa2e6a24f2bdc6f9f07fddb87827c2/src/transformers/generation_logits_process.py#L242-L272\n\nAbstract:\nDespite achieving incredibly low perplexities on myriad natural language corpora, today\'s language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (à la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.\n\nAuthors: Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '28': '#nlp #gpt3 #prompt\n\nThis is an interview with the authors of this work, Aman Madaan and Niket Tandon.\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\n\nOUTLINE:\n0:00 - Intro\n0:45 - Paper Overview\n2:00 - What was your original motivation?\n4:20 - There is an updated version of the paper!\n9:00 - Have you studied this on real-world users?\n12:10 - How does model size play into providing feedback?\n14:10 - Can this be used for personalization?\n16:30 - Discussing experimental results\n17:45 - Can this be paired with recommender systems?\n20:00 - What are obvious next steps to make the system more powerful?\n23:15 - Clarifying the baseline methods\n26:30 - Exploring cross-lingual customization\n31:00 - Where did the idea for the clarification prompt come from?\n33:05 - What did not work out during this project?\n34:45 - What did you learn about interacting with large models?\n37:30 - Final thoughts\n\nPaper: https://arxiv.org/abs/2201.06009\nCode & Data: https://github.com/madaan/memprompt\n\nAbstract:\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret "What word is similar to good?" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user\'s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. All the code and data is available at this https URL.\n\nAuthors: Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '29': "#reinforcementlearning #ai #explained\n\nThis is an interview with Jesse Mu, first author of the paper.\nOriginal Paper Review: https://youtu.be/NeGJAUSQEJI\n\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\n\nOUTLINE:\n0:00 - Intro\n0:55 - Paper Overview\n4:30 - Aren't you just adding extra data?\n9:35 - Why are you splitting up the AMIGo teacher?\n13:10 - How do you train the grounding network?\n16:05 - What about causally structured environments?\n17:30 - Highlights of the experimental results\n20:40 - Why is there so much variance?\n22:55 - How much does it matter that we are testing in a video game?\n27:00 - How does novelty interface with the goal specification?\n30:20 - The fundamental problems of exploration\n32:15 - Are these algorithms subject to catastrophic forgetting?\n34:45 - What current models could bring language to other environments?\n40:30 - What does it take in terms of hardware?\n43:00 - What problems did you encounter during the project?\n46:40 - Where do we go from here?\n\nPaper: https://arxiv.org/abs/2202.08938\n\nAbstract:\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\n\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rocktäschel, Edward Grefenstette\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '30': '#aiart #deeplearning #clip\n\nSince the release of CLIP, the world of AI art has seen an unprecedented level of acceleration in what\'s possible to do. Whereas image generation had previously been mostly in the domain of scientists, now a community of professional artists, researchers, and amateurs are sending around colab notebooks and sharing their creations via social media. How did this happen? What is going on? And where do we go from here? Jack Morris and I attempt to answer some of these questions, following his blog post "The Weird and Wonderful World of AI Art" (linked below).\n\nOUTLINE:\n0:00 - Intro\n2:30 - How does one get into AI art?\n5:00 - Deep Dream & Style Transfer: the early days of art in deep learning\n10:50 - The advent of GANs, ArtBreeder and TikTok\n19:50 - Lacking control: Pre-CLIP art\n22:40 - CLIP & DALL-E\n30:20 - The shift to shared colabs\n34:20 - Guided diffusion models\n37:20 - Prompt engineering for art models\n43:30 - GLIDE\n47:00 - Video production & Disco Diffusion\n48:40 - Economics, money, and NFTs\n54:15 - What does the future hold for AI art?\n\nBlog post: https://jxmo.notion.site/The-Weird-and-Wonderful-World-of-AI-Art-b9615a2e7278435b98380ff81ae1cf09\nJack\'s Blog: https://jxmo.io/\n\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '31': "#neuralsearch #interview #google\n\nThis is an interview with the authors Yi Tay and Don Metzler.\nPaper Review Video: https://youtu.be/qlB0TPBQ7YY\n\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model's weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\n\nOUTLINE:\n0:00 - Intro\n0:50 - Start of Interview\n1:30 - How did this idea start?\n4:30 - How does memorization play into this?\n5:50 - Why did you not compare to cross-encoders?\n7:50 - Instead of the ID, could one reproduce the document itself?\n10:50 - Passages vs documents\n12:00 - Where can this model be applied?\n14:25 - Can we make this work on large collections?\n19:20 - What's up with the NQ100K dataset?\n23:55 - What is going on inside these models?\n28:30 - What's the smallest scale to obtain meaningful results?\n30:15 - Investigating the document identifiers\n34:45 - What's the end goal?\n38:40 - What are the hardest problems currently?\n40:40 - Final comments & how to get started\n\nPaper: https://arxiv.org/abs/2202.06991\n\nAbstract:\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '32': '#nlp #sparsity #transformers\n\nThis video is an interview with Barret Zoph and William Fedus of Google Brain about Sparse Expert Models.\nSparse Expert models have been hugely successful at distributing parts of models, mostly Transformers, across large array of machines and use a routing function to effectively route signals between them. This means that even though these models have a huge number of parameters, the computational load for a given signal does not increase because the model is only sparsely activated. Sparse expert models, such as Switch Transformers and GLAM can scale up to trillions of parameters and bring a number of desirable properties. We discuss everything from the fundamentals, history, strengths and weaknesses, up to the current state of the art of these models.\n\nOUTLINE:\n0:00 - Intro\n0:30 - What are sparse expert models?\n4:25 - Start of Interview\n5:55 - What do you mean by sparse experts?\n8:10 - How does routing work in these models?\n12:10 - What is the history of sparse experts?\n14:45 - What does an individual expert learn?\n19:25 - When are these models appropriate?\n22:30 - How comparable are sparse to dense models?\n26:30 - How does the pathways system connect to this?\n28:45 - What improvements did GLAM make?\n31:30 - The "designing sparse experts" paper\n37:45 - Can experts be frozen during training?\n41:20 - Can the routing function be improved?\n47:15 - Can experts be distributed beyond data centers?\n50:20 - Are there sparse experts for other domains than NLP?\n52:15 - Are sparse and dense models in competition?\n53:35 - Where do we go from here?\n56:30 - How can people get started with this?\n\nPapers:\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (https://arxiv.org/abs/2101.03961)\nGLaM: Efficient Scaling of Language Models with Mixture-of-Experts (https://arxiv.org/abs/2112.06905)\nDesigning Effective Sparse Expert Models (https://arxiv.org/abs/2202.08906)\n\nLinks:\nMerch: http://store.ykilcher.com\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '33': "#laion #clip #dalle\n\nLAION-5B is an open, free dataset consisting of over 5 billion image-text-pairs. Today's video is an interview with three of its creators. We dive into the mechanics and challenges of operating at such large scale, how to keep cost low, what new possibilities are enabled with open datasets like this, and how to best handle safety and legal concerns.\n\nOUTLINE:\n0:00 - Intro\n1:30 - Start of Interview\n2:30 - What is LAION?\n11:10 - What are the effects of CLIP filtering?\n16:40 - How big is this dataset?\n19:05 - Does the text always come from the alt-property?\n22:45 - What does it take to work at scale?\n25:50 -When will we replicate DALL-E?\n31:30 - The surprisingly efficient pipeline\n35:20 - How do you cover the S3 costs?\n40:30 - Addressing safety & legal concerns\n55:15 - Where can people get started?\n\nReferences:\nLAION website: https://laion.ai/\nLAION Discord: https://discord.com/invite/mVcgxMPD7e\nLAION-5B: https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/\nimg2dataset tool: https://github.com/rom1504/img2dataset\nLAION-400M: https://paperswithcode.com/dataset/laion-400m\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '34': "#ai #accel #evolution\n\nThis is an interview with the authors Jack Parker-Holder and Minqi Jiang.\nOriginal Paper Review Video: https://www.youtube.com/watch?v=povBDxUn1VQ\n\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\n\nOUTLINE:\n0:00 - Intro\n1:00 - Start of interview\n4:45 - How did you get into this field?\n8:10 - What is minimax regret?\n11:45 - What levels does the regret objective select?\n14:20 - Positive value loss (correcting my mistakes)\n21:05 - Why is the teacher not learned?\n24:45 - How much domain-specific knowledge is needed?\n29:30 - What problems is this applicable to?\n33:15 - Single agent vs population of agents\n37:25 - Measuring and balancing level difficulty\n40:35 - How does generalization emerge?\n42:50 - Diving deeper into the experimental results\n47:00 - What are the unsolved challenges in the field?\n50:00 - Where do we go from here?\n\nWebsite: https://accelagent.github.io\nPaper: https://arxiv.org/abs/2203.01302\nICLR Workshop: https://sites.google.com/view/aloe2022\nBook on topic: https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/\n\nAbstract:\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\n\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rocktäschel\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '35': '#saycan #robots #ai\n\nThis is an interview with the authors Brian Ichter, Karol Hausman, and Fei Xia.\nOriginal Paper Review Video: https://youtu.be/Ru23eWAQ6_E\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy\'s ability to progress towards the goal, given by the language model, and the policy\'s probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\n\nOUTLINE:\n0:00 - Introduction & Setup\n3:40 - Acquiring atomic low-level skills\n7:45 - How does the language model come in?\n11:45 - Why are you scoring instead of generating?\n15:20 - How do you deal with ambiguity in language?\n20:00 - The whole system is modular\n22:15 - Going over the full algorithm\n23:20 - What if an action fails?\n24:30 - Debunking a marketing video :)\n27:25 - Experimental Results\n32:50 - The insane scale of data collection\n40:15 - How do you go about large-scale projects?\n43:20 - Where did things go wrong?\n45:15 - Where do we go from here?\n52:00 - What is the largest unsolved problem in this?\n53:35 - Thoughts on the Tesla Bot\n55:00 - Final thoughts\n\nPaper: https://arxiv.org/abs/2204.01691\nWebsite: https://say-can.github.io/\n\nAbstract:\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model\'s "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project\'s website and the video can be found at this https URL\n\nAuthors: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '36': "#ai #selforganization #emergence\n\nRead Sebastian's article here: https://sebastianrisi.com/self_assembling_ai/\n\nOUTLINE:\n0:00 - Introduction\n2:25 - Start of Interview\n4:00 - The intelligence of swarms\n9:15 - The game of life & neural cellular automata\n14:10 - What's missing from neural CAs?\n17:20 - How does local computation compare to centralized computation?\n25:40 - Applications beyond games and graphics\n33:00 - Can we do away with goals?\n35:30 - Where do these methods shine?\n43:30 - The paradox of scales & brains\n49:45 - Connections to graphical systems & GNNs\n51:30 - Could this solve ARC?\n57:45 - Where can people get started?\n\nReferences:\nhttps://sebastianrisi.com/\nhttps://modl.ai/\nhttps://sebastianrisi.com/self_assembling_ai/\nhttps://twitter.com/risi1979/status/1519053654921293827?cxt=HHwWhsC9hYfQ4ZQqAAAA\nhttps://distill.pub/2020/growing-ca/\nhttps://arxiv.org/abs/2201.12360?source=techstories.org\nhttps://distill.pub/2020/selforg/mnist/\nhttps://arxiv.org/pdf/2204.11674.pdf\nhttps://github.com/fchollet/ARC\nhttps://github.com/volotat/ARC-Game\nhttp://animalaiolympics.com/AAI/\nhttps://www.deepmind.com/publications/alchemy-a-structured-task-distribution-for-meta-reinforcement-learning-f\nhttps://melaniemitchell.me/BooksContent/CAGTReviews.html \n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '37': '#ai #interview #research \n\nJacob Steinhardt believes that future AI systems will be qualitatively different than the ones we know currently. We talk about how emergence happens when scaling up, what implications that has on AI Safety, and why thought experiments like the Paperclip Maximizer might be more useful than most people think.\n\nOUTLINE:\n0:00 Introduction\n1:10 Start of Interview\n2:10 Blog posts series\n3:56 More Is Different for AI (Blog Post)\n7:40 Do you think this emergence is mainly a property from the interaction of things?\n9:17 How does phase transition or scaling-up play into AI and Machine Learning?\n12:10 GPT-3 as an example of qualitative difference in scaling up\n14:08 GPT-3 as an emergent phenomenon in context learning\n15:58 Brief introduction of different viewpoints on the future of AI and its alignment\n18:51 How does the phenomenon of emergence play into this game between the Engineering and the Philosophy viewpoint? \n22:41 Paperclip Maximizer on AI safety and alignment\n31:37 Thought Experiments\n37:34 Imitative Deception\n39:30 TruthfulQA: Measuring How Models Mimic Human Falsehoods (Paper)\n42:24 ML Systems Will Have Weird Failure Models (Blog Post)\n51:10 Is there any work to get a system to be deceptive?\n54:37 Empirical Findings Generalize Surprisingly Far (Blog Post)\n1:00:18 What would you recommend to guarantee better AI alignment or safety?\n1:05:13 Remarks\n\nReferences:\nhttps://bounded-regret.ghost.io/more-is-different-for-ai/\nhttps://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit#heading=h.n1wk9bxo847o\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '38': "#glide #openai #diffusion\n\nDiffusion models learn to iteratively reverse a noising process that is applied repeatedly during training. The result can be used for conditional generation as well as various other tasks such as inpainting. OpenAI's GLIDE builds on recent advances in diffusion models and combines text-conditional diffusion with classifier-free guidance and upsampling to achieve unprecedented quality in text-to-image samples.\n\nTry it yourself: https://huggingface.co/spaces/valhalla/glide-text2im\n\nOUTLINE:\n0:00 - Intro & Overview\n6:10 - What is a Diffusion Model?\n18:20 - Conditional Generation and Guided Diffusion\n31:30 - Architecture Recap\n34:05 - Training & Result metrics\n36:55 - Failure cases & my own results\n39:45 - Safety considerations\n\nPaper: https://arxiv.org/abs/2112.10741\nCode & Model: https://github.com/openai/glide-text2im\n\nMore diffusion papers:\nhttps://arxiv.org/pdf/2006.11239.pdf\nhttps://arxiv.org/pdf/2102.09672.pdf\n\nAbstract:\nDiffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at this https URL.\n\nAuthors: Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen\n\nLinks:\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://discord.gg/4H8xxDF\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nLinkedIn: https://www.linkedin.com/in/ykilcher\nBiliBili: https://space.bilibili.com/2017636191\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '39': "#parti #ai #aiart \n\nParti is a new autoregressive text-to-image model that shows just how much scale can achieve. This model's outputs are crips, accurate, realistic, and can combine arbitrary styles, concepts, and fulfil even challenging requests.\n\nOUTLINE:\n0:00 - Introduction\n2:40 - Example Outputs\n6:00 - Model Architecture\n17:15 - Datasets (incl. PartiPrompts)\n21:45 - Experimental Results\n27:00 - Picking a cherry tree\n29:30 - Failure cases\n33:20 - Final comments\n\nWebsite: https://parti.research.google/\nPaper: https://arxiv.org/abs/2206.10789\nGithub: https://github.com/google-research/parti\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '40': "#mlnews #dalle #imagen \n\nAll things text-to-image models like DALL-E and Imagen!\n\nOUTLINE:\n0:00 - Intro\n0:30 - Imagen: Google's Text-to-Image Diffusion Model\n7:15 - Unified I/O by AllenAI\n9:40 - CogView2 is Open-Source\n11:05 - Google bans DeepFakes from Colab\n13:05 - DALL-E generates real Cosmopolitan cover\n15:45 - DALL-E tips & tricks\n17:00 - Midjourney moves to Open Beta\n17:50 - DALLE-mini is not Crayon\n19:00 - Deep Learning Resources\n\nAMENDMENTS:\nThe Unified-IO paper is here: https://arxiv.org/abs/2206.08916\n\nReferences:\nImagen: Google's Text-to-Image Diffusion Model\nhttps://imagen.research.google/?utm_source=pocket_mylist\nhttps://arxiv.org/pdf/2205.11487.pdf\n\nUnified I/O by AllenAI\nhttps://unified-io.allenai.org/\nhttps://blog.allenai.org/introducing-ai2s-unified-io-9c0ec7fe1e43\n\nCogView2 is Open-Source\nhttps://github.com/THUDM/CogView2\nfile:///Users/yk/Downloads/big.1.pdf\nhttps://huggingface.co/spaces/THUDM/CogView2\nhttps://arxiv.org/pdf/2204.14217.pdf\n\nGoogle bans DeepFakes from Colab\nhttps://www-vice-com.cdn.ampproject.org/c/s/www.vice.com/amp/en/article/v7v4gx/google-bans-deepfakes-from-its-machine-learning-platform?utm_source=pocket_mylist\n\nDALL-E generates real Cosmopolitan cover\nhttps://www.cosmopolitan.com/lifestyle/a40314356/dall-e-2-artificial-intelligence-cover/\nhttps://www.instagram.com/p/CfEwohiJdXW/?hl=en\n\nDALL-E tips & tricks\nhttps://twitter.com/GuyP/status/1544710725708513280?s=09&t=c3NpErPx80INQVeaWkIqIg&utm_source=pocket_mylist\nhttps://twitter.com/GuyP/status/1552681939806691329?s=09&t=LV2ChcukUziXfvfNK-sY0A&utm_source=pocket_mylist\nhttps://twitter.com/GuyP/status/1547234780001042432\nhttps://dallery.gallery/the-dalle-2-prompt-book/\n\nMidjourney moves to Open Beta\nhttps://twitter.com/midjourney?lang=en\nhttps://twitter.com/search?q=%23midjourney&f=image\n\nDALLE-mini is not Crayon\nhttps://www.craiyon.com/\n\nDeep Learning Resources\nhttps://github.com/jacobhilton/deep_learning_curriculum\nhttps://arxiv.org/abs/2206.13446\nhttps://arxiv.org/pdf/2206.13446.pdf\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '41': "#stablediffusion #ai #stabilityai\n\nAn interview with Emad Mostaque, founder of Stability AI.\n\nOUTLINE:\n0:00 - Intro\n1:30 - What is Stability AI?\n3:45 - Where does the money come from?\n5:20 - Is this the CERN of AI?\n6:15 - Who gets access to the resources?\n8:00 - What is Stable Diffusion?\n11:40 - What if your model produces bad outputs?\n14:20 - Do you employ people?\n16:35 - Can you prevent the corruption of profit?\n19:50 - How can people find you?\n22:45 - Final thoughts, let's destroy PowerPoint\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n", '42': '#stablediffusion #ai #watermark \n\nWatermarking the outputs of generative models is usually done as a post-processing step on the model outputs. Tree-Ring Watermarks are applied in the latent space at the beginning of a diffusion process, which makes them nearly undetectable, robust to strong distortions, and only recoverable by the model author. It is a very promising technique with applications potentially beyond watermarking itself.\n\nOUTLINE:\n0:00 - Introduction & Overview\n1:30 - Why Watermarking?\n4:20 - Diffusion Models Recap\n13:40 - Inverting Diffusion Models\n17:05 - Tree-Ring Watermarking\n26:15 - Effects of Tree-Ring Watermarks\n30:00 - Experimental Results\n32:40 - Limitations\n34:40 - Conclusion\n\nPaper: https://arxiv.org/abs/2305.20030\n\nAbstract:\nWatermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at this https URL.\n\nAuthors: Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein\n\nLinks:\nHomepage: https://ykilcher.com\nMerch: https://ykilcher.com/merch\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nDiscord: https://ykilcher.com/discord\nLinkedIn: https://www.linkedin.com/in/ykilcher\n\nIf you want to support me, the best thing to do is to share out the content :)\n\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\nSubscribeStar: https://www.subscribestar.com/yannickilcher\nPatreon: https://www.patreon.com/yannickilcher\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n', '43': 'Authors: David Ha, Jürgen Schmidhuber\n\nAbstract:\nWe explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.\n\nhttps://arxiv.org/abs/1803.10122', '44': "https://arxiv.org/abs/1705.05363\n\nAuthors: Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell\n\nAbstract:\nIn many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.", '45': 'https://arxiv.org/abs/1611.05397\n\nAbstract:\nDeep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10× and averaging 87\\% expert human performance on Labyrinth.\n\nAuthors:\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu', '46': 'https://arxiv.org/abs/1707.06170\n\nAbstract:\nConventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the "Imagination-based Planner", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a "plan context" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex "imagination tree" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.\n\nAuthors:\nRazvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra, Peter Battaglia', '47': 'Commentary of\nhttps://arxiv.org/abs/1707.06203\n\nAbstract\nWe introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.\n\nAuthors\nThéophane Weber, Sébastien Racanière, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, David Silver, Daan Wierstra', '48': 'Abstract:\nDeep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.\n\nAuthors: Matthew Botvinick, Sam Ritter, Jane X. Wang, Zeb Kurth-Nelson, Charles Blundell, Demis Hassabis\n\nhttps://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0', '49': 'The goal of hierarchical reinforcement learning is to divide a task into different levels of coarseness with the top-level agent planning only over a high-level view of the world and each subsequent layer having a more detailed view. This paper proposes to learn a set of important states as well as their connections to each other as a high-level abstraction.\n\nhttps://arxiv.org/abs/1907.00664\n\nAbstract:\nIn many real-world scenarios, an autonomous agent often encounters various tasks within a single complex environment. We propose to build a graph abstraction over the environment structure to accelerate the learning of these tasks. Here, nodes are important points of interest (pivotal states) and edges represent feasible traversals between them. Our approach has two stages. First, we jointly train a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic manner. Second, provided with the information from the world graph, a high-level Manager quickly finds solution to new tasks and expresses subgoals in reference to pivotal states to a low-level Worker. The Worker can then also leverage the graph to easily traverse to the pivotal states of interest, even across long distance, and explore non-locally. We perform a thorough ablation study to evaluate our approach on a suite of challenging maze tasks, demonstrating significant advantages from the proposed framework over baselines that lack world graph knowledge in terms of performance and efficiency.\n\nAuthors: Wenling Shang, Alex Trott, Stephan Zheng, Caiming Xiong, Richard Socher', '50': 'The AI cook is here! This agent learns to play a text-based game where the goal is to prepare a meal according to a recipe. Challenges? Many! The number of possible actions is huge, ingredients change and can include ones never seen before, you need to navigate rooms, use tools, manage an inventory and sequence everything correctly and all of this from a noisy textual description that the game engine throws at you. This paper mixes supervised explicit training with reinforcement learning in order to solve this task.\n\nAbstract:\nWhile Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research\'s "First TextWorld Problems: A Language and Reinforcement Learning Challenge" and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.\n\nAuthors: Leonard Adolphs, Thomas Hofmann\n\nhttps://arxiv.org/abs/1909.01646', '51': 'Policy Gradient RL on a massively distributed scale with theoretical guarantees!\n\nAbstract:\nIn this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\n\nAuthors: Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\n\nhttps://arxiv.org/abs/1802.01561\nhttps://github.com/deepmind/scalable_agent', '52': "DeepMind's new agent to tackle yet another Esport: Starcraft II. This agent uses deep reinforcement learning with a new technique, called League Training, to catapult itself to Grandmaster-level skill at playing this game.\n\nAbstract:\nMany real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.\n\nAuthors: Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver\n\nhttps://www.deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", '53': "Successor representations are a mid-point between model-based and model-free reinforcement learning. This paper learns successor representation in environments where only incomplete information is available.\n\nAbstract:\nAnimals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent's location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional successor features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible.\n\nAuthors: Eszter Vertes, Maneesh Sahani\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", '54': "MuZero harnesses the power of AlphaZero, but without relying on an accurate environment model. This opens up planning-based reinforcement learning to entirely new domains, where such environment models aren't available. The difference to previous work is that, instead of learning a model predicting future observations, MuZero predicts the future observations' latent representations, and thus learns to only represent things that matter to the task!\n\nAbstract:\nConstructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.\n\nAuthors: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver\n\nhttps://arxiv.org/abs/1911.08265\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", '55': "Schmidhuber thinking outside the box! Upside-Down RL turns RL on its head and constructs a behavior function that uses the desired reward as an input. The new paradigm shows surprising performance compared to classic RL algorithms.\n\nAbstract:\nWe transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also introduce a related simple but general approach for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.\n\nAuthor: Juergen Schmidhuber\n\nhttps://arxiv.org/abs/1912.02875\nhttps://arxiv.org/abs/1912.02877", '56': 'This algorithm solves the hardest games in the Atari suite and makes it look so easy! This modern version of Dijkstra\'s shortest path algorithm is outperforming everything else by orders of magnitude, and all based on random exploration.\n\nhttps://arxiv.org/abs/1901.10995\nhttps://eng.uber.com/go-explore/\nhttps://github.com/uber-research/go-explore\n\nAbstract:\nA grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma\'s Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma\'s Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma\'s Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).\n\nAuthors: Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune', '57': "DeepMind's Agent57 is the first RL agent to outperform humans in all 57 Atari benchmark games. It extends previous algorithms like Never Give Up and R2D2 by meta-learning the exploration-exploitation tradeoff controls.\n\nhttps://arxiv.org/abs/2003.13350\nhttps://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark\n\nAbstract:\nAtari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.\n\nAuthors: Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", '58': "Dreamer is a new RL agent by DeepMind that learns a continuous control task through forward-imagination in latent space.\n\nhttps://arxiv.org/abs/1912.01603\nVideos: https://dreamrl.github.io/\n\nAbstract:\nLearned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.\n\nAuthors: Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi\n\nLinks:\nYouTube: https://www.youtube.com/c/yannickilcher\nTwitter: https://twitter.com/ykilcher\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\nMinds: https://www.minds.com/ykilcher", '59': 'From the makers of Go-Explore, POET is a mixture of ideas from novelty search, evolutionary methods, open-ended le