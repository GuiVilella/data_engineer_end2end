{"data": "{\"value\":\"0.0 Hey, yo, where's my money? Well, give me my money. All right, we're going to get into9.289.28 this video in a second. Today we're going to look at AMP adversarial motion priors for15.8815.88 stylized physics based character control by Xuebin Peng, Cema, Pieter Abbeel, Sergei Levine23.4423.44 and Anju Kanazawa. And this paper is in the domain of control and reinforcement learning,32.2232.22 but it's with a little bit of a twist. So on the high level, this paper trains an agent,39.139.1 a physical agent, as you can see here, to perform some sort of goal in the case on the44.5644.56 right, it's walking up to a target and punching the target. But to do so in a certain style,52.252.2 and the style is provided by an expert data set or a demonstration data set. So the technique60.76000000000000560.760000000000005 that the paper presents mixes two things, it mixes goal achieving reinforcement learning,67.1667.16 and it also mixes adherence to a given style. And the adherence to a given style, that's72.6000000000000172.60000000000001 going to be the adversarial part right here, because that's learned in an adversarial way.78.8000000000000178.8 The mixture of the two at the end looks pretty, pretty cool. So the setup right here is a87.5287.52 setup of goal achieving and imitation learning as we have already outlined. And the way it96.3696.36 works is the following, there is going to be a task and the task can be, you have to101.72101.72 reach a goal, the task can be you have to punch something, you have to overcome some107.12107.12 obstacles and then reach a goal, any anything like this is a task. So the goals are fairly115.04115.04 high level, and they are given, obviously by a reward function. So you place the agent120.68120.68 in an environment and there is a reward function. By the way, the agent here is as we already126.12126.12 also said, is this sort of physical agent that is going to have some sort of a 3d structure.136.46136.46 There is going to be joints that it can move. There's a joint here and one here usually.143.08143.08 So and there's a head. The agent is this physical thing and it's in a physics simulation and149.56149.56 each one of these joints, it can move kind of independently, sometimes free as a as a157.16157.16 ball, sometimes it's restricted. It's modeled very much like a human there are other I believe163.44163.44 other models such as a T Rex, which of course work differently. But you have this agent169.12169.12 and the agent is supposed to reach a goal like somewhere over here, there's a little174.24174.24 flag to the goal. And the way the agent can interact with the world is by putting force180.84180.84 on any of these joints. So it can move these joints in pre specified ways. And that constitutes187.07999999999998187.08 the actions. So the agent will observe the state and the state here is given mostly by193.8193.8 it can observe how all the joints are currently the velocity of the of the joints or of the201.04000000000002201.04000000000002 of the individual parts of itself in relation to itself. So it can sort of feel itself.207.68207.68 And it also knows in which direction and generally how far away the target that it needs to reach214.64000000000001214.64 is. So that's the observation space, the action spaces, it can affect these joints. And the222.2222.2 reward function is often modeled in accordance with the goal. So the reward function for228.23999999999998228.23999999999998 walking to some goal might simply be you get reward if you are closer to the goal. Okay,235.23999999999998235.23999999999998 so this encourages the agent to go over there. So we work with quite dense rewards right241.27999999999997241.28 here. Because I guess the fundamental problems of reinforcement learning aren't exactly the246.88246.88 point here. The point here is, can you teach these things to achieve a goal while maintaining251.78251.78 a certain style? Now, this is the the task and the environment. In addition to that,259.14259.14 you do get a data set. And the data set is demonstrations of a certain nature. So this267.28267.28 is not necessarily demonstrations of how to reach the goal, it can be any sort of demonstrations.274.32274.32 So usually when people do sort of imitation learning or learning from demonstrations,279.34279.34 there is a bit there are some requirements, if you want to do pure learning from demonstration,284.35999999999996284.35999999999996 of course, the demonstrations need to be how to achieve the goal. And that we don't we290.64290.64 don't have that here. In other cases, you do need the sort of policy or the action of298.03999999999996298.03999999999996 whoever performed the data set, we also don't need that here, our goal is simply going to303.12303.12 be we have to reach the task while while sort of adhering to the data set in a way. And311.91999999999996311.91999999999996 this way, we're going to define in a second. So the data set, you can imagine, I think317.59999999999997317.6 there is a good demonstration down here, you can imagine the the data set to give you sort324.76000000000005324.76000000000005 of the style of movement. So in one data set, you can have running movements and walking331.32000000000005331.32000000000005 movements. And in another data set, you could have these movements that were just the these337.52000000000004337.52000000000004 actors walk like zombies. And the goal here is to combine the style of the data set with346.18346.18 reaching the goal. Okay, so the combination would look like a zombie walking to the goal,354.24354.24 which adheres to the zombie walk in the data set, and the goal and specified by the task.361.92361.92 Okay, naturally, you're, you're going to model this as two different reward signals. So there's368.88368.88 the reward signals of how much you reach the goal. And there is the reward signal of how375.0375.0 well you adhere to this style in the data set. The reward goal right here is modeled381.4381.4 by classic reinforcement learning. So this is very much very, very classic. Where do390.2390.2 we have it? So you would simply train, I don't even think it's it says here, it's update396.68396.68 G and D, yada, yada, yada. So this is a policy gradient method reinforcement learning, which404.52404.52 means that you do have a policy function, which takes in a state and maybe a history.411.52411.52 And it will give you an it will give you an action. And with that, you also train a value418.84418.84 function that takes a state and will give you a value for that state. Now, the value426.74426.74 function is purely for training the agent, because you do a do advantage estimation with434.4434.4 this value function. But essentially, this is a standard policy gradient method that439.56439.56 you train this part is lower part of the this lower part of the thing on sorry, you actually447.15999999999997447.15999999999997 trained the whole thing on this reward. But the bottom part you can imagine is it a reward455.21999999999997455.21999999999997 comes from reaching a goal. The top part gives also gives you a reward. Okay. And yes, I462.47999999999996462.48 want to reiterate both of these rewards are used to train the policy and the value in468.24468.24 a policy gradient fashion. So both rewards ultimately, are in this standard advantage474.92474.92 estimation reinforcement learning setting. However, the top reward is calculated differently482.18482.18 than simply do you reach the goal, the top reward is a measure of how close you are in487.20000000000005487.2 style to the data set. And that's given by this motion prior. And the motion prior is493.65999999999997493.65999999999997 given by a GAN by a generative adversarial network. And I'm trying to, to find the formula502.36502.36 here. I think this here is the the best description of it, though it's just a formula. So a generative513.34513.34 adversarial model, I'm pretty sure you're, you're all aware, there is a data set right520.1600000000001520.1600000000001 here. There is a generator right here, the generator gets some random noise as an input,527.1600000000001527.1600000000001 it outputs a sample x from the data set, you get a sample x prime or a mini batch. And533.96533.96 then both of these, or the these either of these goes into the discriminator model. And540.6540.6 the discriminator has to decide for any sample, is it real? Or is it fake? So the way this547.48547.48 generative adversarial network approaches the problem of specifying which motions are554.5600000000001554.5600000000001 real and which ones are not, is by looking at transitions. So the data set here is not560.3000000000001560.3000000000001 images or so like you're used to in a regular GAN. But the data set is transitions. What565.5600000000001565.56 does that mean? So in every situation, your humanoid or whatnot is here, and the goal571.68571.68 is over here. And this is one state, this is s. And then the agent takes an action,581.52581.52 okay, the action could be please lift one leg. And how does that evolve? So the new588.16588.16 agent would be kind of here shifting the weight a little bit, and lifting one leg, okay. So595.16595.16 this would be one action, which would lead to a new state s prime. So you have three601.0601.0 quantities, you have the state, you have the action that the agent took, and you have the607.0607.0 new state s prime. Now, you could parameterize the transition either using state and action,614.6614.6 or state and next state, the paper here does state and next state for the reason that in621.52621.52 the data set, in the data set that you get, right here, you do not have the action available,629.56629.56 you can probably guess it, but you do have the state and the next state. This data set635.46635.46 can come from anywhere it can come from human demonstration, it can come from keyframes641.28641.28 made by a 3d artist, or maybe another agent that has already solved the problem. Therefore,646.56646.56 you don't always have the actions available. So a transition is going to be specified by652.4799999999999652.4799999999999 a state and a next state. And the transitions from the data set are transitions that you660.28660.28 observe in the real world. So these are state next state pairs that you observe in the real665.92665.92 world. And the generator, the generator essentially outputs state next state pairs. Now this generator676.4399999999999676.44 isn't a generator in a like in a classic adversarial network. But this here is generated by your684.84684.84 policy interacting with the environment, right? So here's your policy, it interacts with the691.24691.24 environment. And the environment gives you the state and in the next step, it gives you697.2800000000001697.2800000000001 the next state, right? So by interacting with your environment, you do get state next state704.1600000000001704.16 pairs. These are essentially your generated pairs. And the discriminator is trained to710.68710.68 discriminate between whether or not a transition is from the real data set, or whether it has718.68718.68 been generated by your agent. Now, of course, this whole system isn't back propagatable.726.2199999999999726.2199999999999 And that's why you do train it using reinforcement learning. So the reward the usual back propagation732.24732.24 signal that you would have in a generator right here, you can't do that. That's why737.12737.12 you simply take the output here, the loss of the discriminator as a reward for the for745.6800000000001745.6800000000001 the policy right here. So in this case, the policy using policy gradient is trying to753.0753.0 fool the discriminator into thinking it into it thinking that the transitions that it generates759.6800000000001759.68 come from a real data set, while the discriminator at the same time is always trained to differentiate766.7199999999999766.7199999999999 between the true data set and the transitions that the policy generates. Alright, so that773.54773.54 gives you a reward signal for the policy. And the other reward signal comes simply from778.92778.92 the environment as we've already stated. So these two rewards are then combined with each784.64784.64 other and used to train the policy. The discriminator itself, as we already seen is trained. So792.64792.64 this thing here is actually the discriminator, this more motion prior is trained one hand798.4399999999999798.4399999999999 from the data set. And on the other hand, from the from the policy, generating actions806.92806.92 and generating transitions through the environment. Alright, I hope that is a bit clear right813.64813.64 here. So there are many components to this, but two are important, the policy, which tries820.38820.38 to at the same time reach a goal and fool the discriminator. Those are two rewards,825.52825.52 there are two rewards are combined. And on the other hand, the discriminator itself simply830.3199999999999830.3199999999999 gets transitions from the data set and gets transitions from the policy environment interaction836.92836.92 and tries to train itself to pull the two apart. So it's a it's a classic two player843.7199999999999843.7199999999999 game. And yeah, that that is what you're used to from again. Alright, and that's essentially853.28853.28 it for this thing. Here is the algorithm, we generally initialize every thing there859.9399999999999859.9399999999999 is a replay buffer like in a classic reinforcement learning, which stabilizes training quite865.88865.88 a bit. I also mentioned the value function, which is used for the advantage estimates871.12871.12 of policy gradient. So you for m steps, you collect trajectories using the policy you878.08878.08 already have, then you feed the transitions to the discriminator right here. Now this888.16888.16 here is a feature function of the state. So you only they have special feature functions,894.76894.76 which make the this problem easier, there's a lot of expert knowledge going into how you899.88899.88 build the features, how you represent the environment, and so on. So it's not quite904.6904.6 trivial, but I don't I don't want to go too much into that. You do calculate the style911.0911.0 reward according to equation seven, equation seven is simply the discriminator. It's not917.64917.64 the discriminator loss. So the discriminator loss is actually is this thing right here,922.88922.88 they do use a square loss for the discriminator instead of a classic GAN loss. So the classic932.0932.0 GAN loss would be this thing up here, where it's log D minus log one minus D. Yet they939.06939.06 use this square loss that they found to work a lot better or least square loss, you can944.6944.6 see the discriminator is trained to be close to one, if the data comes from the real data951.32951.32 set, which is capital M here, and it's trained to be negative one when it comes from the957.96957.96 policy. Okay, so nothing stops the discriminator from spitting out any number like 15 or three,966.7800000000001966.7800000000001 it's just trained in a least squares fashion to go to these numbers, which gives you a971.1800000000001971.1800000000001 better gradient. So for this, for these continuous control problems, often, you have to go to979.0400000000001979.04 least squares objectives, because which number is being output is often quite important rather986.28986.28 than just a classification. And even here where it is actually a classification loss,991.04991.04 right, which is surprising, but cool. And then the reward, you know, given a transition998.8998.8 is calculated as so this is clipped at zero. So this is also between zero and one, as you1005.561005.56 can see here, if the discriminator says one, the reward is the highest, the reward is actually1013.281013.28 one. And when is the discriminator one, the discriminator is one if it thinks that the1019.281019.28 reward sorry, that the transition comes from the real data set. So if the policy manages1026.81026.8 to produce a transition that the discriminator thinks comes from the real data set, it gets1032.561032.56 maximum reward, okay. And if it also reaches the goal, it gets maximum reward from that1039.61039.6 part of the reward signal too. So the general encouragement that we give the policy is,1047.67999999999981047.6799999999998 you should reach the goal in a matter that's consistent with the data set. So it should1052.521052.52 probably pick out things that do both, right? It could try to, it could try to switch between1061.15999999999991061.16 the two modes, like, okay, let's do a little bit of data set, let's do a little bit of1064.56000000000021064.5600000000002 goal reaching. But it's probably better if it actually picks things from the data set1069.41069.4 or behaviors from the data set that also reach the goal in a matter consistent with the reward1077.41077.4 with the task reward. So the algorithm just to finish it goes on. And it says, okay, so1085.241085.24 this is the style reward, the true reward is given by a mixture, a weighted mixture between1091.241091.24 the style and the task reward and the weights you have to specify. And then we simply store1099.241099.24 these this trajectory in our replay buffer. And then we use the replay buffer to update1107.321107.32 the discriminator. And we also use the replay buffer to update the value function and the1113.61113.6 trajectory according to policy gradient. They point out a few things that are important1120.51120.5 right here to their algorithm. One of them they find very important is this gradient1125.43999999999981125.4399999999998 penalty. So GAN training can be a bit unstable. And these gradient penalties, they are a way1133.841133.84 to stabilize this training. And they found that simply penalizing the norm of the gradient1141.581141.58 as it comes out of the discriminator is stabilizing the training right here. So this is one thing1152.19999999999981152.1999999999998 they've they helped they this is one thing that they claim is helping them a lot to actually1159.741159.74 converge. And this tells you a little bit that it's still quite quite finicky. They1165.19999999999981165.2 talk a lot about the representation of the actions right here, the policy here in network1171.721171.72 architecture, the policy and value and discriminator functions. They are very simple multi layer1178.681178.68 perceptron. So you can see like the mean, the mean of the policy function is specified1187.21187.2 by a fully connected network with two hidden layers consisting of 1024 and two 512 ReLU1195.481195.48 consistent ReLU. Okay, I guess that's a fully connected layer with a ReLU non linearity,1205.241205.24 followed by linear output. So the networks aren't super complicated right here. What's1209.721209.72 more complicated is the training procedure, the loss, the regularization constants and1216.36000000000011216.36 the reward engineering. So there is a lot of reward engineering happening right here.1221.95999999999981221.9599999999998 And that's what you find in the appendix. So the reward for example, for going and punching1230.341230.34 something is is threefold. So if you are far away, it's one reward. If you're close, it's1237.621237.62 a different reward. And if that target has been hit, it's a different reward, right?1242.63999999999991242.64 I guess the top line makes sense, but the others are sort of reward shaping the behavior1249.08000000000021249.0800000000002 one so you want the the agent to kind of approach the target fast, but then kind of slow down.1256.761256.76 And also, you know, if you look at something like dribbling, where there is a ball involved,1262.181262.18 there is a lot of reward shaping going on, even in in target location, there is a lot1269.21269.2 of reward shaping going on, where you sort of encourage the agent to have certain velocities1275.51275.5 and so on. So this is important because of the experimental results that they show. And1284.36000000000011284.3600000000001 that's where we go back to the video. Where's the video? Right here. So keep in mind, their1294.31294.3 point is you're able to reach a goal in the style of the data set. So this is the simplest1300.581300.58 task they have, it's called target heading. And the goal is simply to walk or to go in1305.861305.86 a given direction at a certain speed, okay. And the example clips they have are displayed1314.87999999999991314.8799999999999 on the right. So the example clips are of someone walking and of someone running. Yet1322.741322.74 there is not really a transition in the data set from walking to running. And the the agent1331.341331.34 learns to this transition by itself. So their point is always, look, we have kind of simple1338.281338.28 things in the data set, we have the individual parts in the data set that the agent should1342.841342.84 do. But we never have the combination of all the things. And to kind of stitch these parts1349.261349.26 together, that's the powerful thing about this method, which is pretty cool. So here,1354.981354.98 you can see at the top right, there is a target speed. And all of these three agents are trained1362.081362.08 agents. And the in the same manner, right, and they're all told to reach that given target1369.141369.14 speed. However, the agent on the left only has been provided with a data set of people1375.961375.96 just walking the date agent in the middle the same but it has only received a data set1382.061382.06 of just agents running. So no walking. And on the right, this agent has received a data1389.31389.3 set of agents walking and running. So you can see that as the target speed changes,1398.21398.2 the like if it's fast, the walker is not able to keep up when it's slow, the runner is not1403.921403.92 able to slow down. However, the agent that has the full data set available can not only1410.06000000000021410.0600000000002 match the speed and change its style according to the speed, it can it also learns the transitions1416.681416.68 from one to the other. And this these transitions are not in the data set itself. Okay, so the1424.34000000000011424.3400000000001 cool part about this method is it can sort of stitch together the appropriate behaviors1430.34000000000011430.34 from the data set. Even if you don't provide these specifically to solve the task. The1439.65999999999991439.6599999999999 Yeah, this is the the t rex. I think this is just to show that you don't have used motion1444.021444.02 capture, but you can use it, you can learn from a provided data set of keyframe animation.1452.781452.78 And you can also see the there is nothing in the data set about reaching a goal, there's1457.521457.52 just kind of demonstrations of the t rex walking. And the method is able to adapt this walking1463.761463.76 style in concordance with reaching a goal. So you can see that the turning is much like1471.01471.0 the turning in the example clips. Whereas if you've ever seen things like this without1477.921477.92 without the the examples, these policies that these things come up with are quite weird.1486.321486.32 So here's a failure case. And so the difference between this method and other methods is other1492.51492.5 methods, such as this motion tracking in the middle, what they try to do is they try to1498.37999999999991498.3799999999999 match a given behavior from the data set as closely as possible. So this it's called motion1505.521505.52 tracking. Now there is a some sophistication to it more than I'm saying right here. But1510.841510.84 essentially, you have a front flip on the left, and then the motion tracking algorithm1515.63999999999991515.64 tries to learn a policy such that the the behavior is followed as closely as possible.1523.241523.24 Now again, this is really good when you have the exact demonstration available from what1529.04000000000021529.0400000000002 you want to do. It's not so good if you if what you have available as demonstrations1535.08000000000021535.0800000000002 is not isn't really what you want to do is just sort of some demonstrations. But there1541.78000000000021541.78 are failure cases, of course, if you want to copy exactly. So if you want to do a front1546.761546.76 flip, and by the way, the reward function here is how closely you match the motion from1555.21555.2 the reference motion. So that's the reward function. However, motion tracking does more1560.561560.56 than that motion tracking really tries to track the motion itself. While this method1564.921564.92 here would only get the reward of tracking the motion. And you can see it doesn't manage1571.281571.28 to to actually learn it more like doesn't try it tries to not fail it so it reaches1580.161580.16 the same end position and that sort of good enough for it. So there is a yeah, there is1588.39999999999991588.3999999999999 a trade off right here. It's probably also given by how much you weigh the different1595.761595.76 components. So here you have a data set of agents walking and agents waving. And then1603.641603.64 what you want to do is you want to have a agent that walks in a direction while they1609.721609.72 wave the arm, or why they they lift the arm or something. So at the left, you can see1616.481616.48 if you only have a data set, if you only have a data set of the waving agents, it's really1624.141624.14 struggling moving forward, right that the walking learns it has no demonstration of1629.04000000000021629.0400000000002 walking. So that's a struggle. If you only have the walking demonstration in the middle,1634.241634.24 then it doesn't really track the arm movement where it should even though there is a reward1641.721641.72 for it right only. Yeah, on the right. I mean, this is somewhat somewhat, but it is kind1649.381649.38 of able to, to interpolate. So if you if you want to check out this video, there is another1656.181656.18 one that actually explains the paper in a short form. This is from from SIGGRAPH, go1662.52000000000021662.5200000000002 check it out. They do have more sophisticated behaviors. So on the bottom here, you can1668.04000000000021668.0400000000002 for example, see the obstacle, run, leap and roll. So the data set contains demonstrations1676.76000000000021676.76 from all of those things, but not the things in conjunction with each other. In this here,1684.961684.96 at least what they describe in the text in this, this right here, what they have in the1691.281691.28 data set is demonstrations of walking and demonstrations of getting up from the ground.1698.01698.0 And whenever so the agent learns that whenever it falls over right here, that it can get1705.521705.52 up faster if it kind of does this rolling motion right here. So this was nowhere in1710.041710.04 the data set. But because the agent wants to go to a get up state, both because that1718.321718.32 will go it that will make it go towards a goal. And also because that matches behavior1724.281724.28 in the data set, it will learn this rolling motion as it falls down in order to get up1729.81729.8 again. So that is that's pretty cool. Also in this strike and punch example, the data1737.241737.24 set apparently only contains agents walking or agents punching, it never contains agents1745.21745.2 walking and then punching. So the transition that you saw at the beginning is a learned1752.87999999999991752.88 behavior that wasn't in the data set. So that's, I think, it's a it's a pretty cool application1760.36000000000011760.3600000000001 of and a combination of two things of adversarial learning and of, of learning, sorry, not from1770.04000000000021770.0400000000002 demonstration, because that's adversarial learning of learning to reach a goal. And1775.04000000000021775.0400000000002 it's a good Yeah, it's a good demonstration of how you can combine the two, they have1778.41778.4 a lot of ablations, where they sort of show that the impact of the data set makes a big1785.721785.72 difference. I mean, you've seen this in the demonstrations. But also here, you can see1790.32000000000021790.3200000000002 that again, in a graphical form. So the locomotion data set contains both demonstrations of walking1796.32000000000021796.3200000000002 and running, while the walk or the run data set only contains demonstrations of either.1802.441802.44 And the here is the target speed versus the average speed that the agent does. Now, if1809.481809.48 you only have a walking data set, the agent, no matter the target speeds, the agent will1814.681814.68 always kind of stick to walking. And if you have the running data set, it can run faster1822.08000000000021822.0800000000002 up here. But if you want it to slow down, it can't really run slower than you require.1829.08000000000021829.08 Only when the data set contains both things, can it transition between the two and actually1835.321835.32 match the running or walking. So what do we think of this? My opinion is it's probably1845.581845.58 it's very cool. And it is a it's a good way of sort of bringing demonstrations into the1852.39999999999991852.4 picture without manually like tracking the demonstrations or copying exactly. So you1859.681859.68 just give some suggestions to the algorithm of what it could do. And you do that in form1866.21866.2 of a data set, which is something that I you know, like, because it's not as invasive as1874.21874.2 telling the agent, you know, you need to match the joint movements and so on of the of the1880.60000000000011880.6 demonstration. This enables demonstrations to come in that are of a much broader range,1887.041887.04 not necessarily reach the goal, not necessarily even have a goal in mind. So that's cool.1892.81892.8 On the other hand, I think it's pretty finicky, because you have to strike the trade off parameter1899.761899.76 between the two rewards quite cleanly, or clearly for your goal. Because we've already1907.761907.76 seen right at some point, the agent won't reach the goal anymore. If, if this reward1914.081914.08 here, if the reward of the style is too high, we already saw this, if you have a data set1921.81921.8 of just running, the agent will simply neglect the goal, it won't go slower than you know,1929.01929.0 the kind of the slowest run or demonstration or a little bit slower than that, it just1934.281934.28 won't change its policy because it needs to match the data set. And this balance seems1941.71941.7 to be quite, quite a important hyper parameter. And that also makes the provided data set1950.041950.04 here quite an important thing to to have available. So which data set you provide is also quite1959.01959.0 important. And lastly, the tasks themselves are the reward of the goal directed task nature,1968.361968.36 or in this paper, extremely engineered. And that's what I want to come back here lastly1974.441974.44 too. So what they tout, for example, in this walk and punch thing, they say, Oh, when the1982.121982.12 agent is far away, it runs towards the target. But if it's close, it only it slows down.1989.67999999999981989.6799999999998 And then when it's really close, it punches the target. And it sort of learns to combine1995.081995.08 these different skills. But and which is cool, right, because the transition wasn't in the1999.47999999999981999.4799999999998 data set. But a big part of it combining these skills is because in the reward, you make2007.43999999999982007.44 the reward different, whether the agent is far away, or whether it's near, you can see2013.522013.52 that right here. So these things are reward shaped to a high degree to encourage these2020.60000000000012020.6000000000001 kinds of transitions to happen, which I think is not really practical in a lot of settings.2029.642029.64 So it's still to be seen how much this is of practical value in other reinforcement2037.42037.4 learning tasks, where you don't have that available. And also in other reinforcement2042.04000000000022042.0400000000002 learning tasks, where maybe the reward is more sparse, and how that affects this thing,2048.982048.98 because essentially, if the reward is much more sparse and irregular, now you have a2055.282055.28 problem because now the style signal is much more prominent. And that's not necessarily2060.842060.84 solved by simply reweighing the style signal. So I'm excited to see what comes out of this2068.20000000000032068.2000000000003 line of work next. It's a pretty cool line, as I already said, it's a good application2074.12000000000032074.1200000000003 of GANs in a different field than images. And with that, let me know what you think2081.02081.0 in the comments. I'll see you next time. Bye bye.2085.56\"}"}