{"data": "{\"value\":\"0.88 Hi there, today we'll talk about switch transformers scaling to trillion parameter5.52000000000000055.5200000000000005 models with simple and efficient sparsity by William fetus Barrett is off and no one11.4411.44 should see her of Google Brain. So as you can see right off the title, we're going towards17.1217.12 trillions of parameters GPT three had 175 billion parameters. This paper claims to have a model with25.5225.52 a trillion parameters. Now, is it really five times bigger or 10 times bigger than GPT three?33.3633.36 That's a debatable question, because the trillion parameters are not used in the same way as in a39.3639.36 classic transformers. They are used actually in a sparse way. That's why the word sparsity is in here.45.647.2 And the way they are used in sparse manner is this new architecture called the switch transformer.54.48000000000000454.48 It's not entirely new, the it's built on mixture of experts. In this paper, that's also called moe.62.862.8 That has been around for a while, and we're going to see what that is. Now, on a high level, switch68.868.8 transformers takes mixture of experts to an extreme, in that it is a transformer. And, and75.5276.64 the feed forward layer is divided up into these experts. And the switch transformer82.882.8 only routes each token to one expert only. That's the sparse part. So the mixture of experts91.8491.84 previously, they always claimed you need at least two experts in order to get a stable training97.6799999999999997.67999999999999 signal, the switch transformer manages to get it down to a single expert. So it's like a hard104.24104.24 routing of information to just a single endpoint per layer of each token. So in that means you can114.39999999999999114.39999999999999 now scale the experts. And you can scale the number of parameters in the model without making122.39999999999999122.39999999999999 the model compute more. That's a very special notion. So you can up the parameters of the model.128.0128.0 But if a forward pass of a data point will still have the same amount of flops that it needs to136.56136.56 forward propagate through the network, very special architecture right here. So yeah, that's why I'm143.28143.28 saying trillion parameters not necessarily comparable to the 175 billion parameters of149.76149.76 something like GPT-3. So how do they do it? Because previously it was claimed it was unstable,157.44157.44 they have new ways of making the training stable, such as selective dropout, selective casting of164.48164.48 parameters to different precisions, and a better initialization. So that's the high level overview171.35999999999999171.35999999999999 of the paper. And we'll dive into it, we'll explore kind of what mixture of experts is and how the178.07999999999998178.07999999999998 model works. And what turns out it's a very long paper, as you can see, when papers have a table of182.8182.8 content. That's a lot of fun. But it's a lot of engineering as well. And we're mostly interested190.16000000000003190.16000000000003 in the model here, what it can do, and what does it how does it sort of fit in to the big world of198.48000000000002198.48000000000002 transformers and language models and so on. Last thing I want to say, trillion parameters is, you205.84205.84 know, it's a catchy title, that most of the paper, they don't work with trillion parameter models,211.84211.84 they work with models in the in the order of billions of parameters. And at the end, they build219.04219.04 a model with a trillion parameters, it doesn't do as well as their models with in as their smaller225.12225.12 models. They also, it feels like they don't put that much work into it, because it's probably also231.12231.12 quite fuzzy and expensive. But just know, we're not going to have trillion parameter models around240.4240.4 anytime soon. Just yet. Interesting fact, the original ResNet paper also built a 1000 layer249.28250.0 convolutional neural network. Even though the ResNets we have today, you know, they are maybe256.56256.56 50 or 150 layers deep, they did build a 1000 layer model. So maybe compare it a bit to that one. It's265.12265.12 just like we can do it, not necessarily we need to. So here you can see something they discover.271.68272.4 The curve on the left is very, very known to people that are in the language model game,279.44279.44 let's say, or in the in the let's scale up AI game. And that is as you increase the size of the286.88286.88 model, the loss will go down. And that's loss as I understand it. So that's test loss. I believe296.48296.48 that is perplexity. So scaling properties, exactly that that might be perplexity or test loss on some305.12305.68 downstream task. In any way, as you scale up the model parameters, the model gets better and better312.0312.0 and better. The interesting thing right here is twofold. First of all, I believe they do hold the318.16318.16 data set constant. So the data set is always the same, the amount of compute you put into it, the326.16326.16 amount of either number of steps or time is also always the same. And in this specific case, the335.36335.36 amount of flops per forward pass is also the same. The only thing that changes is the number of342.16342.16 parameters. Again, it's very special to have a model where you can scale up the number of347.84000000000003347.84000000000003 parameters, yet the flops required to forward propagate stay the same. So you can see here that354.8356.40000000000003 there is a almost unhalted decrease here, it flattens out a little bit towards the bottom,363.36363.36 though that is not necessarily does not necessarily mean it will ever flatten out before it's, you368.72368.72 know, at zero, I will approach zero, I guess. So and you can you can see that, you know, they scale377.84000000000003377.84000000000003 up the model quite a bit. And also, their main comparison here is the T five base. So that's the385.12385.12 text to text transfer transformer. By the way, if you don't know what a transformer is, or what a391.36391.36 language model is, is best you go back to my earlier videos and look up like the GPT three399.84000000000003399.84000000000003 paper or the attention is all you need paper. I've made videos about lots of these things,405.44405.44 I assume that you know them. You can see right here that if you compare to number of training411.68411.68 steps, for example, the this switch models, all of them, no matter how big they are, they provide421.52421.52 massive gains over like something like a T five. And they also do this in time. So this paper is431.84000000000003431.84000000000003 very much about trade offs, you do require more storage for your weights. So you have to have more440.72440.72 memory more RAM. However, that memory can be distributed, it can be sharded, because they use447.6447.6 this mesh TensorFlow library to implement the switch transformers. And because their model has453.12453.12 this sparsity, they can efficiently shard the model. So you trade off more memory, which can be462.40000000000003462.40000000000003 sharded. But what you gain is training speed, in both in terms of time and number of training steps470.48470.48 required. So you are much more efficient. Note that this only all of this holds in this super477.20000000000005477.20000000000005 large regime, right? We this is, they say they've also discovered the speed ups in smaller models.483.92483.92 But you know, as far as the paper is concerned, we are talking about millions, hundreds of millions489.6489.6 of parameters, billions of parameters, even 2 trillion of parameters, together with these495.04495.04 giant corporate corpora of, of text. So that's sort of the regime we are in. And the results do503.52000000000004503.52000000000004 not necessarily transfer down to the lower scale problems that you know, you might face with your511.04511.04 lonely one, colab in the corner. Alright, so in a transformer, you have a transformer is nothing520.32520.32 else, but a bunch of these layers right here. This is this is in itself a transformer layer526.4000000000001527.7600000000001 in its basic form. And it consists of sort of two parts, it consists of this self attention,533.6534.1600000000001 right here. Now that's the standard transformer self attention, that's what was introduced in540.32540.32 attention is all you need. And what's been used ever since in all the transformers. This one right547.2800000000001547.28 here is a is an, as I understand it, a language model. So you know, this this is very standard.555.8399999999999556.48 However, after the self attention, you have this feed forward layer. Now, usually, what you do564.4564.4 is you have an input sequence, and you transform that through multi headed attention569.4399999999999569.44 into another sequence right here. Okay. And then what you do is you take each of these things and577.6577.6 feed them through a feed forward layer. And if I am, as I understand it, this feed forward layer585.2800000000001585.2800000000001 is simply, you know, a regular feed forward layer that you would find in a neural network,592.24592.24 and you pass them, you pass these things individually. So this here, it's a vector,597.6800000000001597.68 you pass it through here, and boom, that becomes the next layer representation, this thing right602.88602.88 here, you pass it through as well, boom, that becomes this one, and so on, right, you pass them609.04609.04 individually to get the next layer representation. So this, this part right here, the attention part,618.56619.12 it sort of aggregates information and relates the individual items of the sequence to each other.626.24626.24 And transforms them into, you know, a new sequence, where sort of all the every token can gather632.4632.4 information from every other token. That's what the attention mechanism does. That's step one. In638.4638.4 step two, every token is isolated, every token is for itself. And the feed forward layer simply645.36645.36 determines, you know, what's given one token given token number one, what is, you know, given its653.04653.04 representation in this layer, what is the best representation for the next layer? Okay. So that's659.76659.76 token number one of the next layer. So the multi head attention is kind of relating tokens to each668.4668.4 other, and the feed forward layers, they are relating layers to each other. Okay, so up here,674.8674.8 you would have the next multi head attention layer. So you can see the feed forward layer as sort of680.64680.64 translating from one layer to the next layer, right getting saying, oh, you come from this layer, I'm686.48686.48 going to translate you such that the next layer understands you. And that happens on a token by692.3199999999999692.3199999999999 token basis. Now you can see this is it's always the same feed forward layer for all the tokens,697.76697.76 right, the tokens are sort of treated like a batch of samples. The idea of this switch transformer,705.76705.76 and also of the earlier mixture of experts transformer is that it might not be a good idea712.72712.72 to have only a single one, right, this is the only feed forward layer, it's the same for all the719.04719.04 tokens, it might actually be a good idea to have a couple of them that sort of specialize in725.28725.28 different things. So what could that be? You know, in a in a basic world, this could just be like732.3199999999999732.32 one for nouns, and this could be a feed forward layer for verb verbs, tokens that are verbs,738.1600000000001738.1600000000001 tokens that are adjectives, and sort of maybe here is like punctuation tokens, right? You might744.24745.0400000000001 think, well, if you are a noun token, the next layer might want to look differently at you,753.0400000000001753.0400000000001 than if you are a punctuation token, right? So this translation from one layer to the next layer759.9200000000001759.92 can now happen dependent on what the token represents. Right? Now we we of course, first of768.7199999999999768.7199999999999 all, we don't have these annotations. And second, is not necessarily that you know, we want to774.4774.4 always divide it by noun verb, adjective punctuation. Ideally, we want to learn this routing.780.7199999999999780.7199999999999 So we simply want to say, look, instead of just one feed forward layer, we give the model a788.16788.16 feed forward layer. We give the model four feed forward layer, feed forward layer one, two, three,793.6793.6 and four. And for each token, the model can decide to which of these feed forward layer it sends the801.1999999999999801.1999999999999 token to. So here you can see this is a token. Now, you know, we are dealing with word pieces,808.48808.48 let's just say the word more, I was like, I was thoroughly confused by when I saw this like, huh,814.9599999999999814.96 why does it say more parameters, but here, it's the string more right, and the string parameters.821.6800000000001822.24 And these are in the vocabulary, and they get an embedding vector associated with them. So that's829.2829.2 what's going on here. Then they go through self attention, as you can see here, both go through833.2800000000001833.2800000000001 self attention, and then each one of them is routed to one of these four experts. Now the838.8000000000001839.44 one here, the one on the left and the one on the right, these are the same experts, right,842.88842.88 they're just duplicated visually here. But these would be the same weight matrices in there. So you850.64850.64 have four feed forward layers in this layer. And each token can be routed to any one of them. And859.12859.12 this routing here, this is learned. So in here, you have a matrix, they call it like wr. And866.24866.24 using wr, you simply do an inner product of wr with your input right here, let's call that873.84873.84 h with your input h, I guess they use h for a different thing. I think they they call this x880.64880.64 again. So you do this with x. And then you get, you get h, which is your routing. And then you889.12889.12 simply build a histogram, you normalize the histogram, I think with a softmax. And that those895.12895.12 are your routing weights. So it's very much like another attention mechanism, except that the903.6903.6 queries this thing here, these are like the queries, these are sort of the queries of this911.68911.68 attention mechanism. And this here, these are the keys and the values. So that's a good keys and918.08918.72 the values of this attention mechanism. The queries are just learned. So the queries are924.32924.32 not dynamically generated. And the keys and values, they are not. Yeah, it's a it's a weak932.6400000000001932.6400000000001 analogy. But you can sort of think of it like this. So there is this routing mechanism. And it940.1600000000001940.1600000000001 decides where a token gets goes to. Now, as you can see, the router is soft, that means there is947.5200000000001947.5200000000001 never a one or a zero right here, there's always kind of a number in between, but they hard clip952.96952.96 that. So they hard clip it, they just route it to the maximum. As you can see here, number two958.88959.44 is the maximum. And they just route it to number two, they don't route it proportionally or anything,965.52965.52 they just to take our max and they route it through, they do multiply the output by the970.8000000000001970.8000000000001 actual number that they got out here. So if the router is unsure, then the output is less. If976.08976.08 they're out to assure the output is more. But this hard routing is what's the key right here. And985.0400000000001985.0400000000001 that means, you know, before, before, you'd have one feed forward layer. So any token that goes993.5200000000001993.5200000000001 forward goes through one feed forward layer. If you do a mixture of experts in the classic sense,1000.081000.08 and you route it in a soft way, you now have four feed forward layer. So every token goes through1006.321006.32 four of these computations. So you've basically multiplied the amount of computation by four,1013.52000000000011013.5200000000001 because you've multiplied the amount of parameters by four, right, you have four times as many1018.56000000000011018.5600000000001 parameters. Now, when you do this arg max routing, like the switch transformer, you have multiplied1026.081026.08 the number of parameters in your model by four, but any token will still only incur one feed1032.321032.32 forward layer. That means you keep the amount of computation that you do per forward pass the same.1038.63999999999991039.4399999999998 And that's, that's sort of the key right here. So now they can scale up massively the number of1045.841045.84 experts, while still keeping the amount of flops the same. And notably, you also don't need any data1054.481054.48 transfer in between the experts. Every every expert can be can, you know, receive their tokens1061.281061.28 and then do their independent work. So you can efficiently shard this across many, many machines.1066.01066.88 This is how this looks. So in in this case, you have three experts and your sequences are of line1074.881074.88 of length six. So you want to sort of route each token there and there can be overflow,1080.81080.8 like every token is independently routed. So it can happen something like this, that a, you know,1086.63999999999991086.6399999999999 a token like three token gets routed to one expert, but it only has space for two tokens.1092.481093.04 And they have some tricks like they have this capacity factor right here, or they can reroute.1098.87999999999991098.8799999999999 These are very much engineering things, which are important, but you know, they don't change the1105.361105.36 sort of final, final result. Now, I want to go down here where they have a display of this sharding1114.241114.8 more like an explanation of the sharding, which I think is very illustrative. So how,1122.241122.24 what do they essentially do? If you think of many machines, you have 16 machines,1129.041129.04 so each little square here is one machine. Okay. Here are the different ways of how you can shard1138.39999999999991138.3999999999999 a model and model sharding. Now we are not going to build a machine anytime soon that can hold a1143.761143.76 trillion parameters just not going to happen. Okay, so you need to somehow shard the model1150.721150.72 or the data or both. And these are the different ways how you can do it. So if you use data1157.681157.68 parallelism, that is the easiest that is also directly built into things like pytorch and so on.1163.441163.44 What you do is, so the top row shows how the model weights are split and the bottom row shows how the1170.32000000000021170.3200000000002 data is split. So how to read this is when you do data parallelism, the weights are split such that1178.08000000000021178.0800000000002 each of the 16 cores has the same weights, you see, so this these weights right here are the same1184.881184.88 as these weights are the same, they're all the same. So this is sharded, the data is run,1191.36000000000011191.3600000000001 so that you take a data set, you take a batch of data. And now you distribute this data point goes1198.881198.88 here, this data point goes here, this data point goes here, and so on. You distribute the data,1205.681206.4 and you do the forward propagation. And at the end, you sort of gather them again, right? So you1212.961212.96 gather them together. Again, if because you have to, you know, calculate your gradient. Okay, so1221.441221.44 that's data parallelism, the model is spread out. And if you want to do an update to the model, then1227.60000000000011227.6000000000001 you need to communicate around these weights. Okay, so all these different pieces have to then1234.321234.32 communicate with each other when there's a weight update. If you do data parallelism, here is how1242.081242.08 the data split, we've already seen this. So one piece, this piece of data is split over 16 cores.1248.241248.24 So you can see like this core right here only has this little piece of the data, and not all of the1253.67999999999981253.6799999999998 data. On the other hand, you can do model parallelism. In model parallelism, you can see1260.81260.8 it's exactly the other way around, namely that one core only has a little piece of model, right? And1268.39999999999991268.4 but every core gets all of the data. So this data here, the bottom row is data, all of the data.1275.60000000000011276.5600000000002 The point here is that if you do model parallelism, that's what you do when the model1283.28000000000021283.2800000000002 itself doesn't fit right over here, the model fits on your machine, but not the whole batch1288.80000000000021288.8000000000002 at the same time, model parallelism you do when the model itself doesn't fit. What you have to do1294.56000000000021294.56 is you have to take your data, right? And you have to send it sequentially. So maybe this is the first1302.561302.56 layer, like that's layer one weights, and then you have to compute layer one, and then you have to1307.21307.2 send it to layer two, and so on. So you have to send it sequentially through the through the sharding1314.39999999999991314.3999999999999 of the model, right? Because you want to forward propagate through all of the model. This is has1319.521319.52 very, very much of a cost of communication, you can build very big models, but it comes at a cost,1326.961326.96 right at the end, you get your y and you calculate your loss and you back prop again, backwards1333.121333.12 through the whole thing. You can mix them, right, you can do model and data parallelism. So here you1340.41340.4 can see that the weights so this is this is layer one weights, layer two, layer three, layer four,1347.281347.28 and here again, you have layer one, layer two, layer three, layer four, and so on. So you can1355.441355.44 mix the two in that you can have model and data parallelism, if both your model and also your data1363.121363.12 don't fit in a single machine. And you can see here that the this upper left part receives,1371.441371.44 they receive the same data, but this here receives different data, right? So you split your mini1377.12000000000011377.1200000000001 batch into four different parts. And you send the first part up here, like that's data one,1383.921383.92 you send that up here, and that goes through the model in this sequence sequential fashion,1388.41389.2 you send data to right to here and so on. So we mix the two. Now, in expert and data parallelism,1398.01398.0 that's what they that's what they do in the switch transformer. So this here is the switch transformer,1405.281405.28 and this here, over here will then that's the switch transformer 1 trillion. So for the 1 trillion1411.921411.92 model, they actually need to mix all of them. But you want to add, you know, if you can, you want to1419.21419.2 avoid model parallelism, model parallelism is really the thing that kills you because of the1425.841425.84 very high communication cost. So in the switch transformer, they have expert and data parallelism.1432.81432.8 What does it mean? So the top row is how the model weights are split. And you can see the weights are1437.91999999999981437.9199999999998 split. But the different color means that they're different weights. So here are weights number one,1443.61443.6 weights two, weights, three, weights, four, and so on. Now, we've already had this over here, right?1450.95999999999981450.96 Different weights in the model parallelism case were split over different machines. However,1456.641459.76 if you look at the data, the data is also split, and the weights, they're not the same. And these1467.21467.2 are exactly these experts. So experts, this means that, you know, this piece of data here only goes1478.41478.4 to this expert, and then to the output, this piece of data right here only goes to this expert, and1486.721486.72 then to the output, right, there is no communication between the different experts, whereas here you1495.04000000000021495.0400000000002 have this super high communication. Okay, so you can see you can scale up the experts as you scale1500.961500.96 up your data, as long as each shard of data is routed to only one expert. And then of course,1507.521507.52 you can mix the expert model and data parallelism. If you really if not even a single expert fits on1515.21515.2 a machine, right? If that's the case, you need to again, shard you do model sharding on the experts.1521.521522.48 All right. So the switch transformer, as I said, this here is the switch transformer that the most1529.441529.44 of the paper is about. And now we can dive into the results. The results are pretty spectacular.1536.81536.8 They mostly come compare, as I said, to t five base, and t five large. And as you can see right here,1545.61545.6 the switch model has significantly more parameters. So 7.4, or here 26 billion parameters compared to1554.481554.48 not even a billion of t five large, yet the number of flops is matched. So they build models where the1561.91999999999981561.92 number of flops for a forward prop is matched, but the the number of parameters are higher. So,1570.01571.04 you know, it is somewhat of a fair comparison, right, you have the same amount of compute done1575.921575.92 per forward prop. And now we see what does it help to just have raw gain in parameters. And it turns1584.32000000000021584.3200000000002 out it helps a lot, you've probably already seen that we get these massive speed ups, massive sample1591.21591.2 efficiencies over a dense model. You've probably so this we've looked at exactly in the in the1601.12000000000011601.1200000000001 intro, they also have benchmarks on, let's see this down here, they also have benchmarks on1608.241608.24 multilingual multilingual data set. And you can see in every single language, the switch transformer1618.161618.16 gains on the dense transformer by quite a bit. So this is in this is lock space, as you can see,1624.161624.5600000000002 and it's quite impressive, actually. And these gains are in time as well as number of steps.1631.761633.3600000000001 So that's pretty, pretty cool. So, as I as I said, the the trade off here, of course, is that you need1643.761643.76 more machines, you need to actually add more machines. And you can see this largest model that1649.681649.68 they built is this switch XXL, which is matched in flops to transfer to the T5 XXL model, yet has1658.41658.4 many more parameters and beats the T5 at log perplexity ending, as I understand in downstream1666.481666.48 tasks by quite a bit. They also built this trillion parameter model. It is not as good,1675.761676.32 mainly because they, as I understand it, they just want to get to a trillion parameters. And I1683.841683.84 think I think it's you know, training isn't really easy at that size. So they scale it down. As you1691.521691.52 can see, it has less number of heads, less number of layers, but the number of experts are way up. So1697.361697.36 that's how they scale to a trillion. And the results are, you know, better than the T5 XXL,1703.61705.04 which is impressive, given that it has less flops per token. However, it is still worse than the1713.281713.28 switch XXL. So the trillion parameter model, it's still, you know, it's still not everything to have1721.761721.76 a lot of parameters, you actually need to do good trade offs. And here they've traded off too many1728.161728.16 parameters for, you know, less number of heads and less number of layers. And that hurts again.1735.361735.36 So, very, very interesting stuff right here. The last thing I want to look at is their tricks for1743.041743.04 getting this to work. So they detail three tricks for getting this to work. And they are right here,1751.841752.4799999999998 three tricks, how they can do this. And people before them have said, no, you need at least two1758.95999999999981758.96 experts, otherwise, it's unstable. So they do selective precision with the large sparse models,1765.521765.52 which means that if for some of these computations, it, you know, it, it pays off to do them in higher1777.84000000000011777.8400000000001 precision, you don't want to send around these flow 32 precision things, you don't want to send1785.21785.2 those from machine to machine, right? So you have your input, you have your multi head attention.1791.84000000000011792.56 And then here, again, this is whatever x prime, and then you send that to the experts. Right,1799.36000000000011799.3600000000001 here are the different experts. And then you send that back. And that's why, okay. Now, you don't1810.41810.4 want this here is communication cost. If you were to send around float 32 vectors, that's a lot of1820.641820.64 data that you have to transmit. So you'd rather send around 16 bit precision, right, as they do1826.881826.88 right here. And however, if you do 16 bit precision, you're you know, the whole machine1832.32000000000021832.3200000000002 learning part doesn't work as well. So what they do is they do as soon as it as a as soon as you1839.04000000000021839.04 as a as soon as a vector arrives here, this is in 16 bit, they scale it up, they cast it to a 321848.87999999999991848.8799999999999 bit vector, they calculate using the 32 bit vector 32. And then they cast it again to a 16 bit vector1858.481858.48 to send it back. And that seems to work. So they do selective, selectively casting the precision up.1867.121867.12 And also they do selective dropout that's down here. So they do expert dropout, which means they1875.041875.04 don't apply dropout to the whole network uniformly, as you would do regularly normally, but they say1882.47999999999981882.4799999999998 they can do a much larger dropout rate at expert layers. And that makes a bit of sense because the1889.361889.36 expert each expert is only used very sparsely. So it makes sense to up their dropout rate,1895.61895.6 because you know, in the end, you might drop out as much signal from a sparsely used expert, if you1903.121903.52 raise the dropout rate, then you do from a densely used layer in a with a smaller dropout late rate.1910.481911.1999999999998 And the last thing is that they simply do better initialization. So they find if they scale down1918.561918.56 the the initial scale of the original transformer by a factor of 10, that leads to a lot more stable1926.15999999999991926.1599999999999 training. It's astounding that after so many years, still something like initialization can,1933.91999999999981933.9199999999998 you know, make or break such a model that is just insane to see. There is a lot more to this paper,1940.961940.96 they do a lot of downstream tasks, they also talk a lot about, you know, this is not only this model,1946.39999999999991946.4 they do a lot of optimizations under the hood, they use mesh TensorFlow and so on. It's clear1953.12000000000011953.1200000000001 that a lot of work has gone into this. And interestingly enough, they can also distill1958.41958.4 these models. So what they can do is they can take this large model and they distill it to a model1964.41964.4 that is as big as t five base, a dense model. So they go from a sparse large model, and they distill1972.481972.48 it into a dense model that is equivalent to t five. And they do outperform t five, if it were1980.641980.64 trained from scratch, and they gain up to something like 30%. So 30% of the gains they made from here1988.161988.16 to here, they can retain by distilling it down. They say they can distill it down way over 90 95%1996.641996.64 of the model, which is also pretty interesting. And, you know, pretty cool. Because then you could2003.60000000000012003.6000000000001 sort of distribute the trained models around and people could use them. Alright, so that was it2009.60000000000012009.6000000000001 for me, definitely check out the paper and all the experiments downstream tasks and so on. It's a2014.642014.64 very cool paper has a lot of cool experiments. There's code, at least pseudo code. And that was2022.882022.88 it. Thank you. Bye bye.2027.68\"}"}