{"data": "{\"value\":\"0.0 Hi there, today we'll look at Big Bird transformers for longer sequences by Manil Zahir and Gurugra6.886.88 Rugganesh et al of Google Research.9.929.92 So this paper on a high level proposes to replace the quadratic attention mechanism14.614.6 in transformers by a mix of random attention, windowed attention and selective global attention,23.4423.44 therefore achieving a complexity of linear memory requirement instead of quadratic memory29.6429.64 requirement.31.2831.28 And as a result of that they can process longer sequences than traditional transformers like36.636.6 BERT and achieve better results in some NLP tasks and they also evaluate on genomics tasks.44.1644.16 So we'll go through this paper a bit, look a bit at the proof because they give a theoretical49.5649.56 kind of guarantee that their random attention mechanism can still be Turing complete and57.3257.32 still achieve the same things as a full attention mechanism.62.5262.52 But we'll also look at the drawbacks.64.3264.32 I sort of have mixed feelings about this paper and I think I'll voice my concerns as we go70.2870.28 through here.71.2871.28 But first let's look at the paper, let's look at the architecture and I think this is actually76.2400000000000176.24000000000001 a pretty cool paper for the empirical progression of the field to process longer sequences with83.483.4 transformers.85.1485.14 As always if you like content like this feel free to share it around, leave a like and90.3290.32 tell me in the comments what you think about the paper and about what I think, whatever,96.4896.48 you just go nuts.100.32100.32 All right so the basic premise right here is that the transformers they've been pretty108.64108.64 impactful especially in NLP.111.08111.08 So they say transformer based models such as BERT have been one of the most successful115.24115.24 deep learning models for NLP.118.12118.12 Unfortunately one of their core limitations is the quadratic dependency, mainly in terms123.32123.32 of memory, on the sequence length due to their full attention mechanism.127.96127.96 So really briefly the full attention mechanism that I've done, there are numerous videos132.6132.6 about attention mechanism BERT, attention is all you need and so on.136.34136.34 So if you want a detailed explanation of what that is just go look up the corresponding142.04142.04 videos but briefly what you'll have in NLP is a set of tokens, a sequence of tokens as148.52148.52 an input and you want to transform them layer after layer into sort of a higher order representation157.68157.68 of that same sequence.159.66159.66 And for that you build these layers out of nodes and you have as many nodes usually as164.96164.96 you have tokens in the sequence and the next set of, so each token is represented by a171.18171.18 vector at the beginning and each layer transforms this sequence as I said into sort of a higher178.10000000000002178.10000000000002 level representation.179.10000000000002179.10000000000002 So you want the vector of this token right here to be a better representation than the186.28186.28 vector was right here and you do that by incorporating information from all the other tokens into193.64000000000001193.64 that particular vector.196.35999999999999196.35999999999999 Now as I said this is called an attention mechanism and we don't actually have to go200.79999999999998200.79999999999998 into how it works right here but you can see pretty clearly that if you want to do this206.11999999999998206.11999999999998 for every token you need to have information routed from every token to every token like213.0213.0 from here to here, from here to here and so on and this is just one token and then you218.72218.72 need to do it for this token and for this token and for this token.222.16222.16 So what you'll ultimately get if n is your sequence length you'll get some n squared226.76226.76 amount of computation and memory requirements for this.231.24231.24 So this is a problem and usually this means that you know this sequence length in BERT236.28236.28 this is limited to something like 512 tokens which is okay for some applications but if243.92243.92 you want to summarize you know entire articles, entire books even or do question answering249.88249.88 with lots of context it's not really enough.253.4253.4 So people have been thinking about how to scale this input, how to scale this and of259.71999999999997259.71999999999997 course the main culprit is this quadratic attention mechanism because if you you know264.71999999999997264.71999999999997 double the 512 you need you know four times the amount of compute and memory.270.86270.86 So how does this paper go about reducing that quadratic dependency?275.84275.84 The goal right here is of course to get this to some O of n right because then as we double283.23999999999995283.23999999999995 the input length we simply need to double the compute requirements and that would be287.76287.76 fantastic and that's what this paper does and it does so without you know sacrificing293.15999999999997293.15999999999997 the properties of the transformer.296.28296.28 So here's the architecture that BigBERT proposes.300.96300.96 By the way BigBERT another character from Sesame Street I guess will continue the naming307.32307.32 here after Elmo and BERT.311.2311.2 I'm waiting for the model that's the count.316.64316.64 Yeah that's going to be a fun model but so BigBERT basically has three different types323.4323.4 of attention here.324.79999999999995324.79999999999995 These are adjacency matrices in this attention mechanism.327.84327.84 So here is the input layer and the output layer is right here.333.64333.64 So that basically means that node i right here would be connected, sorry that's not339.2339.2 a straight line, would be connected to this particular node and also to this particular344.67999999999995344.67999999999995 node.345.67999999999995345.67999999999995 So we're now trying if we have node i right here we're now trying to not connect it to353.52353.52 all of these nodes but we'll say we'll just select some at random and then connect it359.91999999999996359.91999999999996 to that.360.91999999999996360.91999999999996 Okay this is what we call random attention and you can pretty clearly see if you connect366.65999999999997366.65999999999997 each of the i nodes to r equals 2 to two random nodes then you don't have an n squared anymore376.2376.2 but you'll have an like an O of r times n which you know if r is a constant is an O383.64383.64 of n attention mechanism.386.76386.76 Okay so the main goal between the random attention mechanism is that for each query basically394.76394.76 you select random tokens that you attend to and that random number is a fixed number that's402.08402.08 not dependent on the sequence length.405.28405.28 And the paper is a little bit unclear about whether or not those random ones are the same412.4412.4 for every sequence or are switched up or are the same for every layer or are switched up418.0418.0 but they formulate all of this as sort of in sort of a graph in sort of a random graph.423.91999999999996423.91999999999996 So they formulate the attention mechanism in form of a graph.429.03999999999996429.04 So if we transform all of these nodes into a graph a full attention mechanism would mean435.28000000000003435.28000000000003 that each graph each node is connected to each of the other nodes right fully connected441.88441.88 graph I don't maybe that's it.447.12447.12 So that would be a full attention mechanism and then they say well if we just have random453.48453.48 connections between these things then there are some theorems from graph theory that say459.68459.68 that each random walk in this graph is going to so this graph is going to mix pretty quickly466.12466.12 so I can get from each node to each other node by a random walk in a logarithmic time473.28000000000003473.28000000000003 and this random walk which basically means that you go from here to here this would be478.70000000000005478.7 one layer of the transformer and then if you want to go from here to here that you would484.7484.7 have to do that in the next layer so this formulation as a random graph leads me to489.65999999999997489.65999999999997 believe that layer after layer the random attention pattern is going to be the same496.8496.8 but also the formulation of the paper leads me to believe that this random attention differs504.48504.48 from sequence to sequence so I believe what's happening is that they you know get a new510.04510.04 sequence then they decide on this pattern right here once and then they use this layer516.16516.16 after layer the same pattern again so you can see that in the traditional attention524.32524.32 information can basically flow from each of the nodes to each other node in one single531.2531.2 step right because each node is connected to each other node you see this in the graph535.72535.72 right here however if we only select a subset then you know it needs to if I want to go544.6800000000001544.6800000000001 from as I said from here to here then I need to do it in two steps and therefore I need551.2551.2 two layers and that's going to be the culprit of this method here and you know while it556.38556.38 is mentioned in the paper it's sort of I feel at least that's my my assessment of this paper563.16563.16 it's kind of swept under the rug a little bit I mean they do have a theorem that clearly568.22568.22 says we can construct an example of a task that in the full attention setting can be575.52575.52 solved with a single step so a single layer that in our random attention setting needs582.56582.56 a lot of layers so a lot of steps but you know the rest of the paper is sort of shaky589.52589.52 on on this thing but nevertheless you can see how the random attention can if you have596.92596.92 enough layers do the same information routing as the full attention okay however this is603.9599999999999603.9599999999999 not a property of the random attention and we'll see this in the next thing right here609.8609.8 so the next ingredient that this paper uses is window attention and you can see over here615.76615.76 that Big Bird is ultimately going to be a combination of the three types of attention620.64620.64 which will which we are looking at here so window attention basically means that each627.0799999999999627.0799999999999 each eye each token at the eye of position is going to attend to itself of course so633.68633.68 here is I but it is also going to attend to its neighbors so here is I minus one and here640.92640.92 is I plus one and this is a you know this is a window size W that you can that is a647.52647.52 parameter but also it is a constant and therefore you again go from n squared to W times n which656.8399999999999656.84 you know is O of n if W is a constant and this might be familiar to you because we've663.72663.72 already seen this in the long former papers I've made a video or I think even two videos670.32670.32 on the long former which used exactly the window attention in combination with the global677.0677.0 attention and if you want to know more about that go watch these videos but the new thing683.0683.0 in Big Bird right here is this edition of the random attention again the the window692.6692.6 here is is has exactly the same properties as the random attention so you have instead699.82699.82 of a fully connected graph you have a sparsely connected graph now if you have random attention706.92706.92 the sparsely connected graph is like like the one on the right but if you have a windowed712.72712.72 attention you can it is kind of not randomly connected but each node is connected to its719.32719.32 neighbors like this and you can also see that if I want to go from this node to this node725.5600000000001725.5600000000001 right here I can't do it in one step but I can do it in two steps I go here and I go731.64731.64 here so in the terms of the attention layers if I want to go from node one to node three741.5600000000001741.56 I have to do it in two steps because each node is only connected to its neighbors so746.16746.16 the connection patterns would sort of look like this so I have to go from one to two754.0754.0 and then in the next layer from two to three so the paper basically makes up for the lack760.92760.92 of full attention by adding layers and you all also might recognize this from a convolution768.1999999999999768.2 operation like this basically because it is a convolution operation right in a convolution774.5200000000001774.5200000000001 each node a only aggregates input from its neighbors for the next layer and then we know781.4000000000001781.4000000000001 that as we go up the layers the de facto window that each node looks at is going to be like787.5787.5 a cone kind of like this so this is very similar to how a convolutional neural network works794.3000000000001794.3 and the reasoning is very similar because the reasoning is well in a sentence the most799.7199999999999799.7199999999999 important words for any given word are probably going to be its neighbors like the words around805.56805.56 it and as you go up the layers you branch out more and more but ultimately the this811.5999999999999811.5999999999999 neighborhood principle holds in NLP as well so again we already saw this in the long former819.76819.76 but that's the reason behind the window attention and that's the second ingredient and then824.0799999999999824.08 the third ingredient is this global attention now the global attention is selected tokens832.1600000000001832.1600000000001 that are so important and that's you know fixed by the developers that are so important838.88838.88 that they are they are connected to everything else so for example in these transformers846.3000000000001846.3000000000001 you often have what's you know this kind of CLS token so this is a special token that853.08853.08 you prepend to some piece of text and the output of this token is going to be your classification860.1600000000001860.1600000000001 output because you don't want to bind your classification if you need to classify the865.34865.34 entire sequence you don't want to bind that decision to one particular word what you want871.0400000000001871.0400000000001 to do is you will have an extra token and that's this CLS token that kind of aggregates876.48876.48 information from all of this so layer after layer layer after layer you'll have so if883.48883.48 we go here layer after layer we have this one special node and in each step every single890.94890.94 other node is able to send information right here to this node and receive information898.48898.48 from this node okay so now as a result of this as you as you may be able to see every908.44908.44 single every single path is kind of maximum length of two because if I want to go from914.8000000000001914.8000000000001 any node to any other node I can simply you know send information to this global node920.28920.28 and then the global node in the next step can send information to whatever other node926.36926.36 and that is a property that they use in their proof that this attention mechanism is as933.6800000000001933.6800000000001 sort of as powerful as the classic fool attention mechanism and we'll go through that in one939.0939.0 second but first I hope this was clear that this combination of random attention window944.52944.52 attention and global attention is what is called Big Bird okay they have some engineering954.08954.08 tricks that go along with this but in concept you can imagine Big Bird being long former959.5600000000001959.5600000000001 plus these random attention right here and you know as an engineer as an NLP engineer966.14966.14 that makes kind of total sense I you know I totally believe that a the introduction972.7972.7 the addition of these random attention of these random attention patterns can absolutely979.36979.36 help your classification or whatever your NLP tasks because you know more attention986.1800000000001986.1800000000001 better and I also am completely willing to believe that you know using the full attention992.36992.36 matrix while it is of course more accurate it won't hurt too much to leave some of that999.16999.16 attention away because essentially all the path lengths are just becoming two or even1004.961004.96 with the random attention are really short or logarithmic to route information from a1010.361010.36 node to some other node so the loss that you incur is kind of in a logarithmic scale in1018.241018.24 terms of performance while the gain that you make is sort of in a in a quadratic or like1024.12000000000011024.1200000000001 a linear scale you go from quadratic to linear and that seems to me like a good empirical1030.321030.32 trade-off all right however the the proofs here the proof of of how how these how these1042.761042.76 things are constructed are a little bit I don't know so what they do in the proof that1050.15999999999991050.1599999999999 this function can sort of appropriate is a universal approximator people have already1055.43999999999981055.44 shown that full attention mechanisms are universal approximators so they show here that this1062.441062.44 sparse attention mechanism is also a universal approximator they make big use of star graphs1068.36000000000011068.3600000000001 what they say is okay if we have a star graph which is one node connected right here to1074.01074.0 every other node this is a star graph if we have a star graph we can achieve the same1082.01082.0 thing then with a full graph a full graph is where every node is connected to every1086.761086.76 other node but as I already said what they need for this is multiple layers of this star1093.041093.04 graph so and that has to do with the fact that if I want to route information I basically1099.41099.4 have to go via this middle node right here and there is an additional complication because1105.241105.24 this middle node in our case right here is only one node I can't route information at1112.361112.36 the same like I can't have this routing right here at the same time that I have this routing1120.881120.88 right here like going from here to here because I only have one middle node and I kind of1126.01126.0 this is not how that like this is very dumb math but maybe you have to imagine that there1132.961132.96 is one memory slot and you can only use that one memory slot at the same time for one of1140.41140.4 these things so essentially what you'll have to do is you'll have to do the green thing1145.241145.24 first and then in the next step you'll have to do the blue thing second and then so these1151.81151.8 are now pairwise routing between nodes but ultimately what an attention mechanism does1156.81156.8 is it does everything to everything right in a single layer it routes information from1161.661161.66 all the nodes to all the other nodes and to achieve that you need multiple rounds of this1168.62000000000011168.6200000000001 and it turns out that in the worst case you actually need n rounds of this so you know1175.281175.28 you trade off your you go from n squared to n memory and compute requirements in a single1182.60000000000011182.6000000000001 layer but in the worst case you need n layers to recover the power of the full of the full1190.60000000000011190.6 transformer and that is the last one of their theoretical results right here so first they1196.981196.98 prove universal approximations and second they prove Turing completeness these two properties1203.95999999999981203.9599999999998 have been proven for full attention mechanisms and third they prove that there are tasks1209.95999999999981209.96 where we actually do need n layers to solve them with their limited attention so you know1221.01221.0 I'm not sure but I feel you can make any sort of polynomial algorithm into a linear algorithm1229.12000000000011229.1200000000001 like this like I have a like a cool sorting algorithm right so if this is my sequence1234.41234.4 that I want to sort what I can do is I can simply you know take a random subset of them1241.28000000000021241.2800000000002 like this this and this and then kind of go and sort them and then put them like I sent1247.60000000000011247.6000000000001 them to the to the global memory like this I sort them and then I put them back right1255.36000000000011255.3600000000001 and if I do this for enough if I do this for enough rounds okay you know if I do this for1262.80000000000021262.8 enough rounds you know at the worst case I need n rounds to sort my or log n rounds1268.121268.12 if I do it smartly but you know in you know the single step here is the single step is1275.521275.52 just O of n so I have now an O of n sorting algorithm I you know I have my sort of a bit1283.741283.74 of wary to express things like that and yeah but you know it is from an empirical standpoint1293.681293.68 I absolutely believe that this this is enough now my second coral right here is that if1301.521301.52 you look at the proof first of all what it makes use is this star graph and the star1307.61307.6 graph corresponds to the global attention so that's not much to do with the random attention1312.841312.84 though they use the random attention in their proof but I at least believe that it would1318.521318.52 be possible with the global attention only and then the second thing is if you look at1325.91999999999981325.9199999999998 the parameters that they use for the for the experiments and I've already set this in the1332.841332.84 long former video so in the long form of video it turned out that if you look at how big1338.841338.84 this window attention is it turns out that it you're still well you know the original1345.67999999999981345.6799999999998 BERT attended to 512 tokens and then you look at the window and the window was still 5121352.87999999999991352.8799999999999 tokens it's just that the global attention was even more so ultimately they ended up1356.87999999999991356.8799999999999 using more memory than the original BERT and here if I look at the parameters of their1365.361365.36 thing and they have multiple experiments right here and I believe this is the base version1371.15999999999991371.1599999999999 so that this is the base version they also have this large version but here this is the1377.43999999999981377.4399999999998 12 layer version and you can see they have this block length and we'll get into the block1385.781385.78 length in one second but then you can see that their window size is three times the1391.91999999999981391.92 block length the number of random tokens is three times the block length and the number1397.161397.16 of global tokens is two times the block length so that results in eight times B so eight1403.481403.48 times 64 is you know can I calculate this or am I stupid it's 512 yes I actually calculated1418.881418.88 this before so this is 512 tokens so you know you you go from from BERT that has 512 tokens1429.161429.16 and attends to 512 tokens to also attending to 512 tokens of course the advantage here1436.28000000000021436.2800000000002 is that they now have 4096 sequence length so they have the freedom to not attend to1447.41447.4 as many tokens as they have in the input length but you know to put it in perspective this1454.441454.44 here uses more memory and more compute on it on its face than BERT because BERT attends1463.54000000000021463.5400000000002 to as many tokens but has a smaller input sequence and you know I there's sort of a1472.41472.4 thing where in order to make these sparse attention things work you have to go pretty1478.21478.2 pretty you know high in the number of things you attend to you can leave away some but1483.32000000000021483.3200000000002 it's not like you can scale up orders of magnitude of your input sequence length so that's the1490.921490.92 this promise of linear attention is sort of it's kind of fulfilled but not there yet the1496.28000000000021496.28 second thing I would like to point out is that in a lot of cases the number of random1502.441502.44 tokens is actually set to zero so really making use I believe of these of the of the global1509.87999999999991509.8799999999999 of the number of global tokens so it that seems a bit strange in that they continuously1517.841517.84 refer to their random attention mechanism but then in a lot of experiments they don't1523.63999999999991523.64 actually have a random attention mechanism I believe they have to do that because that's1529.32000000000021529.3200000000002 kind of what makes them different from the long former in principle but still yeah so1537.881537.88 the last novelty let's say is an engineering novelty in that they now always consider not1545.181545.18 single for example they don't consider single random attention they always consider these1549.721549.72 in blocks and that's because our current hardware is really bad at sparse stuff really bad at1557.061557.06 single indexing gathering single things so if you can do everything in blocks you basically1563.761563.76 get you get these blocks almost for free so it takes only marginally longer to retrieve1570.41570.4 this full two by two block right here than it would to retrieve the single instance right1575.91575.9 here of course that means you have you know four times you still use four times more memory1582.161582.16 but it is not four times slower than the original thing so you can use these blocks right here1589.961589.96 you can do it for the random attention you can do it for the window attention as you1593.32000000000021593.3200000000002 can see here so you break this window pattern a little bit into blocks and that makes it1599.881599.88 a lot faster so that speeds up and get the speed up almost for free and then they make1606.41606.4 another approximation in that the way they do this windowing is and let's just go really1614.71614.7 briefly so you can see right here that it would be very cumbersome to gather so what1624.84000000000011624.84 we need we're just going to focus at this this dotted thing right here is a bit confusing1630.321630.32 so you want to attend to these things and these you can just get out with a matrix slice1637.87999999999991637.8799999999999 really easy but then you want to attend to this kind of blocky thing right here from1643.721643.72 the window attention right like this thing and this is hard to get out because you'd1649.63999999999991649.64 have to kind of index each row individually and that's very slow so what they do there1655.82000000000021655.8200000000002 is this matrix role operation where you can sort of roll the axis around so what you'll1661.56000000000021661.5600000000002 do is you'll take this thing right here and you put it to the left right here and you'll1667.261667.26 take for example this thing right here and you'll put it to the right or no like it's1674.08000000000021674.08 up and down but in essence that's what you do and you can you can fold all of this blue1679.67999999999981679.6799999999998 stuff into a rectangular matrix if you know if you can see right here so you kind of roll1688.361688.36 this back roll this back roll this forward and you replace whatever is missing by these1695.561695.56 now this again gives you some inaccuracies because this block right here was never intended1702.521702.52 to be attended to and all of a sudden you see you have the k6 in here so it gives you1709.63999999999991709.6399999999999 a bit of inaccuracies at the edges of the sequence but you can take that you know you1716.01716.0 can take that hit for the increased performance that you gain by now having a rectangular1720.961720.96 matrix tpus are really efficient at this not as efficient at this and then the only thing1728.561728.56 that's really slow is gathering these random blocks right here but also by having the same1734.761734.76 amount of random blocks per input token what you'll do is you'll end up with just one of1741.361741.36 these columns right here or you know are of these columns and that again gives you a rectangular1747.041747.04 matrix so this thing right here you can process very very efficiently using a tpu and you1753.721753.72 know the mistakes you make are basically this thing right here and this thing right here1760.41760.4 because those weren't intended and are at the edges of the sequence so these were the1766.641766.64 tricks of big bird to quickly summarize big bird is basically taking a transformer saying1776.241776.24 well why do we need all of this attention all of this full attention maybe we only need1782.481782.48 some of that and can already do a big job a good job especially now considering the1787.61787.6 attention mechanism goes over multiple layers so we don't need a routing from each token1794.221794.22 to each token we we can make up for not having a fully connected graph by simply running1800.31800.3 multiple layers so their sparsity is first of all you have this random attention which1808.36000000000011808.36 i believe changes from sequence to sequence but stays within or among the layers of the1814.081814.08 same sequence then you have the window attention with the reasoning so the reasoning behind1819.63999999999991819.6399999999999 the random attention is that if you have a randomly connected graph the path lengths1824.121824.12 are on average logarithmic so you can route information efficiently the reasoning behind1829.321829.32 the window attention is that probably neighbor information is very important and that has1835.67999999999981835.68 been shown empirically and then the global attention the reasoning behind this is that1840.56000000000021840.5600000000002 some of the tokens that are fixed by the developers are so important that it it's very beneficial1847.721847.72 that each other node is connected to them and that they are connected to each other1852.441852.44 node the result of that is the big bird attention mechanism which is basically long former which1859.681859.68 already had these two plus the random attention this achieves a linear linear complexity in1869.41869.4 terms of of memory and compute though linear has to be qualified a bit because it's modified1875.241875.24 by the window size by the number of random attention tokens by the number of global tokens1881.761881.76 and in practice often ends up being you know fairly large ish and also the the the theoretical1892.61892.6 guarantees now come with the fact that you need multiple layers in the worst case you1897.841897.84 need sequence length amount of layers which you know in the worst case would result right1903.21903.2 back into a quadratic requirement for memory and compute they do some engineering some1911.481911.48 engineering tricks right here and their results are pretty good so the results in various1919.41919.4 tasks and we'll we'll look at some of the tasks right here so these are def set results1926.761926.76 using base size models for example where you can see they do outperform basic roberta models1934.081934.08 they outperform long former which may mean that the random attention is useful but you1939.721939.72 know in these things it's all also always may just mean that you've thrown more compute1945.041945.04 at it at least I'm not really looking that they outperform the models because as you1951.36000000000011951.3600000000001 can see right here if they compare to state-of-the-art and you know granted these are models that1955.961955.96 have been trained specifically for these tasks and are you know crafted and engineered and1962.321962.32 big bird manages to big bird manages to hold itself against them in a lot of tasks and1968.981968.98 even get state-of-the-art on some what I'm more interested in is that it you know it1974.41974.4 can reach good numbers it doesn't necessarily have to be state-of-the-art but it can reach1979.921979.92 good numbers which tells me that okay probably the the empirical hit that I take by not having1988.12000000000011988.1200000000001 the full attention is you know is justifiable by the speed up and memory savings I do get1995.041995.04 yeah especially when result when you see results mixed like this you know sometimes the other2002.39999999999992002.3999999999999 model is good and sometimes the big bird is good on different variations and so on I would2008.842008.84 not you know I would not make a big deal out of the fact that it is state-of-the-art I get2013.482013.48 that the authors have to do that I would do so as well but you know you don't don't think2020.62020.6 that this is the like the best thing now it's very probable they just thrown also a lot2027.122027.12 of compute at it what is cool is they do some genomics experiments so not only do they have2033.82033.8 NLP state-of-the-art but also they go into genomics and experiment with data there don't2040.67999999999982040.6799999999998 want to go into that because you know ultimately it's another task and that believe the paper2045.41999999999982045.42 is about the architecture alright so that was big bird I hope you enjoyed this video2052.922052.92 and learned I learned something certainly if you want to check out the proofs they're2059.62059.6 actually pretty entertaining to read and yeah I'll see you next time bye bye2079.16\"}"}