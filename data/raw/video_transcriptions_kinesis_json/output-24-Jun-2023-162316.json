{"data": "{\"value\":\"0.0 Hi there, today we'll look at deep residual learning for image recognition by Kaiming5.60000000000000055.6000000000000005 He, Xiangyu Cheng, Shaoqing Ren and Jian Sun.11.3611.36 So this, you know it, this is an old paper.15.6815.68 It is from 2015.18.218.2 But I thought we'd still look at it because this not only is it one of the most influential23.9223.92 papers in modern deep learning, it is also a very well written paper.30.16000000000000430.160000000000004 And I remember it like it was yesterday when this came out.34.234.2 This was like a bomb.37.12000000000000537.120000000000005 So around that time, this meme was going around.42.2842.28 I was winning ImageNet, but then someone made a deeper net.48.4448.44 This was a, this was a time when after AlexNet, people were trying to build bigger and bigger56.1856.18 networks.58.0458.04 And every time someone managed to build a bigger network, the accuracy on ImageNet data63.963.9 set would increase pretty much in lockstep with how much bigger the network was.70.1670.16 But people got to the limit of building big networks.74.2874.28 And then this paper drops and changed everything.78.0878.08 And now residual connections are everywhere, not only in image recognition, they are in83.3683.36 transformers, they are in whatever, wherever you go, you'll probably find some residual89.889.8 connections somewhere in there.93.893.8 So yeah, let's let's look at this paper.97.4897.48 And let's revisit what kind of problems people had then and how they solved it.104.25999999999999104.26 So here they go directly into into this problem of deep neural networks.112.36112.36 And the problem that people had was they knew that if you can increase the if you can increase120.36000000000001120.36000000000001 the depth of a neural network, you can make it perform better, you can make it generalized126.04126.04 better, you can reach lower training loss, but optimizing it was hard.131.8131.8 Specifically, this was a phenomenon that people observed.134.84134.84 If you have a 20 layer neural network, you could train it and you know, there's this138.78138.78 learning rate drop, people have had already figured out that you need to drop drop the143.74143.74 learning rate, and it would reach a certain level.147.96147.96 And here, this would be the test error over here.151.68151.68 However, if after a certain point, if they increase the depth even more, the training159.04000000000002159.04000000000002 error would actually go up again.161.46161.46 And so with the test error, and this is not a problem of overfitting, because overfitting167.44167.44 would be when the training error is lower or as low, and then the test error went up.173.60000000000002173.60000000000002 So this is the first thing, this is not a phenomenon of overfitting of too many parameters.178.24178.24 So why can't we train bigger layers networks until that time have very much followed kind185.06185.06 of the original network design that was envisioned by sort of people like Yann LeCun, and also193.8193.8 Alex net, and the most popular ones were these VGG nets.198.48198.48 And they were very much of the philosophy that you'd have like some, you have the image203.64000000000001203.64000000000001 here, and you input that into into convolutional layers, which first would kind of keep a big213.24213.24 resolution, but would increase the channel size by you know, some amount, and then you219.84219.84 would sort of downscale the image as you increase the number of filters.225.94225.94 So you would stack more and more filters, when I draw more filters, you would stack230.8230.8 more and more filters while downscaling the resolution of the image.236.74236.74 The reasoning was that if you do image classification, right, then, you know, where on this where243.72243.72 on this, maybe you want to classify this into a Lego tower or whatever that is.251.28251.28 It's not that important where it is.253.4253.4 So on the lower levels, you would want to parse out like very low layer features like258.92258.92 edges and so on.260.48260.48 And these are still important where they are right, the fact that here's an edge, here's264.52264.52 an edge, here's an edge.266.12266.12 But then as you go higher up and go to more and more abstract features, and we already271.0271.0 knew that these neural networks, they tend to learn more and more abstract features as276.22276.22 you go up the layers, the hypothesis was that the exact localization of these abstract features282.08282.08 would be less and less important.284.32284.32 So if there is if you recognize that there is a rectangle, it's not that important where289.4289.4 it is, just that it's somewhere there, and maybe where it is in relation to the other.295.36295.36 So if you have, if you recognize, want to recognize a car, the lower layers would recognize301.64301.64 the fact that there are edges.303.72303.72 And then the intermediate layers would recognize the geometric shapes of maybe here, the wheels308.56308.56 and these bodies.310.40000000000003310.40000000000003 But it's not that important where exactly they are.312.64312.64 And then the higher layers would learn to combine the individual parts to each other.319.44319.44 And again, it becomes less and less important where these things are.324.6324.6 And more and more important that you build more expressive features.327.54327.54 So people would downscale the resolution upscale the number of filters.331.84000000000003331.84000000000003 Now that's a good heuristic.333.72333.72 But this is based this was basically the architecture of these networks.340.20000000000005340.20000000000005 And we would question why would if we increase the number of layers, so if we instead of346.6346.6 one here, we have two of these layers, right, we simply have two of these layers.353.68353.68 And here we have two of these layers.356.04356.04 Why does it get worse?359.36359.36 Especially this paper here makes an interesting observation.365.84000000000003365.84000000000003 So it is not caused by overfitting.370.1370.1 And adding more layers leads to a higher training error.375.72375.72 The degradation indicates that is not all systems are similarly easy to optimize.381.36381.36 Let us consider a shallower architecture and its deeper counterparts that adds more layers385.48385.48 onto it.387.14387.14 There exists a solution by construction to the deeper model, the added layers are identity392.24392.24 mapping and the other layers are copied from the learned shallower model.396.52000000000004396.52000000000004 So pretty easy if you have a shallow model, like five layers that learns a particular403.36403.36 function, I can pretty easily prove that there is a deep model that learns the same function408.92408.92 by simply copying over these five layers and having these here learn the identity function.416.24416.24 Okay, so if we're able to learn this, we should be able to train this network to at least422.88422.88 the same accuracy.424.04424.04 That's what this paper argues, because it can, you know, these layers can simply learn428.56428.56 the identity function.430.04430.04 So it must have something to do with the easiness of optimizing these deep architectures, not436.52000000000004436.52000000000004 with overfitting.437.52000000000004437.52 And this is, I think, if you read the entire text here, it's very, very clear.444.24444.24 If you read it, they lead you through this reasoning saying that, look, all these layers449.91999999999996449.91999999999996 have to do is learn the identity function, and then we could at least get the same accuracy.455.91999999999996455.91999999999996 So so what?458.71999999999997458.71999999999997 Why don't they learn the identity function?460.71999999999997460.71999999999997 Well, because we initialize most weights, you know, towards zero, we initialize them466.12466.12 randomly, but mostly we initialize them around zero, our initialization procedure, usually471.68471.68 sample from some Gaussian with some kind of a standard deviation, but around the mean477.56477.56 of zero.479.64479.64 And also, if we use things like weight decay, l two regularization, all of these things,485.0485.0 they do they bias the weights towards zero.489.06489.06 So if there is any natural thing that these networks are good at is they learn the zero495.0495.0 function really well, learning the identity function is as difficult as learning any other502.04502.04 function, the identity function, the convolutional filter is actually pretty difficult to learn.508.2508.2 Because you know, if I have a my, if I have my three by three filter, where is my no,514.28514.28 no, this is my three by three filter, the identity function is it like a one here and522.16522.16 zeros everywhere else.523.48523.48 That's the that that would be one of the things it's not that easy, you need to learn nine528.84528.84 weights in the correct way.532.98532.98 So this paper says, Can we do something to make the default function of the network not539.0539.0 be the zero function, or whatever the randomly initialized function?542.9200000000001542.9200000000001 Can we make the default function, the one function, can we make the default function,547.36547.36 the identity function?549.46549.46 And that brings you to residual connection.552.52552.52 So instead of learning to transform x, via a neural network into x, which is the identity560.52560.52 function, why don't we have x, stay x, and then learn whatever we need to change?570.24570.24 Okay, so if let's call that tilde, if the assumption is that it's a good default to577.12577.12 not change much, so this is almost the same as this, we might make this, build this directly584.84584.84 into the architecture, the fact that these two are equal, plus, plus some deviation that592.44592.44 is learned right here.594.28594.28 And the hypothesis is that, especially the deeper you go, if you go very deep, each function601.78601.78 here will actually learn not that much, it will learn to basically change the signal606.74606.74 a little bit, but mostly, it will learn the identity function if it behaves well.612.08612.08 And therefore, it might be, you know, reasonable to build this into the architecture.616.96616.96 And of course, this has turned out to be very accurate, it has actually been reasonable622.32622.32 to build this into the architecture.624.38624.38 So that's what they propose right here.627.4627.4 So instead of just having weight layers one after another, what they propose is to have633.0633.0 these skip connections in here.635.9635.9 So these skip connections, they will, instead of learning the function, they call this entire640.8640.8 function h of x, which might be very complicated, they learn the function, whatever f and f650.28650.28 is whatever you need to change about x, you see, at the end, you add x to it.655.52655.52 So these weight layers here, they simply learn whatever makes this next, this output different663.72663.72 from this input, and learning differences.667.3000000000001667.3000000000001 Now you have the desired property, because what do we know about weight layers from before?672.48672.48 Well, they tend towards the zero function, right?675.5600000000001675.5600000000001 If we use weight decay, or generally how we initialize them, they tend towards the zero680.48680.48 function.681.48681.48 Well, if f tends towards the zero function, then h becomes the identity function.688.1688.1 So the default function of this network is the identity function.692.6800000000001692.68 And whenever we learn something, we learn how to deviate from the identity function.696.9599999999999696.9599999999999 And that is, that is a much better default function.702.64702.64 Now it's not entirely true that the default function is the identity function, you see706.4799999999999706.4799999999999 that here, for example, there's after the skip connection, there is actually a relu.712.76712.76 So there's still it's still a nonlinear function in total the network in total.718.42718.42 But the default for the individual blocks here is the identity.723.0799999999999723.0799999999999 Okay, now if you chain these blocks, you get a residual network.728.12728.12 And that's what they propose right here.731.36731.36 So on the left, you see this original VGG architecture, like we described it.736.42736.42 So you can see you have an image which has four channels, and you first up it to 64 channels,742.8399999999999742.8399999999999 you keep the resolution, and then you max pool, which halves the resolution, but you747.76747.76 go up with the filters to 128, you max pool again, go up with the filters, and so on.755.4755.4 Now this has, even though it doesn't look like it, this has a lot of parameters, and761.8761.8 it needs a lot of computation.763.2763.2 So it has 19.6 billion floating point operation for a forward pass.768.84768.84 In contrast, the networks we're going to build here, the residual networks have 3.6 billion775.68775.68 flops.776.76776.76 So they are much, much less in terms of complexity than the old VGG networks, while still being784.68784.68 much deeper.785.88785.88 Okay, the hypothesis is the deeper, the better.790.28790.28 And as a trade off per layer, you don't actually need to have that many parameters because796.16796.16 you don't learn that much per layer.798.62798.62 But the succession of layers gains you much more than simply having single massive layers.804.72804.72 You can see at the same size of resolution here, you the the resonance can get away with810.6800000000001810.6800000000001 much less amounts of filters.813.58813.58 And that's why they are less, they are of less size.818.88818.88 So this is the comparison, the VGG 19.822.1600000000001822.1600000000001 Now they do build this 34 layer network, which they call plane.826.24826.24 And you can see, it is simply a 34 layer network with no pooling right here.833.62833.62 And here instead of pooling, they do a stride to convolution, which has also become, this838.84838.84 has become kind of more standard than doing max or average pooling to downscale to do844.92844.92 simply stride to convolution.847.18847.18 So this paper has actually set the standards for a lot of things in modern deep learning.853.44853.44 So our goal is to going to be to compare, first of all, the VGG 19 to the 34 layer plane861.16861.16 to show that you lose performance when you simply up the number of layers.866.64866.64 But then when you introduce the residual connections, as you can see right here, so there is always872.28872.28 this jumping connection right here.874.6999999999999874.6999999999999 So along these jumping connections, the signal can travel as the identity function.879.48879.48 What we're going to see is that if we go from plane to residual, introducing no extra parameters,886.36886.36 just the skip connections will change everything.890.78890.78 We'll make this network all of a sudden trainable and make the deeper networks the better networks.898.16898.16 Okay, the only little caveat here is, of course, in order to build a residual connection, the905.04905.04 output has to be of the same size as the input because you need to add the input to the output.910.56910.56 And this here, for example, is not given.912.36912.36 So here, you can see this signal after this layer is going to be half as big because it's918.9599999999999918.9599999999999 a stride to convolution.920.76920.76 So the output right here is only half the size, but it is twice the number of filters.929.16929.16 You can see right here, this has 64 filters.932.08932.08 And here we go to 128 filters.934.8934.8 That's why this connection right here has parameters in order to simply expand the number941.36941.36 of filters.942.36942.36 So these are these one by one convolutions that simply project the 64 filters to 128950.88950.88 filters.951.88951.88 However, this doesn't introduce too many parameters because it's only one by one.957.24957.24 In fact, here, the 34 parameters residual network.963.0963.0 No, I'm wrong.965.5600000000001965.5600000000001 You have different options.967.14967.14 So the world has ended up at the option of doing one by one convolutions.973.1973.1 But in this paper, they still they still explore three different options.977.28977.28 And I guess here in this particular experiment, the option A is simply to zero pad.983.12983.12 So to leave the first 64 channels, but to simply append 128 zero padded filters there993.8993.8 or channels, option B is the one by one convolution.998.64998.64 And option C is actually that all of these connections right here also have the one by1004.83999999999991004.8399999999999 one convolutions, which introduces extra parameters.1008.43999999999991008.4399999999999 And they they realize that option C isn't improving over option B substantially, and1015.681015.68 in fact, is only improving marginally.1018.561018.56 And they say, okay, that's probably just because we have more parameters.1021.541021.54 So ultimately, they went with option B. And I think that's what the world does right now.1028.561028.56 I also I when I read this first, I particularly enjoyed this paragraph right here.1034.61034.6 Let's read it together.1036.01036.0 Our implementation for image net follows the practice in the image is resized with the1040.681040.68 shorter randomly sampled in this for scale augmentation a this crop is randomly sampled1045.781045.78 from the image or its horizontal flip with the per pixel has been subtracted.1049.841049.84 The standard color augmentation is used we adopt the batch normalization right after1053.67999999999981053.6799999999998 each convolution before activation we this an age old discussion was born when to use1060.31060.3 batch normalization before count before the activation or after the activation I still1066.01066.0 I think people are still fighting over this today.1070.51070.5 We initialize the weights as in 13 and train all plane residual nets from scratch use SGD1077.19999999999981077.2 data data data, the learning rate starts from this is divided by then.1081.08000000000021081.0800000000002 So here in this paragraph, in this paragraph, they detail basically all the training procedure1086.81086.8 and all the tricks that they use that I remember specifically that, you know, I've read all1091.521091.52 of this, which was the idea and I could follow like, Oh, this is super well explained.1096.281096.28 This is so cool, and so on.1098.721098.72 And then I expect basically an implementation of that.1101.681101.68 And then there's one single paragraph with like 20 lines saying, Oh, and by the way,1106.441106.44 we use these 50 tricks from these other papers.1110.781110.78 And yeah, that's when it I guess, it was already happening, you needed to do all the modern1116.41116.4 tricks in order to really reach the top accuracies.1121.71121.7 But you know, in hindsight, we know it wasn't the tricks that helped them, it was actually1126.561126.56 their idea.1129.34000000000011129.3400000000001 I just I just thought it was rather funny.1132.36000000000011132.36 So you can see right here the results of this.1137.761137.76 If you look at the left, these are the plane networks.1140.21999999999981140.2199999999998 And we've already sort of seen this.1141.67999999999981141.6799999999998 Now this is on ImageNet right here.1144.43999999999981144.4399999999998 You can see the 18 layer network simply has lower train and validation accuracy.1150.341150.34 So the solid line here is the validation.1157.01157.0 On ImageNet, bold curves denote validation error of the center crops.1162.13999999999991162.14 So I guess they do, yeah, they do center crops.1164.36000000000011164.3600000000001 So the training error is going to be higher because they do these different augmentations.1170.661170.66 But you can see the training and the validation error are higher in the deeper network if1177.481177.48 you don't reuse residual connections.1179.28000000000021179.2800000000002 Again, this is not due to overfitting.1182.01182.0 And this is because we can't train these deep networks, because we should be able to the1188.56000000000021188.56 solution space of the 18 layer network is a subspace of the solution space of the 341194.121194.12 layer network.1195.39999999999991195.3999999999999 Everything tells us we should be able to learn the 34 layers to at least the accuracy of1200.81200.8 the 18 layers, but we can't.1203.081203.08 However, introduce residual connections, bum, bum, bum, bum.1207.41999999999981207.4199999999998 And you can see that the trend is exactly reversed.1210.441210.44 Now the 34 layer with residual connections has a much, much lower training and validation1217.041217.04 error than the 18 layer.1219.041219.04 In fact, look at this table right here.1222.681222.68 If you introduce the residual connections to the 18 layers, it's marginally better.1228.61228.6 However, if you introduce the residual connections to the 34 layers, it is a lot better.1234.61234.6 And this is another testament to the fact that these residual connections, they really1238.941238.94 help more and more the deeper you go, you can see the effect in.1243.981243.98 So this 18 layers, this is sort of a VGG 19 depth network.1250.36000000000011250.3600000000001 While it and there, we already know we can train these without residual connections,1254.881254.88 right, because we were able to train VGG 19.1258.281258.28 However, if we go higher to more layers, we can these residual connections all of a sudden,1265.721265.72 make it a lot a lot better.1268.761268.76 You can see that it's not it's not that we can't train the 34 layers, but the residual1274.521274.52 connections just help a lot more.1276.721276.72 And it most of a sudden, most of most importantly, they don't degrade the performance from the1284.361284.36 shallower network.1287.641287.64 So they, they explore the different options right here and compare it to others.1295.161295.16 Different options, as I said, being a, b, and c, where a is the zero padding for the1300.281300.28 projection, b is having projections simply between where the channels don't fit, and1305.761305.76 c being having projections in every single residual connection.1310.241310.24 And you can see right here that the option b gives you quite a bit of a boost, while1315.481315.48 option c doesn't give you that much of a boost introduces many more parameters.1320.641320.64 And you know, overall, is, I guess, they decided against it, which, since then the world has1328.12000000000011328.1200000000001 also decided against it.1330.36000000000011330.3600000000001 They also do deeper networks.1333.52000000000021333.5200000000002 So they built deeper networks, like 50 layer ResNet, 101 layer ResNet and 152 layer ResNet1344.28000000000021344.2800000000002 and the 152 layer ResNet ended up being the best one, as you can see here.1350.32000000000021350.32 And you can see a pretty gain like it almost almost lockstep gain depth.1356.67999999999981356.6799999999998 More depth means better network and this at the time, this these numbers, they were unheard1363.481363.48 of, like even 50 layer deep neural network was bombastic, but 152 layers, it was, it1372.81372.8 was crazy.1373.961373.96 And the fact that still it has less parameters than the VGG 19 and performs better.1381.561381.56 That was mind mind blowing, absolutely mind blowing.1385.381385.38 And then at the end, they built an ensemble of these models and ended up taking the 20151392.281392.28 ImageNet competition winner.1394.481394.48 That was still like very important back then it was still very important who wins who wins1399.581399.58 ImageNet that year, where I think I haven't even followed up on the last few years.1405.63999999999991405.6399999999999 It's some kind of wide fix ResNet whatnot with pre trained and 50 billion extra data.1412.841412.84 Yeah.1413.861413.86 So for the deeper networks, they decide that they are computationally rather become rather1421.121421.12 expensive.1422.121422.12 So they introduce these bottleneck blocks here on the right, where as you can see, so1427.63999999999991427.64 here, if you have a 64 dimensional input, you do 64 feature channels in your convolution,1436.761436.76 have a 64 dimensional output, you can save computation if you first project the higher.1443.60000000000011443.6000000000001 So here you have a 256 dimensional input, and they say we can save computational power1449.881449.88 by pretty much projecting down to 64 first, because then our complexity of this layer,1455.921455.92 which is the expensive layer, will be the same as the complexity of one of these layers.1462.08000000000021462.0800000000002 And then we can project up again, the one by one convolution, they are significantly1467.661467.66 lower computational intensive than the three by three convolutions, like it's nine times1472.741472.74 less operations, if you think about it.1477.241477.24 So that's what they use to build the deeper residual networks.1481.32000000000021481.32 And these residual networks, the ResNet 50, 101, 152, they are still staples today, you1487.63999999999991487.6399999999999 can have these are you can have pre trained versions of those and people still use it1492.621492.62 like ResNet 50 is used in every segmentation whatnot application.1499.39999999999991499.3999999999999 So yeah, this has turned out these decisions here have, you know, made it long way.1506.39999999999991506.4 Here you can see the number of parameters in these residual networks.1511.521511.52 And this was the absolute craziest thing right here.1516.80000000000021516.8000000000002 1202 layers, okay.1521.881521.88 So you can see still until here, ResNet 110.1525.761525.76 Now this is on CIFAR 10, right here, not on ImageNet anymore.1529.521529.52 But you can see that even 110 layers still had less parameters or actually the select1536.81536.8 the same order of parameters than these previous networks that were only 19 layers deep.1544.421544.42 This was unheard of and much more unheard of 1202 layer network to train on CIFAR 10.1554.521554.52 It's a bit of an overkill, but they say their goal was explicitly to study depth.1560.21560.2 And you can see here that with the deeper and deeper networks, they outperformed all1566.541566.54 of the previous networks, so all of the baselines and themselves as they went deeper and deeper1572.561572.56 and deeper.1574.241574.24 However, once you go to 1002 layers, you go up again.1579.841579.84 So here's the question, was this all just kind of a trick, a hack?1586.241586.24 And do we run into the same problem again?1588.41999999999981588.4199999999998 And that's the question they asked themselves.1591.01591.0 And the answer is no.1592.95999999999981592.9599999999998 So if you look right here, so here you see again, the plane networks.1600.67999999999981600.6799999999998 In the plane networks, you can pretty easily see that the more layers you have, the higher1607.821607.82 your error goes, whereas in the residual network, it's exactly the opposite way.1613.67999999999981613.6799999999998 The more layers you have, the lower your error.1617.061617.06 And if you compare this 110 layer network with the 1200 layer network, you see your1624.121624.12 validation error going up again.1626.361626.36 However, your training error and I can't zoom in more, but it's the same.1631.39999999999991631.3999999999999 It's the same and it's at zero.1633.561633.56 So here they conclude and the the here they conclude, now we are overfitting.1640.761640.76 They don't use like the biggest data augmentation like we use today.1644.321644.32 So overfitting was still a thing back then.1647.81647.8 So now they conclude, okay, now we have actually built a large enough network that is overfitting1653.21653.2 and then and the fact that we go up again in the training error is due to the fact that1659.761659.76 we are probably overfitting.1662.81662.8 So not only have they enabled us to build deeper networks, they have effectively shown1669.21669.2 that this can get you to the to the point where you don't need deeper networks anymore,1676.63999999999991676.6399999999999 at least on C410 because you are overfitting and it can effectively get you there.1682.041682.04 This is a lot of evidence for the fact that this biasing the networks towards the identity1686.961686.96 function is a very valid thing to do and is the solution to the we can't train deep networks1695.081695.08 problems.1696.081696.08 Lastly, they investigate the size of the responses.1699.961699.96 So their hypothesis is that if if it is really beneficial to bias the network towards the1705.961705.96 identity function, and if it is really true that each of these layers only learns a little1714.781714.78 bit, right, because the identity function is already very good.1718.61718.6 Each of these layers only needs to learn kind of a small function.1723.121723.12 They look at the responses of these things.1726.01726.0 So the response magnitude of these layers, right here of the signal through the layers,1732.521732.52 and they compare those with the response magnitude of the other neural networks where you don't1738.63999999999991738.6399999999999 have the skip connection.1740.281740.28 The hypothesis is, if we look at these, then the responses of these layers should be much1746.721746.72 larger because they have to learn much more.1752.921752.92 And the responses here will be much smaller because the identity function is already doing1757.361757.36 most of the work.1759.481759.48 And that's exactly what you find.1760.81760.8 So here, the layers are ordered by response and you can see the plane networks in the1765.61765.6 dashed lines are significantly above the residual network, even and that's not a function of1772.121772.12 the depth because if the depth was actually equal here, you would expect that the dashed1779.241779.24 lines would would stretch like this, right, they would kind of stretch out.1783.761783.76 However, exactly the opposite is happening.1786.241786.24 You can see that the residual networks even at the beginning, their responses are very1790.39999999999991790.3999999999999 much smaller.1791.861791.86 And this is kind of what I like about this paper.1794.281794.28 It's, it's one narrative.1796.961796.96 It is a hypothesis.1798.87999999999991798.8799999999999 And then every single, like the hypothesis is taken, and they make predictions from the1804.61804.6 hypothesis, they say, Okay, if we are right with our hypothesis, not only should our idea1810.13999999999991810.1399999999999 get us better accuracy, that's what most people most papers do today.1814.87999999999991814.8799999999999 But also, you know, but also, it should be that we can, for example, push our network1821.441821.44 to the brink of where we actually are overfitting, like here.1826.161826.16 And it should also be that the responses of our signal through our layers is smaller.1833.561833.56 And yeah, that's research like this is just pretty, pretty cool.1838.41838.4 And it's, I think, a lesson for us that sadly, the world has taken the ResNets, but the world1845.481845.48 hasn't all taken the research methodology of this paper.1850.36000000000011850.36 So yeah, if you again, if you want a good read, it's very well written, you I'm very1856.47999999999981856.4799999999998 sure you can follow it even if you have read very few papers.1861.01861.0 And with that, yeah, I hope you enjoyed this.1865.87999999999991865.8799999999999 Please tell me what you think of going through kind of old papers, looking at whether or1870.561870.56 not they have stood the test of time.1873.67999999999981873.6799999999998 And yeah, any other comments, leave them in the comments.1877.741877.74 I do read them.1879.041879.04 And I'll see you next time.1880.041880.04 Bye bye.1880.54\"}"}