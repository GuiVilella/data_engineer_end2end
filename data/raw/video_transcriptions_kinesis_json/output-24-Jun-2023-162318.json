{"data": "{\"value\":\"0.0 Hello there. Today we'll look at train short, test long,4.56000000000000054.5600000000000005 attention with linear biases enables input length extrapolation,8.948.94 also called Alibi by Ophir Press,12.20000000000000112.200000000000001 Noah A. Smith, and Mike Lewis.14.7614.76 So on a high level, this paper replaces18.1218.12 the position encodings or position embeddings of transformers by23.1623.16 a new very simple system that enables27.1627.16 these transformers to extrapolate to much longer sequences31.3231.32 at inference time than they have been trained on.34.0834.08 So you can train on quite short sequences and then inference will not suffer,39.7639.76 will not degrade even if the inference sequence length43.76000000000000543.760000000000005 is much longer than the training sequence length.47.0447.04 This goes from two times longer to 10 times longer to more.52.40000000000000652.4 So this builds on what people have learned57.3257.32 of position encodings in the last few years,59.9259.92 what works and what doesn't,61.3661.36 and it advances this one more step.64.8464.84 There's still room for improvement after this,67.467.4 but it's quite a simple thing to do.70.3670.36 The code is available.71.5671.56 I'll link to it in the description.74.474.4 It seems like it might be worth a try if you implement79.6879.68 transformer-based language models and you want to83.4400000000000183.44000000000001 infer on longer sequences than you've trained on, give this a try.88.4400000000000188.44000000000001 As always, if you enjoy paper reviews,91.1291.12 don't hesitate to subscribe and tell me in the comments what you think.97.4400000000000197.44000000000001 Let's get into it. So what's the problem?101.0101.0 The problem is position encodings.104.2104.2 As we've said, transformers were released in107.92000000000002107.92 2017 by the original Attention is All You Need paper,111.8111.8 and they already dealt with the question of position encodings.116.12116.12 Now, why is that?117.52117.52 That's because a transformer fundamentally isn't a sequence model per se,121.6121.6 it's actually a set model.123.36123.36 So let's say you have a sequence of tokens,126.44126.44 and in this paper,127.72127.72 we exclusively deal with autoregressive text generation.133.36133.36 But there's no actual reason why this is136.52136.52 the only case where this should be useful,138.84138.84 but that's what we're dealing with.141.16141.16 So you want to predict the next token from a series of tokens.145.56145.56 So here you have five tokens and you want to predict148.08148.08 the next one that comes after that,150.36150.36 and then the one after that,151.76000000000002151.76000000000002 and then the one after that, and so on.154.24154.24 So since a transformer essentially transforms a sequence of inputs into160.52160.52 an equally sized sequence of outputs in every layer,165.68165.68 the transformer other than a fully connected network,169.88169.88 the transformer itself doesn't really know per se where a particular item is.177.16177.16 So for example, for this node right here,179.56179.56 the transformer would generate the query and then match that up to keys that are emitted here,187.44187.44 and then it would route information via the inner product.191.16191.16 However, it doesn't matter if this node here, for example,196.07999999999998196.07999999999998 is here or over here,198.12198.12 if it has the same key,199.72199.72 the information routing happens the same way.202.72202.72 Ergo, to the transformer,204.84204.84 it doesn't matter where the inputs are.207.0207.0 So essentially, it's dealing with the input sequence as a set and not a sequence.211.76211.76 Now, recognizing that the original transformer already had to deal with position embeddings,217.56217.56 meaning, if let's say every sequence element comes in and initially,224.12224.12 like the initial sequence,226.44226.44 you give every token an embedding.228.72228.72 So these are your standard token embeddings that you know from Word2vec or GloVe or something like this.235.24235.24 So initially, you give every token a similar embedding.239.16239.16 Now, let's say these two tokens here are actually the same token.243.12243.12 So the cat and the ant.248.44248.44 Okay, maybe not. So two words can be the same in the same sentence,255.4255.4 even though they might mean a bit different things because they're at different places.260.28000000000003260.28000000000003 So what you want to do is you want to augment these embeddings right here by position embeddings.268.6268.6 The position embeddings can be as simple as simply appending,273.36273.36 let's say, okay, to any of these vectors,276.68276.68 I append one dimension,278.08000000000004278.08000000000004 I simply write the position in it.279.8279.8 So this is value zero,281.40000000000003281.40000000000003 this is value one, this is value two.283.56283.56 I simply append the dimension and I put the number there.286.8286.8 This won't work too well because we're in linear space and numbers between zero and one and so on.294.32000000000005294.32000000000005 So there are various schemes how to do this.297.44297.44 The first scheme that the original paper came up with is this scheme of the sinusoidal encodings,307.04307.04 which means that if we,310.04310.04 let's go down here.313.68313.68 This is our sequence.315.32315.32 How do we make the position encodings?317.88317.88 They said, why don't we have multiple dimensions of position encodings?325.6325.6 Our position encoding is a vector.327.72327.72 Now, let's say that the one dimension,333.64000000000004333.64000000000004 we simply index a really long sine wave.338.20000000000005338.20000000000005 So the sine wave would continue back here,340.28000000000003340.28000000000003 a really long sine wave by the position.342.84000000000003342.84000000000003 So this token would get,345.48345.48 so here is the zero.348.84000000000003348.84000000000003 This is a sine wave.350.72350.72 So the first one would be assigned a zero,352.72352.72 then this one would be assigned like a 0.5,355.08000000000004355.08 this one like a 0.7,357.0357.0 0.5, and so on.360.44360.44 You see like, but then these aren't unique.364.8364.8 For example, this and this, they have the same one on the first dimension.368.91999999999996368.91999999999996 Let's say, well, in the second dimension,371.12371.12 we'll do a sine wave,372.15999999999997372.15999999999997 but we'll make it double as fast like this.377.44377.44 Now again, we index all the tokens by where they are.381.52381.52 So this again would be zero.383.03999999999996383.04 This may be 0.7 here.385.64000000000004385.64000000000004 Now, this would be also 0.7 maybe,388.96000000000004388.96000000000004 and now this would be,390.76000000000005390.76000000000005 this is like 0.1.394.48394.48 So now you can see this vector here is already different from this vector here.399.72399.72 So as you build up your sine waves,402.36402.36 you can make them even faster,405.0405.0 and even faster.408.0408.0 As you build that up,409.48409.48 you eventually get unique representations for each position.413.08000000000004413.08000000000004 But also the advantages,415.44415.44 and that's what the original paper hypothesized is that,419.12419.12 now the transformer can reason about distances between tokens.425.0425.0 So it can say, well,427.04427.04 if two things are relatively close in this top-most dimension right here,435.08000000000004435.08000000000004 I can be reasonably sure they're close together.438.56438.56 But how close together?440.4440.4 Well, if they're also pretty close in the lower dimensions,443.72443.72 then they're probably right next to each other.446.08446.08 Or it can say, well,447.44447.44 I want something that's like medium size apart from this word that I'm on.454.32454.32 Not right next to it,455.6455.6 but kind of away.457.12457.12 So it would look for something that's different in one of these dimensions.461.4461.4 So the hypothesis was that with these things,465.04465.04 it could reason about absolute and relative positions from the tokens to each other.472.04472.04 It doesn't have to learn that relationship between word 1 and word 3,478.72478.72 and word 2 and word 4 separately.481.08000000000004481.08000000000004 It could actually just learn at one point the relationship between484.72484.72 any two words that are a bump apart in this dimension,489.08000000000004489.08000000000004 and then that would replicate across.491.56491.56 It could potentially also extrapolate.494.28000000000003494.28 However, this didn't turn out to work really well.499.91999999999996499.91999999999996 That is for two reasons.503.88503.88 At least this paper makes it seem like that's for two reasons.507.67999999999995507.67999999999995 The first reason is that the embeddings themselves don't really seem to extrapolate that well.515.48515.48 So the functions that are learned from these embeddings,518.8399999999999518.84 it's not like they transfer to longer sequences as much.524.76524.76 That's the first point.525.84525.84 The second point is these vectors that we build up here,529.8000000000001529.8000000000001 the position encodings,531.6531.6 what they were doing is they were simply adding them to the vectors that are the word embeddings.538.9200000000001538.9200000000001 That works fine, I guess,541.08541.08 especially if you also train the word embeddings at the same time,544.1600000000001544.1600000000001 the model can circumvent that.546.12546.12 But as you go up the layers,551.36551.36 you have to carry through this information.554.76554.76 So now all your computations within a layer have to,558.96558.96 first of all, deal with what are the meaning of the tokens and how they relate to each other.564.0564.0 But second, it would also have to carry through this positional information to the upper layers.570.08570.08 That's where more follow-up positional encodings made574.84574.84 a difference in that, for example,579.2579.2 they said something like,581.0581.0 well, we don't want to just add them to the bottom.585.36585.36 We also want to inject them into every layer separately.590.24590.24 We inject them here, we inject them up here, and so on.593.4593.4 So the model always has access to the position encodings597.36597.36 firsthand and doesn't need to carry through this information.601.48601.48 So this is one of the improvements that has happened.604.6604.6 The second improvement is to simply switch up607.6800000000001607.6800000000001 the sinusoidal encodings by themselves,612.32612.32 and that's a thing that we're going to see today.615.52615.52 The third is actually related to the first one a little bit,619.2619.2 is that if you say I'm going to inject the position information everywhere,627.48627.48 it also matters where and how you inject the position information.631.6800000000001631.68 So as you might know, if there is an incoming embedding here,637.9599999999999637.9599999999999 for every token, we're actually going to create a query,641.5999999999999641.5999999999999 a key, and a value.644.7199999999999644.7199999999999 The trick seems to be that if I only inject648.92648.92 the position information into the query and the key and not the value,655.7199999999999655.7199999999999 if I inject it into the query and the key,658.16658.16 I influence how information is routed here, that influences that.663.12663.12 But then the actual information that's transmitted to the next layer,667.12667.12 those are the values,668.6668.6 and I do not inject the position information into the values at all.674.52674.52 Therefore, the information that flows from layer to layer to layer has680.0680.0 no positional information in it at all, at least not directly,685.92685.92 because the values remain position information free.693.5999999999999693.5999999999999 We inject the position information at every layer into697.56697.56 the queries and the keys or the computation that we do with them.702.4399999999999702.4399999999999 These are the improvements that came together in the last few papers.709.52709.52 They compare different embeddings right here.713.7199999999999713.72 This sinusoidal is the original one.716.72716.72 Rotary embeddings as they're used in GPT-J,720.1600000000001720.1600000000001 T5 bias as it's used in T5,723.32723.32 and then their new one, Alibi.725.36725.36 Here you can see this model, for example,727.76727.76 is trained on 1,024 tokens in its training distribution.733.52733.52 However, when they make new inference on longer tokens,739.08739.08 you can see right here,740.1600000000001740.1600000000001 everything performs quite well.742.28742.28 This is perplexity, lower is better.745.4745.8 If you go longer,748.12748.12 the sinusoidal embeddings shoot up immediately,751.1999999999999751.1999999999999 so they fail immediately.753.12753.12 Also, the rotary embeddings,755.16755.16 they don't seem to cope super well,757.16757.16 a bit more, but not super well.759.9599999999999759.9599999999999 Even if you go double the sequence length, they fail.763.76763.76 The T5 bias is better,766.92766.92 but the T5 bias is a learned embedding,771.0771.0 takes more memory and needs longer to compute and to train,776.16776.16 which is a disadvantage there.778.28778.28 Also, it degrades relatively quickly.781.92781.92 Then the Alibi embeddings that they suggest,785.44785.44 they are not learned,787.0787.0 they are fixed embeddings like the sinusoidal and the rotary embeddings,791.4791.4 but they can deal with way longer sequences right here.796.76796.76 They keep up the speed of not having to learn embeddings,801.72801.72 they keep up the not wasting memory on things because they're not learned,806.88806.88 they don't increase the computation time,810.04810.04 and they manage still to bias the model in a way that it can extrapolate to much longer sequences.817.48817.48 How does it do this?820.48820.56 Here you can see memory stays relatively low,824.8824.8 doesn't increase, inference speed stays relatively high,829.28829.28 training speed stays relatively high.832.12832.12 How does it do this?833.8399999999999833.8399999999999 Here is the main model,836.4836.4 the main way that we do this.839.52843.3199999999999 As I said, we're dealing with auto-regressive language modeling,848.04848.04 which means that we're dealing with causal attention.851.3199999999999851.32 That's why only a triangular matrix appears right here.855.0855.0 There is in my mind not really a reason why this can't be extended to full self-attention.862.0862.0 In this case, you would just fill in the rest of the triangular matrix right here.868.84868.84 But consider again our model of transforming a sequence to another sequence,876.5200000000001876.5200000000001 and just view one single token like this token right here.881.2881.2 This token produces Q2, Query 2,885.0885.0 and it pays attention to all of the keys in the input sequence.890.0400000000001890.0400000000001 This is the attention mechanism.892.2800000000001892.2800000000001 The query is multiplied with all of the keys to decide where it should get its information from.900.5200000000001900.5200000000001 Now, if we simply do it like this,904.0904.0 and this is with the causal attention,906.2800000000001906.28 it can only actually pay attention to all the keys that come before it.911.52911.52 Query 2 would be multiplied only by Key 1 and Key 2,916.1999999999999916.1999999999999 and not Key 3 because it can't look into the future.920.28921.3199999999999 If it were just that,923.8399999999999923.8399999999999 then as you can see from this calculation,926.52926.52 there's no notable difference between these and these.929.68929.68 It depends only on what the key is to decide on the information,934.8934.8 not the position at all.937.04937.04 Now, what we do is pretty simple.940.16940.16 We simply add the distance between the two positions.949.4399999999999949.4399999999999 So for Query 2 and Key 2, this here,953.24953.24 the distance is zero because they are the same position in the sequence.958.0958.0 So this is token number 2 in layer L,965.72965.8 and this up here is token also number 2 in layer,970.72970.72 I'm terrible at doing L, L plus 1.974.96977.2 If it's the same token,979.6979.6 we don't do anything.981.16981.16 Other than that, we add the distance or we subtract the distance right here,987.84987.84 multiplied by a number m.990.8000000000001990.8000000000001 This is really a number,992.76992.76 so I was also surprised,994.1600000000001994.1600000000001 m is a number,995.76995.76 just a number like 0.7 or something like this.999.6999.6 So you can see the further into the past a given key is,1009.961009.96 so the further into the past,1012.481012.48 the more is subtracted from the attention value.1015.80000000000011015.8 Remember, these things here are attention values.1019.01019.0 These things decide if this is high,1021.761021.76 that means that Key 3 is really relevant for Query 3.1028.321028.32 If this is high, it means Key 2 is really relevant for Query number 5.1034.15999999999991034.24 What this here does is it simply says,1037.61037.6 well, however, the further in the past it is,1042.121042.12 the more we are simply going to subtract from that value.1046.041046.04 So whatever value you compute,1047.561047.56 however important it is,1049.121049.12 the further in the past,1050.63999999999991050.6399999999999 the more we're simply going to subtract from it.1053.321053.32 We'll do that in a linear fashion.1055.23999999999981055.2399999999998 So if your token is here and you look back,1060.841060.84 then it's degrades linearly.1066.121066.12 You just subtract more and more and more and more from that value.1070.81070.8 You can go negative as much as you want.1074.01074.0 Why? Why does this make sense?1077.39999999999991077.3999999999999 I was first a bit confused.1078.561078.56 I'm like, wait, you just subtract.1080.61080.6 It seems like you might want to multiply or something like this.1083.561083.56 But remember, once, for example,1086.121086.12 for Query 2 here,1087.321087.32 we built the multiplication.1089.61089.6 Sorry, this is a bit heavy.1091.761091.76 We built the multiplication of Query 2 and Key 2.1097.39999999999991097.3999999999999 This is an inner product.1099.561099.56 We also built the multiplication of Query 2 and Key 1.1104.441104.44 Now, what do we do with the two things?1106.481106.48 We do a softmax,1108.01108.0 which means that these are numbers,1113.041113.04 and they go into a softmax,1114.561114.56 which is going to give us a distribution.1117.01117.0 The softmax is something like e to the Query 2 Key i,1124.321124.32 divided by sum over j e Query 2 Key j.1131.481131.48 They go into an exponential function.1135.041135.04 Now, you can see why subtracting something makes sense,1138.61138.6 because essentially here, we're working,1140.721140.72 this is log space.1143.01143.0 Therefore, subtracting something in log space essentially means that you1147.61147.6 multiply it or you divide it by a constant.1153.081153.08 You divide it multiple times or by a higher constant,1157.321157.32 the more in the past it is.1159.67999999999981159.6799999999998 There we go. If this will be the histogram without the biases,1164.521164.52 with the biases, you simply say,1167.081167.08 well, whatever is more recent,1168.67999999999981168.6799999999998 so the more of the right ones,1170.761170.76 is going to be even more important.1173.481173.48 After the softmax, of course, it's normalized.1175.87999999999991175.8799999999999 This gains in importance and this would drop in importance,1179.63999999999991179.6399999999999 whatever it is.1180.61180.6 Even if it were this is higher initially than this,1185.95999999999981185.9599999999998 it will just decrease whatever is in the past and remain whatever is close by.1193.321193.32 Actually, it decreases everything,1195.15999999999991195.1599999999999 but it decreases whatever is in the past more.1198.61198.6 It's just a bias that says whatever is in the past is less important.1202.91999999999981202.9199999999998 Now, I told you this m is a number.1205.521205.52 How do they pick the number?1208.241208.24 They simply come up with a scheme.1211.921213.2 First of all, here's the formula.1216.61216.6 For routing to token i,1222.561222.56 you take the query,1224.361224.36 multiply it by all the keys and simply add m times this vector right here.1231.281231.44 Now, I'm not sure if the order needs to be correct.1238.21238.2 I guess if this is the vector right here,1241.241241.24 the keys have to be reverse order or something like this,1246.36000000000011246.3600000000001 because this adds to the most recent token,1250.561250.56 this to the second most recent token and so on.1253.81253.8 Here is how they choose m.1257.481257.48 m is different for each layer.1260.81260.8 No, m is different for each head.1263.921263.92 Sorry. m is different for each head.1267.761267.76 So they say, okay,1271.681271.68 if we have eight heads,1273.761273.76 the slopes that we use are the geometric sequence.1277.21277.2 The geometric sequence that starts at1279.81279.8 half and multiplies each element by half to compute the next element.1284.561284.56 For models that require 16 slope heads,1288.121288.12 it's a bit different.1290.441290.44 So as you know, transformers,1293.041293.04 they have multiple heads.1294.921294.92 So if this attention computation is essentially split,1300.12000000000011300.1200000000001 so you have incoming signal and1302.241302.24 the attention computation is essentially split over multiple heads,1306.21306.2 the attention computation is done somehow here,1309.641309.64 and then it's averaged or added together at the end.1315.01315.0 They're simply saying, well,1317.08000000000021317.0800000000002 this m number in these different heads should be different,1322.84000000000011322.84 because it might be more useful to have a harder slope,1327.15999999999991327.1599999999999 it might be more useful to have a flatter slope.1330.041330.04 So they come up with this scheme where they say the slope is one-half,1335.15999999999991335.1599999999999 then the slope here is one-quarter,1339.321339.32 the slope here, so it's slightly less slopey,1343.561343.56 here it's slightly less slopey, and so on.1346.281346.28 So they have these almost like different options.1350.01350.0 I quite like that,1354.481354.48 because I think whenever you have parallel things in your architecture,1359.881359.88 like multiple heads for attention,1362.581362.58 and it's my personal opinion that you should1366.041366.04 do something to make them different from each other.1369.481369.48 Otherwise, you just rely on noise and you build an ensemble, which is cool.1374.481374.48 Ensembles are cool. I think you can make them more effective if you say,1379.081379.08 all of these different options,1380.61380.6 they're slightly different in how they work,1383.37999999999991383.3799999999999 and the model can therefore choose a bit which one to utilize most.1389.361389.36 Now, you could still replicate those if you want more capacity or anything like this,1395.761395.76 but I'm generally a fan of doing something like that.1400.121400.12 So all the heads have slightly different slopes,1404.61404.6 as you can see, in how important or how unimportant they make the past.1411.19999999999981411.1999999999998 These slopes are predefined by them, and that's it.1415.71999999999981415.7199999999998 So yeah, that's that.1418.15999999999991418.1599999999999 The M is one number per head in the fashion that we've shown,1423.91999999999981423.9199999999998 and it's really simple.1426.241426.24 The drop-off is completely linear,1429.15999999999991429.1599999999999 and the simplicity might be the key right here,1432.91999999999981432.92 because now we test whether this extrapolates in the experimental results,1438.641438.64 and you can see that this extrapolates quite well.1443.12000000000011443.1200000000001 So I already shown you before, of course,1446.041446.04 the perplexity in what they've shown,1450.961450.96 but here is another test on the WikiText dataset.1456.41456.4 So again, we have perplexity on the y-axis,1459.881459.88 and the square dots you see, they're always the classic sinusoidal embeddings,1466.441466.44 and they are always trained on as long a sequence as you test,1471.36000000000011471.3600000000001 because we've already seen if you make the sequence longer, they just fail.1476.241476.24 So here, the comparison is really you train on a sequence,1480.32000000000021480.3200000000002 and that is exactly the length of the testing sequence.1484.28000000000021484.2800000000002 So they should be perfectly adapted to that length.1487.921487.92 Now, the top line is the new embeddings trained on 512.1494.80000000000021494.8000000000002 So the top line is trained on this size,1499.281499.28 yet if you test it, it already performs better.1504.12000000000011504.1200000000001 Now, what do you make of this?1509.961509.96 Like the claim is somehow, well, it's just a better position embedding by itself,1517.161517.16 because you can see here it's already better.1519.41519.4 I don't know, maybe this is also just experimental,1523.641523.64 like machine learning experiments in papers always making the baseline worse than themselves.1529.441529.44 But what we can say is that you can see it generally,1535.761535.76 the perplexity decreases or remains constant as you up the scale,1542.12000000000011542.1200000000001 even if you've trained it on a small length.1546.441546.44 And when you actually train it on larger lengths,1550.36000000000011550.3600000000001 so this line starts here, the one they trained here, obviously,1553.641553.64 I guess they could test it on shorter sequences, but what's the point?1558.281558.28 You become even better because you've trained on longer sequences,1563.08000000000021563.0800000000002 and again, you see the same pattern also with the one that you trained on very long input.1570.241570.24 So, in general, you see on long texts, the perplexity decreases as you train for longer, obviously, right?1582.881582.88 So, it still has an effect, you still want to train on as long sequences as you can,1587.921587.92 because that will gain you in performance.1590.161590.16 However, it's not too bad if you train on short sequences,1596.81596.8 and then extrapolate to longer ones with this embedding.1600.241600.24 In contrast to the sinusoidal embeddings that just completely fail1604.081604.08 when you give them anything longer than like 1.1 times the training length.1610.63999999999991610.6399999999999 And they have various comparisons about perplexity and how many words per second.1618.39999999999991618.4 Here is a cool plot that shows if you train on the same length as the sinusoidal embeddings,1627.36000000000011627.3600000000001 you get much lower perplexity and only a tiny bit of a slowdown, it seems,1633.12000000000011633.1200000000001 probably because you inject the position encodings into every layer.1640.41640.4 By the way, have you seen here, the position encodings,1643.441643.44 they only go to the query and key computation.1647.60000000000011647.6 They don't go into the values at all.1650.081650.08 We don't add them to the embeddings at the beginning.1652.81652.8 So, this is exactly one of the things we've talked about at the beginning.1656.481656.48 So, this is how they sort of incorporate one of the learnings of the last years.1661.041662.7199999999998 So, because you have to do this every layer, it's a tiny bit slower,1666.561666.56 but you gain a lot in perplexity.1668.95999999999981668.96 And if you go to train with smaller sequences, obviously you're going to be faster.1677.521677.52 And as you can see, your perplexity, it doesn't suffer too much.1681.041682.0 In fact, in their experiments, again, take it with a grain of salt,1685.84000000000011685.8400000000001 but in their experiments, it is even lower than the full length training with the sinusoidal embeddings.1693.60000000000011694.72 So, they go into, as I said, into various experiments right here.1698.481698.48 In generally, their message is always the same.1701.441701.44 There is a weird phenomenon where the perplexity actually gets better as you go beyond your training length.1711.441711.44 And they attribute this in part to the so-called early token curse phenomenon,1719.041719.04 where it depends sort of on how you split your evaluation data.1723.441723.44 And if they modify that, they see that, at least as I understand it, they can say that, okay,1730.32000000000021730.3200000000002 if for some evaluation protocols, we actually don't get better,1734.961734.96 so it's probably due to this early token curse,1738.241738.24 but nevertheless, the perplexity stays flat or you don't suffer that much if you train on short sequences.1747.761748.4 Hey, this is Janek from the future.1750.721750.72 Just a short addendum here to make it clear.1754.241754.24 And they also describe this in the paper.1756.721756.72 What is probably happening isn't that the transformer is all of a sudden able to reason about much longer contexts.1765.60000000000011765.6000000000001 But what is probably happening is that it still only looks at the most recent context1772.721772.72 because the more distant past has been down weighted so much by these biases that it becomes irrelevant.1780.01780.0 But nevertheless, it still enables the transformer to handle these long sequences.1786.161786.16 And potentially, if something's really important in the past, it can pick up on that.1791.041791.04 All right, back to the video.1792.321793.2 So all in all, I think this is a very, very simple, cool paper.1801.121801.12 I want to see in practice really if this works out, if this does something.1807.121807.12 Again, they've only tested on language modeling, autoregressive language modeling,1811.91999999999981813.28 where I'm not exactly sure why they haven't tested it on other things.1819.841819.84 Maybe they have and I've just not noticed it, though.1822.63999999999991823.36 It should work in other things, but only time will tell if this is really worth something,1830.561830.56 if this is really useful in practice,1833.67999999999981833.68 if there are so many cases where you can only train on shorter things, yet evaluate on longer things.1841.041842.16 That's why I would be also interested in non-autoregressive language modeling tasks.1848.161848.16 Because if you have to, say, answer a question about a document,1852.56000000000021853.2 it's much more about integrating whole information about the document1856.961856.96 or finding relevant things in the document.1859.041859.04 And there I'd be interested in the discrepancy between training and inference.1862.87999999999991864.08 All right, this was it.1865.441865.44 I hope you sort of understood what it is.1868.39999999999991868.3999999999999 Check out the code.1870.081870.08 Apparently, it's really pretty simple to include this in any sort of existing transformer.1876.15999999999991876.72 And yeah, tell me what you think.1878.961878.96 That was it.1879.681879.68 Bye bye.1889.68\"}"}