{"data": "{\"value\":\"0.8 Hello there, today we'll look at Infinityformer infinite memory transformer by Pedro Enrique7.847.84 Martins, Zita Marino and Andre F. T. Martins. On a high level, this paper proposes a transformer16.08000000000000216.080000000000002 that can attend to unbounded memory in the past. It does so by building up what he calls a long22.7222.72 term memory, which is a continuous signal rather than a discrete signal as most of the other30.2430.24 transformers do. It uses continuous attention to do so and that enables it essentially to36.5636.56 continuously compress the past into this continuous long term memory and then attend to42.442.4 it as it predicts next tokens. It also introduces the concept of sticky memories, which essentially50.23999999999999550.24 are events in the past that are of particular importance to the future. So by keeping those57.2857.28 sticky memories specifically around, they increase performance yet again. So we'll go through the64.064.0 paper, what the model looks like, how it works, and what it does in the experimental results.70.8871.6 Ha caught you, you wouldn't have guessed it, but this video is sponsored by Weights and Biases.76.7276.72 If you're in the ML space and you don't know about Weights and Biases, what are you doing?81.281.2 Please, if you track your experiments using a spreadsheet, a piece of paper,85.7685.76 TensorBoard, weird folder names, like I used to do, stop that. Use Weights and Biases. It's one91.5291.52 line of code and you can log any of your experiments to the cloud, not just metrics, but97.2897.28 models, data sets, output images, little videos, anything you want. Say hello to Zyrk.103.84103.84 Believe me, when I started the PhD, I was looking for something like Weights and Biases and I tried109.12109.12 every single thing there is. I tried every productivity tool, every note taking tool,114.08114.08 and I just couldn't get anything to work for one part because the features were just lacking,118.96000000000001118.96000000000001 for the other part because I was just too lazy. And Weights and Biases solves both of those123.92123.92 problems. It has all the things that I need to track my experiments, collaborate with others,128.72128.72 and so on. But also it's just a single line of code and everything else works automatically.133.2133.2 It even boosts my productivity because whenever I have logged a model, I can just call a function139.92139.92 to download that model from the Weights and Biases website. I don't need to place it in a correct145.28145.28 folder or keep track of it myself. It's just there. On top of that, it relieves me from the150.79999999999998150.79999999999998 stress of writing stupid Overleaf reports because I can write a Weights and Biases report and share156.79999999999998156.79999999999998 that with the people that I want to show my work to. The Weights and Biases report is so much more162.07999999999998162.08 useful than a PDF. It's essentially a website, but you don't need to code any HTML or CSS or whatnot.170.48000000000002170.48000000000002 You can include dynamic content, you can reference the runs you did, you can pull out data from the175.84175.84 runs, you can present that in a neat fashion. And it gets even more easy, you don't even need to...180.4184.72000000000003 And it gets even more simple, you don't need to even set up anything. In fact, Weights and189.36189.36 Biases runs in the cloud by default, you can host it on premise, but it really wants to live in the195.28195.28 cloud. All you have is an API key, you log in, and you're good to go. So please check it out.202.64000000000001203.20000000000002 Accounts are completely free for personal use. I promise you will not be disappointed. Give it208.4208.4 a try. And now let's get into the video. Bye bye. Cool. So there are a couple of good things and a223.04000000000002223.04000000000002 couple of questionable things about this paper. Also, there are a lot of engineering choices in228.48000000000002228.48000000000002 this paper, which I don't necessarily want to go into. There are a lot of things that one could do234.0234.0 differently, I feel, which in influences the experimental results as well, I guess, but we'll240.88240.88 just take it for what it is. The other thing is that I believe this should be called not infinity248.4248.4 former, but inf t former, that's actually how you find it on. If you Google for this, you have you255.36255.36 can enter inf t former, inf t being of course, the abbreviation in LaTeX for this symbol right here.264.16264.16 And I think, you know, to make it more unique, we should just call this the inf t former.268.72269.68 Alright, so what does the inf t former propose, they say in the abstract right here that276.72276.72 transformers struggle when attending to long context, since the amount of computation grows282.64282.64 with the context length, and therefore cannot model long term memories effectively. So there289.03999999999996289.03999999999996 are a number of things written hidden right here, they say the amount of computation grows with the294.24294.24 context length. Now for classic transformers, it's actually worse, right, the amount of computation299.52299.52 grows quadratically with the context length. But even for some of these, let's say linear305.68305.68 transformers, the amount of computation still grows linearly with the context length. So they313.6313.6 they see even this as a problem, they say they cannot model long term memories effectively.319.92321.6 Now, they say several variations have been proposed to alleviate this problem, but they326.48326.48 all have a finite memory capacity, being forced to drop old information. In this paper, we propose333.28000000000003333.28 the inf t former, which extends the vanilla transformer with an unbounded long term memory.339.67999999999995341.03999999999996 By making use of a continuous space attention mechanism to attend over the long term memory,346.4346.4 the inf t formers attention complexity becomes independent of the context length. Now already352.71999999999997352.71999999999997 remember right here, there is rarely a free lunch, I don't want to say there is no free358.0358.0 lunch, because I've definitely eaten free lunches before. But there is rarely a free364.32364.32 lunch in these kinds of things. If we have a finite computation, we cannot pack infinite371.2371.2 information in there. So if we are attending to unbounded long term memory, that means something378.64378.64 else will have to give. And of course, the thing that gives here is just the amount of information385.12385.12 you can retain. Now this can be a good thing to trade off sort of boundedness in time for boundedness392.0392.0 in information. Yet still, you have to keep that in mind. As I said, they also introduced this thing398.96398.96 called sticky memories that keep important things around. Now, as we go through this, this gets it409.2409.2 in my mind, at least this gets more and more into just like a classic LSTM model. So the classic415.68415.68 LSTM model, of course, takes in some sort of a, a input, then models a hidden state, then propagates423.28423.28 that hidden state when it inputs the next thing, and so on. And it sort of has to keep track of430.64430.64 what's important in its own hidden state, as to decide what it wants to remember what it doesn't436.32436.32 want to remember. So as with the transformer, the LSTM has in fact an unbounded memory, right, it443.2443.2 can remember things for arbitrarily long, yet it only has finite capacity to do so it needs to450.15999999999997450.15999999999997 overwrite some memory every now and then. So this is a bit how you can think of this model is456.24456.24 essentially the same principle as an LSTM trading off unboundedness for finite representation space.465.36465.36 I'm not saying this is an LSTM, it is a little bit different, it might be a smarter way to do470.48470.48 unbounded computation, it might not be, but in concept, it is the same the similar thing. Okay,479.68480.40000000000003 so what's up with this continuous attention that they keep talking about?484.96000000000004486.96000000000004 This is, in essence, quite a simple concept. Namely, if you have a sequence of let's say tokens,495.2495.2 right, and every token has an embedding vector, so every token is associated with a vector that502.32502.32 is its embedding. And this can be the first layer, but this can be also the intermediate,508.0508.88 the intermediate values of the computation. So from one layer to the next, you always in the514.48514.48 transformer have number of tokens of these embedding vectors that travel through the model,521.36521.36 they get transformed into by the next layer into new embedding vectors, and so on, and so on.526.88527.6800000000001 Now, the NFT former, what it does is it takes this signal right here, and, and changes that536.24536.24 from a discrete signal into a continuous signal. So you would no longer have dimensions that,542.8000000000001542.8000000000001 you know, the first, the topmost dimension here, the first dimension of all these vectors might be547.6800000000001547.68 whatever 459.13. That's no longer the case, what you would have is like a continuous signal.557.8399999999999558.88 Okay, now, how do you do that pretty easily? What the NFT former does is it takes each of564.3199999999999564.3199999999999 these dimensions separately, okay, each of these dimensions, it plots these points up570.4799999999999570.48 on a sort of continuous plane. So this, this here, so this, it labels it from zero to one. So you580.16580.16 divide this interval into, I guess, five different points, because we have five tokens. For the first586.64586.64 one you label, sorry about that, you label with a four, where is a four? I suck at this. So here is595.2595.2 a four, so dot here, then here is a five, I guess, so dot here, 9.1 and three, like here, okay, so607.6607.6 here's three. Cool. And then what it does is it calculates an interpolation. So the interpolation615.76615.76 would be this, approximately, right? So it calculates an interpolation of these points,623.44623.44 and then it simply stores that interpolation, it forgets about the embedding vectors themselves,629.6800000000001629.6800000000001 and it simply stores that signal. And that is it's so called long term memory, simply this signal.637.5200000000001638.08 Now, you might wonder, why don't we just store the embedding vectors, right? Instead of the signal?645.2800000000001645.2800000000001 And that is, of course, a good question. The goal is, of course, that you can store the signal more651.2651.2 efficiently than the embedding vectors. So if we can describe the signal here with less than five658.4000000000001658.4000000000001 numbers, then we might be able to then we might be able to save some space, right? Like what,667.12667.12 like this is reasonable, this could be a polynomial of degree three. If, for example,674.08674.08 like, if I draw this, you know, this is reasonably a polynomial of degree three, ergo, we'd have to680.96680.96 store like three numbers, maybe plus a bias of four. But if we agree that we always store688.88688.88 polynomials of degree three, then no matter how many embedding vectors we have, we're always going694.88694.88 to store the signal as three numbers or four numbers, right as a constant amount of numbers.701.6800000000001701.68 And that is essentially the trick right here on how we get away from the sequence length,706.88706.88 we simply commit to a representation, a fixed representation of a signal, and then we interpolate717.04717.04 the embedding vectors using this fixed representation. Now the fixed representation722.8722.8 here isn't a degree polynomial, but it is in fact, a series of radial basis functions.730.7199999999999730.72 So we associate each point in time, which is the the here the one the two, like the the interval738.4738.4 from zero to one, we index this into a radial basis function. And radial basis functions are745.52745.52 nothing more than so this is one, this is one, this is one, okay, so these are these are three754.48754.48 essentially, these are three radial basis function spaced out right here. And how could we represent760.48760.48 the signal from up here? Using that maybe we can say okay, that's plus, you know, if here is one,767.52767.52 like that's plus 4.5 of that of of, let's call that psi one, then minus, you know, it goes down777.76777.76 like minus three of psi two, and then it goes up again, like plus four of psi three, maybe some789.92789.92 sort of a bias plus two, okay, so four numbers, three radial basis functions. Alright, so these797.6797.6 things here are completely independent of the data, they're not learned, they're simply fixed802.64802.64 once, like this is going to be the our basis for representing all of the signals. And then the way811.12811.12 we transform the discrete signal into the continuous one is we run a regression. So the817.04817.04 regression you can run by solving this system right here, by figuring out what is the matrix B823.36823.36 here. And that's a linear system. What is the matrix B? How do I have to mix the radial basis830.88830.88 functions here in order to match my signal as closely as possible? The way they do it is they839.36839.36 run a ridge regression, ridge regression is simply a regression with an L2 penalty, I think,848.88849.76 is that the case? Yes, I think so. So you run y is equal to x times w. So you're trying to find w858.72858.72 x times w, you're trying to find that so your loss is going to be the distance of these things866.72867.36 squared, and then you have some sort of a regularization constant. And on the L2 norm of875.6875.6 the weights. So you solve this, there's a closed form solution. This is the closed form solution881.12881.12 for ridge regression with f being the matrix containing these basis vectors, this one right here.886.4886.4 And there you get your B matrix. So you transform x, which is dependent on the length of your sequence,894.4894.4 right into B, which is only of the length of how many basis vectors you decide to have, in this902.4902.4 case, three, or three plus one, if we want to buy us again. All right, so and that's how you have a910.0910.0 continuous signal, you might already hear, you might already say, wait, isn't this just a special916.16916.16 case of a system that simply compresses a sequence into a variable length sequence into a fixed length924.72924.72 sequence? Like, isn't this just a way to embed like a continuous, like an unbounded sequence?931.28932.08 And I'd say yes, absolutely. That's the first thing. The second thing is, is certainly937.36937.36 the whole procedure is certainly not independent of length, as this system right here is absolutely944.32944.32 dependent on the length of your signal. And you can also see that the longer your sequence gets,950.32950.32 the more mistakes you'll actually make in representing it because you only represented955.36955.36 using the same basis vector. So here is where the trade offs happen by going from length l to length,964.32964.32 I believe they call it n, the length here of the number of basis vectors is n. So that's the first971.44971.44 thing, here's where the trade off happens. The second thing, which really kind of interests me,978.08978.08 and here you see this again, right? So by the way, this, then they consider their their memory,984.72984.72 right? So you can technically do this with all of the past, right? You take all of the past,990.08990.08 you remember the vectors right here, and then you interpolate. Or what you can do is you can,997.2800000000001998.1600000000001 what they call, you know, if you really go to unbounded memory, you take the past, you take1004.961004.96 the current sequence, you can do what you can do is you can contract the past, which means you can1010.80000000000011010.8000000000001 interpolate the interpolation. So you can sample it in a more coarse grained fashion at the1017.61017.6 handy, you can sample it in a more coarse grained fashion than you originally produced it, which1023.441023.44 leads to samples like here, and then you concatenate with the new signal. And then you1029.12000000000011029.1200000000001 simply interpolate again into the whole signal. So you can see the more distant past is now1036.721036.72 compressed to that. And the more recent past is appended to that. And of course, in the next step,1042.961042.96 you'll contract this whole thing to a shorter sequence and append the more recent thing right1049.60000000000011049.6000000000001 here and interpolate again, how this is conceptually no different from an LSTM. It brings1056.641056.64 about the same problems as an LSTM, namely more recent things are more likely to be in memory than1062.721063.28 way past things and so on. So calling this, you know, being able to attend to the past,1070.81070.8 this, you know, being able to attend to unbounded unbounded memory and so on is a like, it's a bit1080.721080.72 shady. Like that just that's just my opinion, you have to be aware of the trade offs. Second of all1087.21087.2 second is the fact that in order for this to work, right, and we haven't even gotten to the1095.441095.44 attention part yet, we're just representing our signal as as a continuous signal. In order for1102.241102.24 this to work, you're counting on the fact that there is some kind of a regularity, right here,1108.32000000000021108.3200000000002 I've drawn these points specifically such that I could draw a neat line through them. Yet there is1114.81114.8 absolutely no reason why the embeddings of the continuous, you know, next to each other tokens1123.761123.76 should be in any way continuous such that you can interpolate it, right, you count on the fact that1130.161130.16 you can compress the signal, because the signal like the samples go like, right, then you're like,1136.321136.32 Whoa, I can I can represent this by one line, right, one radial basis function goes through1142.161142.16 all of them. Cool. But there is no reason why this should be like the signal could be like,1147.681147.68 like, completely, completely random in terms of what the real floating point numbers are in the1158.241158.24 individual dimensions. Yeah, they mitigate this a little bit by smoothing the signal first before1165.84000000000011165.8400000000001 they before they interpolate it. But in my mind, that kind of only makes it less accurate, it1173.21173.2 doesn't make the problem go away, it just makes it sort of less accurate. Because if there is an1179.12000000000011179.1200000000001 actual value to having a pattern like this, if that's actually an important, an important pattern,1187.60000000000011187.6000000000001 then neither interpolating it very coarsely with only few basis functions, nor first smoothing it1195.36000000000011195.36 will, will necessarily help. So, you know, I just from a principled standpoint, I am skeptical that1206.081206.8799999999999 this is the case that signals that these signals here are necessarily such that they are easily1213.43999999999981213.4399999999998 interpolatable. But of course, I might be wrong. So, you know, that's it, I might be wrong, right?1222.47999999999981222.48 Okay. So what do we do with it? Alright, let's say we have the past in this long term memory, right?1232.081232.08 This is all of the past, we've interpolated it into this fixed long term memory, this continuous1239.12000000000011239.1200000000001 signal that we represent as a superposition of a fixed set of basis functions, we have our short1246.481246.48 term memory here, which is simply whatever we would put anyway, into the context of the transformer,1253.041253.04 right? And then we have our sequence that we actually want to deal with. So the attention1260.881260.88 within the discrete part of the transformer is as you know it, this is self attention,1268.881268.88 a training, I guess, masked self attention for certain tasks. This is as you know it,1275.041275.04 the question is how do we make use of this long term memory right here. And here is how we do it.1283.041283.04 So for each location in where we want some sort of a prediction, right, we produce a query, as you1290.81290.8 know, if in a transformer layer, every single token produces to go from one layer to the next1298.081298.08 produces a query vector, the query vectors tell what this token wants to know about the sequence1306.241306.24 in the last layer. Now every token also emits a key and a value vector, so key and value, key and1316.321316.32 value and so on. I'm only drawing the keys and then this is routed by inner product. Now the1322.481322.48 query, of course, we can keep the query simply tells what does this token wants to know. So the1329.12000000000011329.1200000000001 query is also taken to go to the long term memory, right? So the query vector of each discrete token1337.521337.52 now goes to the long term memory down here. And we'd have to find a way to ask the long term1344.881344.88 memory something according to this query. So how do we do it? What we need is we need some sort of1351.681351.68 a notion of a key and a value for this long term memory. And here's how we compute it. Remember,1359.681359.68 we have, it's not the continuous signal is described by this matrix B right here. So if the1367.28000000000021367.2800000000002 continuous signal is described by the matrix B, then of course, we can compute key and value1373.52000000000021373.52 and compute keys and values from B. These W matrices right here are learned parameters1381.21381.76 that take B and make it into keys and values. Now, the keys and the values are of different1389.681389.68 length, they are sequences, they're discrete sequences, right? They're of different length1394.721394.72 than the length of the sequence we're dealing with. But that doesn't matter. Nothing in a1400.081400.08 transformer actually specifies that the next layer always have to has to have the same length of1405.361405.36 sequence. So what you can imagine, the way you can imagine this is from the long term memory,1411.521411.52 essentially, what we're doing is we're building another sequence, it's not as long as the sequence1422.01422.0 that generated the long term memory. But essentially, we're building another sequence of1427.841427.84 tokens, they are, you know, not necessarily corresponding to individual tokens in the input,1435.281435.28 they're corresponding to how the thing is constructed. But nevertheless, and from those,1441.61441.6 we can certainly generate keys and values as we do regularly. Okay. So we essentially compress1450.561450.56 the past into this pseudo sequence of fixed length via a continuous representation. And then we just1460.01460.0 use attention again, to map the keys here with the queries. Now, when it comes to actually computing1471.67999999999981471.6799999999998 the thing, it's not it's not as easy. So this is in concept. But when it comes to actually computing1479.21479.2 the thing, what we want to do is we don't want to really abstract this into series, we would like to1484.321484.32 use continuous attention. So continuous attention essentially means that our attention doesn't go1492.41492.4 directly to one particular token. So it's not like, we know this token and this token and this token,1499.761499.76 but since we have a continuous signal, our attention should be something more like, well,1504.41504.4 I want to attend to this part of the sequence. And we model that as a probability density over1512.641512.64 the sequence, specifically, we restrict ourselves to a Gaussian. So what I can say is I can, my query,1521.12000000000011522.48 the interactions between the queries and the keys will give me a Gaussian, where I say I would like1528.721528.72 to attend to this particular part of the sequence, right? This is where in the past I want to attend.1536.081536.08 And this is how broadly, let's say I want to attend, you know, how many, how much of the1542.721542.72 surrounding I want to consider. So this, this ultimately defines a Gaussian, like where it is,1548.161548.16 and how, how far the Gaussian is spread. Right? So I can attend to per query, per token per head,1559.21559.2 I can attend to one location in the past, and its surrounding and the width I can also specify.1566.881567.68 And this is also learned. So as I understand it, these affine transformations right here are also1574.161574.16 learned transformations. Maybe I'm wrong in that it just says affine. But yeah, and then the sigmoid1584.161584.16 and the soft plus are just regular functions. But you can see right here, this is essentially,1589.21590.16 as you're used to multiplying keys and queries. But then instead of attending to the tokens1596.481596.48 themselves, because we don't have tokens, right, we, we specify a Gaussian to attend1602.32000000000021602.32 over the continuous signal. And ultimately, we can integrate, essentially, we can integrate the two1612.15999999999991612.1599999999999 things. So we can integrate the values that we obtain from the from the sequence, this these1620.87999999999991620.8799999999999 values, we integrate them according to the probability distribution that we get. And that's1627.521627.52 going to be our output values. So these here are going to be our output values.1633.841636.56 Now, once we have the output values from the long term memory, we add them to the output values that1642.561642.56 we get from the short term memory and the sequence itself, add them together, I think they go through1648.481648.48 another affine transformation after that. And there is your output. And the output is going to1655.281655.28 be one output per token in the sequence that you're interested in. Okay, so I know this was1664.241664.24 fairly lengthy. But to recap, we take the past, we do, we do a regression, a ridge regression1673.761673.76 in order to determine the coefficients to represent the past as a continuous signal with respect to a1680.961680.96 fixed set of radial basis functions. This gives us a fixed size representation, independent of1688.081688.08 how long the past is, then the way we use the past is we take the queries that come from the attention1697.12000000000011697.1200000000001 mechanism, we transform the representation of the past, which is this B matrix right here,1707.041707.04 into keys and values, we take the inner product between the queries and the keys. And this1714.961714.96 determines a Gaussian window for us where in the past we want to attend to, we integrate the values1724.321724.32 from that region according to the Gaussian. And that's going to be our output signal from the1731.441731.44 long term memory. This gets added to the output signal of the regular attention mechanism, and1737.60000000000011737.6000000000001 that gives us the output signal as a whole. Okay, this is essentially, essentially it. And1747.041747.92 if we do this one after another, right, we could simply always go to the past and compress it. But1756.08000000000021756.08 we can also do this trick that I mentioned before, this unbounded memory trick, where you always take1762.721762.72 the signal from the past, you compress it essentially by sub sampling it, you concatenate1769.121769.12 the new signal, and then you interpolate again. And on top of this, they introduce these sticky1776.481776.48 memories. And the sticky memories simply say, look here, the points that I have sampled, the points1784.63999999999991784.64 the points that I have sampled this past signal on here, I simply, well don't believe my drawing,1790.881790.88 but I simply did that uniformly, I sampled this uniformly, that kind of gives me a good sampling1798.241799.2800000000002 of the of the signal, right, I can also sample this differently, I can over sample certain regions1806.481806.48 and under sample certain regions. So here they say, why don't we over sample, according, why1813.681813.68 don't we sample according to these Gaussians that we've determined during the attention mechanism.1820.41820.4 So the Gaussians, of course, are summed up over all the attention heads, and over all the sequences1828.961828.96 in, so we're sorry, all over all the tokens in the current sequence that you're looking at, because1835.84000000000011835.8400000000001 all of these things attend to the same past. If we sum up all these Gaussians over these things,1842.01842.0 then we should get an idea of where most of the attention went and where no attention went. And1848.961848.96 the idea of sticky memories is simply, let's over sample the regions where a lot of attention went.1856.881856.88 So maybe a lot of attention went to this bump right here. So we over sample that, and maybe1861.921861.92 not much attention went to this region right here. So we don't sample anything like this.1866.881866.88 Then once we have sampled, we spread these things out, I guess, equally, we could, and then we1874.01874.0 interpolate again. And that's how we keep the more important things in memory more accurately.1882.961884.0 Now again, this is all heuristics. And this is a bit what my criticism here is as well. All of1891.441891.44 these things, you know, in an LSTM, it's at least learned like how to compress the past, and how to1899.921899.92 to read it, how to use the past, which memories to keep and so on. All of all of this is learned,1907.281907.28 right, the LSTM, all the gates are learned, and so on the the weighting functions. Now that's also1912.881912.88 the culprit in an LSTM, because you have to back propagate through time. And that's just not1918.01918.0 possible for very long sequences. So that's a bit of the LSTM downfall as well. Whereas here,1923.921924.64 we don't have to back prop through time, because everything is a heuristic. However, everything1930.241930.24 being a heuristic, it's, you know, like, how do we know? Okay, maybe it works. But, you know,1937.841937.84 I'd rather, I'd rather not use just heuristics for doing that kind of stuff. Yeah,1946.321946.32 but I guess there's room for improvement. So here they detail that yeah, they smooth the,1951.761951.76 they smooth the signal with a CNN before they do the multivariate ridge regression and so on. There1958.87999999999991958.8799999999999 is a regularization where they regularize the variance of the Gaussian that they predict.1967.67999999999981967.68 Yeah, these are details. So the ultimate loss has the training loss plus the KL divergence. Maybe1974.81974.8 they did that after they just saw the model simply wants to attend to everything all the time.1982.241983.28 I don't know. But then they evaluate the model on various tasks, such as this sorting task. And I1989.921989.92 have to say, they construct the tasks fairly cleverly. And they also evaluate the training1997.60000000000011997.6 cleverly by making sure the model can't like use simple strategies to solve it. And what they see2005.362005.36 is that things like the transformer XL, which tries to have some sort of a long term memory,2011.842011.84 but not doesn't do it really, like doesn't. I've made a paper on transformer XL, sorry, a video. So2020.39999999999992020.3999999999999 if you're interested in that, you can read it. And also this, this compressive transformer seems to2025.842025.84 be a little bit what the inf deformer is, but without going via this continuous signal, though2032.242032.24 the compressive transformer seems to be a transformer that always tries to sort of compress2036.242036.24 the past into fixed size memory, if I understand it correctly. And generally, they find that their2045.842045.84 model is relatively on par with the compressive transformer outperforming it a little bit.2053.75999999999982053.76 Now this being machine learning and so on, I would not, I would not be confident that there is2060.962060.96 a difference between the two model or which one is actually better, just from these results. In2067.362067.36 their results, they are better. And when they add the sticky memories, they are even better,2073.52000000000042073.84 which I guess makes sense. But again, take that with a grain of salt, they do analyses on what2081.762081.76 which parts of the long-term memory this continuous attention goes to. And in general, this seems2088.882088.88 pretty reasonable. If you look at kind of, you know, these, where in these long texts, where the2098.56000000000042098.5600000000004 attention goes to, like apparently here, the ground truth is U2 as I guess the answer of a2107.282107.28 question or, oh no here, I guess this is masked out, maybe. And the attention, I'm not exactly2116.24000000000022116.2400000000002 sure where it's trying to predict U2, maybe it's masked language modeling or some sort of question2121.68000000000032121.6800000000003 answering. However, it seems to be reasonable. There is a helicopter. It seems to be reasonable,2130.08000000000042130.08 at least in this one example they show. So they do, sorry, not mask language modeling, actual2137.62137.6 language modeling against something like GPT-2. And they outperform that. And they do some more2148.642148.64 analysis. So again, I don't want to go too deep into the experimental results right here, because2156.02156.0 again, with lots of engineering choices, it seems to be, it seems to be, you know, like it's tricky2166.642166.64 to make sense of small differences between models. What I would go for is the general trends and the2173.122173.12 general trends are okay. You know, I don't know if the code's out, I haven't seen any code. If it is2181.042181.04 out, give it a try, I guess. Otherwise, you know, wait for about 30 minutes until LucidRainz has an2187.922187.92 implementation available. And with that, I'll see you next time. Bye bye.2211.52\"}"}