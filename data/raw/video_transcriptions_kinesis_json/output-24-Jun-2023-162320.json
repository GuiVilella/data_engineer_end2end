{"data": "{\"value\":\"0.0 Hi there, today we're looking at regularizing trajectory optimization with denoising auto5.885.88 encoders by Rene Bonet, Norman DiPaolo and others of various places.12.4812.48 But a lot of the people are from curious AI.16.2816.28 And we actually had a discussion with Hari, who is the CEO of curious AI.23.08000000000000223.080000000000002 And this was on our machine learning street talk podcast.26.626.6 So this is another YouTube channel for those of you who don't know, where every week or31.4431.44 so we try to have either an interesting discussion or a guest like sort of an interview or talk37.76000000000000537.760000000000005 about comment on the talk.40.3640.36 So if it is not out yet, I'll link to it as soon as it comes out.44.40000000000000644.400000000000006 But if you're watching this video later, make sure to check out our conversation with Hari49.3249.32 because it was absolutely fantastic.52.7452.74 And in general, if you like videos like this, consider subscribing, liking, sharing if you're57.9257.92 still here at the end, and liked it.61.0861.08 Okay, so this paper on a high level deals with model based reinforcement learning.67.267.2 Model based reinforcement learning means that you are using a model of the world to do reinforcement73.3200000000000173.32000000000001 learning.74.3200000000000174.32000000000001 So in essence, if you have your reinforcement learning setup where you are an agent, and82.1282.12 you have to interact with the world, you have to do so in many steps in like a round trip87.1600000000000187.16000000000001 fashion.88.1600000000000188.16000000000001 So you put an action you act, and the world gives you back an observation.93.1893.18 And you have to act in the world over and over and over such that you will be able to99.1699.16 maximize your reward.101.4101.4 Now what is model based reinforcement learning?104.44104.44 Model based reinforcement learning basically means that the agent here has internally111.76111.76 a model of the world.113.60000000000001113.60000000000001 So it sort of understands how the world works.118.4118.4 Situations where you have a accurate model of the world are things like chess.122.48122.48 So in chess, the rules are very clear, you know how the world's going to behave if you127.44127.44 perform a certain action.129.16129.16 But in real world applications, it's very, very hard to actually make a model.134.44134.44 So people usually rely on learned models.137.4137.4 So what does it mean?138.4138.4 It means you basically learn a neural network that tries to predict how the world is going143.0143.0 to act.144.44144.44 So this here is going to be a deep neural network that you learn from what you see in150.36150.36 the world.151.44151.44 Now trajectory optimization basically means that you are now you now have this world model,159.12159.12 and you use it to look ahead, as I said, so you are in the state like here, and you can164.24164.24 do let's say three different actions, and you use your world model here, world.170.56170.56 And you see, you think how's the world going to react if I do either of those three things,175.52175.52 and then you get into three different states.177.72177.72 And then again, you after each one, you consider three actions, three actions here, three actions183.12183.12 here, and so on.184.42000000000002184.42000000000002 So ultimately, you're going to kind of have an overview over a planning horizon, which190.20000000000002190.20000000000002 here we call H.192.18192.18 You kind of look ahead a couple of steps, or there are various ways of doing this.196.98000000000002196.98000000000002 But ultimately, you will basically find that this this path here is really good.202.42000000000002202.42000000000002 So I think I'm going to take this as a first action.208.4208.4 So trajectory optimization considers finding the best green path here in this tree of possibilities215.56215.56 that your world model gives you.218.60000000000002218.60000000000002 Okay.220.24220.24 Now what does what do these people say?223.08223.08 They say this procedure often suffers from exploiting inaccuracies of the learned model.230.86230.86 What does that mean?231.98000000000002231.98000000000002 That basically means that if I have a world model, and it is not accurate, then it is237.12237.12 basically, basically the the thing that tries to find the best green path here, the optimizer243.32000000000002243.32000000000002 is sort of trying to find the the best path against this world model.250.16000000000003250.16 Now if that world model is inaccurate, that can lead to devastating consequences.255.12255.12 So what do we mean by this?256.6256.6 I'll give you an example.259.04259.04 If you have a room, right, and the room is, let's take our classic room, like this.267.56267.56 And you are here and you would like to go here.272.32272.32 And so you're a reinforcement learning agent, you do some exploration, right, you explore277.0277.0 a bit here, the next episode, you might go here, and you might go here, and so on.281.64281.64 And over time, in this framework, you're going to build a model of the world.285.72285.72 So at the beginning, we won't tell you how these rooms look, you have to discover it290.4290.4 by yourself.291.62291.62 So maybe at the beginning, we only tell you, there's these four walls, the rest you have296.16296.16 to figure out.297.4297.4 So on and on, you're going to fill in your blanks, you do your first explorations, and302.24302.24 you've had there's a bit of a wall here, right, and there might be some wall here, I crashed307.6307.6 into that, right, you're going to do here, you crash into wall, you saw there's a wall311.68311.68 here and here, ah, there's a wall here, you go maybe here, oh, there's no wall.316.14316.14 So you go further, there's no wall anywhere here, you crash here, okay, we already knew320.86320.86 there's a wall, maybe you crash here.323.52323.52 Alright, so right now, you have, okay, you go here, you have you have a model of the328.48328.48 world in this situation, where there's a wall here, a wall down.333.78000000000003333.78000000000003 And if you now try to do trajectory optimization, remember, you have to go from here to here.340.40000000000003340.40000000000003 If you try to do trajectory optimization, what is it going to turn out, it's going to344.96000000000004344.96000000000004 turn out like look, there you go, that works just fine.350.46000000000004350.46000000000004 And that's not because you're so good at planning.353.48353.48 I mean, you are good at planning, but because your model is inaccurate here, because it358.68358.68 has never seen this, your entire training distribution that you trained the world model363.40000000000003363.40000000000003 on only explored the area over here.366.96000000000004366.96000000000004 Right?367.96000000000004367.96000000000004 So you see how the more efficient this planning algorithm is, like the blue arrow, the thing373.68373.68 that finds the blue arrow, the more efficient that is, the more consequential it is when378.68378.68 your learned world model has mistakes, because it will exactly exploit these mistakes in385.2385.2 order to get the shortest path possible or the highest reward in that case.392.12392.12 And this, they call this like almost an adversarial attack on the world model, which is a pretty397.72397.72 good way of framing it.403.48403.48 They propose actually to solve this problem.406.88406.88 They say we propose to regularize trajectory optimization by means of a denoising auto411.68411.68 encoder that is trained on the same trajectories as the model of the environment.416.62416.62 We show that the proposed regularization leads to improved planning with both gradient based421.54421.54 and gradient free optimizers.423.74423.74 We also demonstrate that using regularized trajectory optimization leads to rapid initial428.82428.82 learning in a set of popular motor control tasks, which suggests that the proposed approach434.71999999999997434.72 can be useful tool for improving sample efficiency.439.20000000000005439.20000000000005 So in essence, what do they do?442.56442.56 They basically say, okay, we want to regularize this using a denoising auto encoder.450.32000000000005450.32000000000005 And I think it's best if we if we look at the at the math for doing this.456.56456.56 So the math here starts off as follows saying, you want to learn a world model.464.48464.48 This is f here, f is the world model, it takes in a state and an action, and it gives you469.72469.72 the next state or an approximation to it.473.72473.72 And the parameters here indicate that this is some sort of function that you learn like477.64000000000004477.64000000000004 a deep neural network.479.64000000000004479.64000000000004 You can do this in fully or partially observed environments.483.96000000000004483.96000000000004 Now, when you plan, what you want to do is you say I have a planning horizon H, right,492.44492.44 and I have a reward function.495.32495.32 And the reward function is going to give me a reward for each state action pair.500.12500.12 So if I'm in a state and I do a certain action, I'm going to get some reward, this could be506.58506.58 you have reached the target, or this could be you know, how much money you've collected510.92510.92 or whatnot.511.92511.92 So you're going to look at a horizon H, you're going to look eight steps into the future,517.56517.56 and you want to maximize the sum of all the rewards here.521.72521.72 So in the limit, this reduces to simply like, for example, reaching the target in our rooms527.52527.52 case, if if H is infinite, but you can consider a lower planning horizon.534.0400000000001534.0400000000001 So you want to find the action sequence that maximizes this reward in the future.540.84540.84 Okay.541.98541.98 And now this reward relies on your environment model.548.5600000000001548.5600000000001 Okay.549.96549.96 So here's the the algorithm.553.52553.52 First you collect some data, okay, that's how you start off, then train the dynamics558.64558.64 model, the world model using the data you've already collected.563.36563.36 Then for each time step t, you want to optimize this trajectory.568.12568.12 So you want to find the best next action sequence and take the first action implement the first573.12573.12 action and get the new observation.575.94575.94 And do you do this in a loop until the end and at the end, you say at this data to D.580.84580.84 So that's what that's what you do, you use your world model to get the best action sequence.588.08588.08 That's how you optimize the trajectory.590.6400000000001590.6400000000001 And then at the end of the episode, you've you've done an episode, right, you went somewhere,595.6595.6 you put all of this into your training data to make the world model better.601.98601.98 Something to note here is that the world model will only learn about things that you have608.04608.04 done, right.610.16610.16 So there is kind of an interaction effect.612.04612.04 That's the green area here.613.6800000000001613.6800000000001 The world model only knows the paths, the world model only can accurately estimate the618.76618.76 world where you have been.622.6622.6 And that's going to turn out to be the entire problem because the these blue arrow finder629.02629.02 can now go away from the from that.636.1999999999999636.1999999999999 That's explained here, potential inaccuracies of the trained model cause substantial difficulties641.84641.84 for the planning process.644.48644.48 Rather than optimizing what really happens, planning can easily end up exploiting the649.12649.12 weaknesses of the predictive model.652.12652.12 Planning is effectively an adversarial attack against the agent's own forward model.656.46656.46 This results in a wide gap between expectations based on the model and what actually happens.663.0663.0 Okay, and they have this, this example here, where it's like an industrial control process.669.84669.84 And what you have to imagine, there's like some sort of a container here with liquid674.84674.84 liquid in it.676.6800000000001676.6800000000001 And there are two two pipes that lead to this container, pipe one, and pipe two.683.4000000000001683.4000000000001 And there are valves here.685.0400000000001685.04 So there's this valve right here.687.48687.48 And there's this valve right here.690.4399999999999690.4399999999999 So these are valve one and valve two.692.98692.98 And there is also an output pipe right here.695.78695.78 And that's a another valve right here.698.5799999999999698.5799999999999 So you can control these three valves, the in two inputs and one output, and you have704.4704.4 to somehow optimize the reaction in here.709.5799999999999709.5799999999999 So this is a chemical reaction made up out of the two liquids that flow in here.714.52714.52 And you have to somehow optimize a property of that.717.0717.0 And that's highly nonlinear and has maybe like time, time shift.721.18721.18 So when you open a valve, it's going to take a while and then it's very nonlinear.726.24726.24 And then you are not supposed to break the pressure limit.729.54729.54 So you have to also outflow some stuff.732.9732.9 And if you just do this with a learned model, it looks like this.737.4399999999999737.4399999999999 So first of all, here is a classic controller, like people have been doing this stuff in742.48742.48 industry, and they basically build controllers for it.747.6747.6 And you can you can do that.749.0600000000001749.0600000000001 And that works out really okay ish.752.12752.12 As you can see, right here, this is the product rate, what you're supposed to optimize.757.0757.0 And you see some sort of a smooth, you're supposed to actually bring it to this dashed762.76762.76 line right here.764.6800000000001764.6800000000001 And this is some sort of smooth thing, right?768.28768.28 And you're supposed to, I guess, bring the pressure here and the A in purge, I don't773.8399999999999773.8399999999999 know what these quantities are, but you're supposed to bring them to the dashed line.777.56777.56 And it's very nonlinear, and very time dependent.782.1782.1 So that works.783.1783.1 And you see here kind of the smoothness by which the variables are manipulated.786.52786.52 Now, if you just learn a world model, and then do this trajectory optimization, basically,794.48794.48 this is some sort of a, yeah, reinforce planning based reinforcement learning with a world800.28800.28 model.801.36801.36 You see right here, it works, but it's super jittery, the pressure spikes here.807.88807.88 And apparently, this here is a pressure limit.810.48810.48 So it spikes the pressure limit.812.72812.72 And you can see that the manipulated variables are up and down and up and down and up and816.4816.4 down.817.4817.4 Because at each step, it basically completely overestimates its its potential reward, it823.08823.08 thinks like, wow, this is really good.825.12825.12 But all it does is find a weakness in the model and not a really good action per se.830.94830.94 Now with their method, to already take it away, you can see that now the control task837.0600000000001837.0600000000001 super smoothly and very quickly converges to these optimal things.842.36842.36 And you can see that the variables being manipulated are also rather smoothly manipulated.848.2800000000001848.28 And that's an indication that the model is accurately estimating the rewards.856.76856.76 Okay, so how do they do it?860.6860.6 via what they call trajectory, we are regularization of trajectory optimization.867.04867.04 So in essence, what do we want to regularize here, there are many things we could do to871.3199999999999871.3199999999999 solve this.872.64872.64 But the way this paper goes is they say we not only do we want the most return, we also881.4399999999999881.4399999999999 want a high log probability of our of our taken path.888.5888.5 So this here, as you can see, this is observation action, and, and so on observation action.894.88894.88 So this is the future.896.8199999999999896.8199999999999 This right here is the future.902.3199999999999902.32 So this, this sequence here is going is what is going to give me the reward right here,910.5600000000001910.5600000000001 right.911.5600000000001911.5600000000001 So G is also dependent on these things, but it's not said explicitly here.916.0600000000001916.0600000000001 So G is dependent on your plan, maybe let's not call this the future.919.96919.96 This is the plan.921.5600000000001921.5600000000001 Okay, this is the plan you came up with.925.2600000000001925.2600000000001 So this is directly going to influence G, and G is the reward you're going to get under929.12929.12 your model.930.12930.12 And also you want the log probability of the plan itself to be high.935.32935.32 Now there, I think there is a bit there is something missing here.938.86938.86 And that is conditioned on your training distribution right here.944.0944.0 And I think that's a actually rather crucial part.946.84946.84 Now this, that's the KL thing.949.9949.9 So this is conditioned on your training.951.88951.88 So what you want is you want the plan to be basically in your training distribution.960.66960.66 So you, you want what you, you want your plan that you're going to execute, if that is actually968.5968.5 part of your training data set, then you know, I have already executed this once before.976.24976.24 And it's reasonable to assume that therefore my world model has learned from this experience982.36982.36 and is going to give me an accurate reward.985.7985.7 If we go back to our rooms example, then up here somewhere, if we go back to our rooms993.24993.24 example, right, you see that anywhere in the green area where I have already explored,999.1800000000001999.1800000000001 the world model is fairly good, right, it's going to give me accurate reflection of the1004.441004.44 world.1005.441005.44 But if I want to go outside the green area, it is not.1009.61009.6 And inside the green area is basically where my training data is.1014.40000000000011014.4000000000001 Now if I in the future actually take a path here, crash into a wall right here, right,1020.68000000000011020.6800000000001 the you saw in the algorithm, at the end of an episode, I'm going to add my trajectory1025.181025.18 to the training data for the world model.1027.761027.76 So this green part here expands to include that.1032.01032.0 And now, if I go here again, if my plan goes there again, now I can trust the world model,1039.481039.48 but also now it has it is actually correct because it has a wall here.1043.21043.2 So you see that the regularization basically you not only do I want the biggest reward1049.041049.04 possible under my world model, I also want that the plan that I'm about to execute is1055.841055.84 has a high probability under my training distribution.1059.521059.52 Okay.1060.621060.62 And the way we do this is by denoising auto encoders.1064.87999999999991064.8799999999999 So I want the log probability here to be high and you do this via a denoising auto encoder.1074.91999999999981074.9199999999998 What's a denoising auto encoder?1077.121077.12 A denoising auto encoder is basically so if you have, for example, an image and the image1086.121086.12 is of a our trusty cat whiskers and a denoising auto encoder is an unsupervised method where1095.521095.52 you have it's basically an auto encoder.1097.821097.82 So there's a bunch of layers compressing to a hidden representation, then uncompressing1104.081104.08 it again.1106.081106.08 Okay.1107.61107.6 And at the end, you want to output the same as at the beginning.1113.67999999999981113.6799999999998 So it's basically an auto encoder.1116.021116.02 But the special part about the denoising auto encoder is that first, you take your input1122.321122.32 and you know, you put some noise on it.1125.121125.12 So that could mean could mean anything here.1128.521128.52 But here, what they do is they do they make some Gaussian noise on it.1133.41133.4 Now, I can't really draw Gaussian noise here.1136.161136.16 But it would be kind of convolved with Gaussian Gaussian noise.1138.981138.98 So I'm just going to add some noise like this.1142.441142.44 So noise, noise, noise.1145.441145.44 So there's some noise, you see, and then you feed that.1150.81150.8 That's now what you feed in here.1153.481153.48 And the algorithm is supposed to reconstruct this, this original image so that the algorithm1159.761159.76 is basically supposed to take away the noise, it doesn't see the original image, but it's1164.721164.72 supposed to produce it.1166.481166.48 And you do this with your training data.1168.21168.2 So what does that mean, ultimately, for our trajectory optimization?1173.021173.02 It means that if I have a trajectory that I did before, and it maybe goes here, right?1181.581181.58 What I can do is I can make a noisy version of it, which would be the black one right1189.41189.4 here.1190.41190.4 So I put some noise on it, some noise.1193.21193.2 It's, it's kind of the same, but okay, and the denoising auto encoder is supposed to1199.01199.0 give me back the red one.1201.441201.44 This will simply give me some sort of a probabilistic model of my training distribution.1207.481207.48 So they go through the math here and show that these denoising auto encoders actually1211.281211.28 naturally output this log probability, sorry, the gradient of the log probability.1219.12000000000011219.1200000000001 Because optimal denoising theory says that for zero mean, and Gaussian noise, the optimal1226.961226.96 denoising function, the optimal, the optimal denoising function for zero mean Gaussian1234.041234.04 corruption is this thing right here.1237.541237.54 So it is, if, if I if you give me x, and you tell me x is has been corrupted by zero mean1247.721247.72 Gaussian noise of size sigma n, then the best and do you simply tell me give me1256.921256.92 back the original image, the best thing I can do is to take what you gave me and add1263.32000000000021263.3200000000002 this gradient of the log probability of x, if, if I can, if I have a model of the log1270.56000000000021270.5600000000002 probability, right.1272.481272.48 So that's the best thing I can do.1276.921276.92 And that's the best denoising function.1279.58000000000021279.58 And now you have to think a bit of in reverse, if we train a denoising auto encoder, that1287.621287.62 is going to approximate this best function that there is, okay.1292.261292.26 So we know that the best possible denoising function is this, we train a denoising auto1297.43999999999981297.4399999999998 encoder, which in the optimal case is going to converge to the best denoising function.1303.15999999999991303.16 So if we then reformulate, and we do denoising auto encoder of x minus or x tilde minus x1314.921314.92 tilde, that is go or divided by the standard deviation.1319.281319.28 Sorry, the variance that is going to give us this quantity right here, the gradient1325.521325.52 of the log probability, and the gradient of the log probability of x is exactly what we1335.241335.24 need to run gradient descent on our function.1339.51339.5 So here is our function again, g plus this regularization.1343.361343.36 Now they don't regularize over the entire future, but over these windows.1347.921347.92 But in essence, it's g plus the log probability of your plan.1352.121352.12 If you take the gradient of that, of course, you take the gradient of the sum, so it's1356.19999999999981356.1999999999998 the gradient of g plus the gradient of the log probability with respect to the actions.1364.281364.28 And here's simple application of the chain rule will tell you that you have to propagate1370.01370.0 through the input through the x and you need this quantity, the gradient of the log probability1376.01376.0 with respect to its inputs.1379.47999999999981379.48 Now as we just saw, the optimal denoising auto encoder is going to output that problem1388.241388.24 that thing.1390.41390.4 So if we train a denoising auto encoder, and we suppose it's reaches a good accuracy, then1396.921396.92 we can obtain this quantity basically for free.1400.81400.8 And that's the entire trick here.1404.281404.28 So in essence, what does it mean?1408.081408.08 In essence, what it means is that if we are in our room again, and we have our partial1414.43999999999981414.4399999999998 model of the world, let's say we have this model, because we are here, and all we've1420.39999999999991420.3999999999999 ever explored is so we've explored this, these things right here.1425.87999999999991425.8799999999999 Okay, and this.1429.841429.84 Now when I go and do my trajectory optimization, and my trajectory optimization wants to go1435.361435.36 here, I simply say, no, I don't know that I haven't seen that yet, you can only plan1440.81440.8 basically within the space where we have already been.1445.361445.36 So you can plan like here.1449.67999999999981449.6799999999998 So here now there is of course, there is going to be some exploration.1454.41999999999981454.4199999999998 So some probability that you can go away a bit, but not too much, right.1459.61459.6 So in this case, it would result in the planning only to happen in spaces where we've actually1465.261465.26 been.1466.261466.26 So it might go here, and then here, because okay, here, we haven't been anywhere.1471.641471.64 But then that would lead me to take the first step in this direction, and not in this direction.1478.321478.32 And if I take my first step in this first direction, then of course, I'm going to be1484.21484.2 already a bit on the correct path right here.1486.921486.92 Whereas if I take the first step into this direction, then after that, I'm gonna have1491.041491.04 to, if once I crash here, I'm gonna have to correct really hard.1495.281495.28 And that's exactly what's going to give you this super jittery control.1500.01500.0 Whereas if you only plan where you've already been, you won't the probability that you're1505.21505.2 going to have to do like a 180 is going to be much, much lower.1510.841510.84 Okay.1513.71513.7 That seems like1518.87999999999991518.8799999999999 that's about it.1519.87999999999991519.88 Let's look at the experiments.1521.721521.72 So their experiments.1526.21526.2 Basically, I actually want to go down here to this industry, sorry, not the industrial1531.961531.96 control process, but to the Mujoko experiments.1536.661536.66 So these are kind of continuous control tasks, you might have seen it.1540.32000000000021540.32 So there's some like one is a, a, the ant here is basically this 3d.1550.481550.48 And it's like a blob.1552.081552.08 And it has, I think, four legs, and each leg has two joints.1555.561555.56 And it just needs to walk as far as possible, or reach some sort of goal.1560.241560.24 And the half cheetah is like a 2d thing where I think it's something like this.1567.11567.1 It also has these two legs, and it's supposed to walk forward and not fall over.1572.67999999999981572.6799999999998 And you can put force basically on each of the of the joints here.1579.15999999999991579.1599999999999 So you see that their baselines are Gaussian processes.1584.821584.82 And this pets thing is a previous baseline to do, to also do model based control with1593.81593.8 a learned model.1596.13999999999991596.14 And here they, theirs is the main, their main one is the red one.1602.80000000000021602.8000000000002 And as you can see that it goes much faster, well, basically outperforms the rest in these1610.28000000000021610.2800000000002 high in these more complicated tasks.1613.381613.38 And then card poll or something like this is is lower dimensional, easier tasks.1619.51619.5 And you can see that at least it does not hurt.1624.58000000000021624.58 They make they say here, something they don't, they don't show in the plots, they say that1632.13999999999991632.1399999999999 if you let this run for a while, then basically the their method doesn't make any improvement1640.67999999999981640.6799999999998 anymore, whereas the baseline methods will sort of at some point surpass it.1647.89999999999991647.8999999999999 And the reason that is, and I'm not sure if it's on this exact task, but they mentioned1653.11653.1 that which it's it's I respect so far, is because they say since we only plan where1661.31661.3 we know, where did I draw it?1665.89999999999991665.8999999999999 Since we only plan where we know, we basically do much less exploration than others, we kind1672.93999999999981672.9399999999998 of stick to what we know when we plan.1676.11676.1 So inherently, we do less exploration.1678.13999999999991678.14 And in our conversation with Hari, he basically said this, this is intended.1684.56000000000021684.5600000000002 And the base, the intention is that you want to do your planning, where you know, and then1690.58000000000021690.5800000000002 explicitly add a component that does exploration.1694.30000000000021694.3000000000002 So you have control over so you can basically say, huh, I, I've never been here, sort of1700.86000000000011700.8600000000001 now, you would be in an exploration phase, you would explicitly go there, rather than1707.261707.26 intermingle your planning with your exploration and basically rely on your planning to screw1715.281715.28 up and you're exploring because if your plan, if your planning never screws up, then you1722.781722.78 won't explore either, right, then you will always reach your goal, or your planning will1727.181727.18 always be correct.1728.181728.18 And these other methods that don't have this explicitly, they explore every time their1733.161733.16 planning screws up, and you don't want that.1735.31735.3 You want your planning to be as good as possible.1738.31738.3 And they do that by sticking to what they know.1740.89999999999991740.8999999999999 And then they the next step, which is not in this paper would be to add an explicit1745.241745.24 exploration policy to reach areas they've never reached before.1750.781750.78 Okay, so that's the reason why they don't ultimately reach the best accuracy.1757.021757.02 But they do reach a the initial accuracy much faster than the other tasks, because they1764.89999999999991764.9 plan better.1765.91765.9 They have a long discussion here of what still problems are, like local minima, or the planning1774.28000000000021774.2800000000002 horizon problem, open loop versus closed loop compounding errors in planning.1780.54000000000021780.5400000000002 But I'm going to leave this out for now.1783.30000000000021783.3000000000002 And I thank you for being here, I very much invite you to check out the paper for more1788.58000000000021788.5800000000002 details.1789.58000000000021789.5800000000002 It's pretty cool, pretty easy to read, actually, it's very written very well.1793.84000000000011793.84 And with that, see you next time.1795.981795.98 Bye bye.1825.58\"}"}