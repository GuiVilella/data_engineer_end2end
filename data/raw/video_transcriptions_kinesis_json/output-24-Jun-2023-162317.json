{"data": "{\"value\":\"0.0 Hi there, today we're looking at vertex learning visual representations from textual annotations6.26.2 by Karen Desai and Justin Johnson of the University of Michigan.10.8210.82 So this paper at its core is pretty simple.14.0614.06 On a high level, it proposes to take the task of image captioning, which is where you're19.1819.18 given an image and you're asked to produce a caption for the image, and basically train24.224.2 a model to do this, and then just take the visual part of it as a baseline to transfer31.0831.08 learn on other visual tasks.33.6433.64 And that appears to work surprisingly well if you don't have much data.40.7640.76 So if you if you don't have much data to pre train on, this appears to work very well.46.8446.84 Alright, as always, if you like content like this, then consider sharing it out, subscribing53.7853.78 to the channel, or tell me what you think in the comments.58.4658.46 So as I already said, the the idea here is pretty simple.64.2400000000000164.24000000000001 So people have been looking for pre training tasks for visual tasks.69.6869.68 So a visual task is anything where the input is an image.74.7674.76 And then you usually have some sort of neural network that processes the image.79.179.1 And then at the end, you can have many things.81.2400000000000181.24 So you could have a classifier that classifies the image into one of many classes, if you86.1999999999999986.19999999999999 know ImageNet, that's a thing.89.889.8 So if there's a cat here, then the the ImageNet classifier here would say cat, or you could97.1999999999999997.19999999999999 have something like a object detector that tries to predict on the image where the cat103.96103.96 is like with a bounding box, you could have a you could have a semantic segmentation where111.0111.0 it's like, all of these pixels here are cat.114.92114.92 And maybe all of these pixels here are sky.118.08118.08 And so it labels every pixel, there's many visual tasks that you can formulate.124.48124.48 And they all sort of share the same architecture.127.48127.48 And specifically, they all share this part right here, the if you will, this is the visual133.48133.48 encoder, it's usually a convolutional neural network.137.42000000000002137.42 And what's really different between the tasks is mostly this last part here that does the142.2142.2 actual task.143.72143.72 But this and is often called the backbone.146.67999999999998146.67999999999998 So this is the backbone.148.64148.64 And the idea now is, if I have a bunch of these tasks, sometimes I don't have money155.1155.1 labels for these tasks, I don't have many labeled images, so that I could train this159.2159.2 big architecture from scratch, like in medical images, or just in domains where you don't165.95999999999998165.95999999999998 have many images.166.95999999999998166.96 So couldn't I somehow come up with a method to create this backbone beforehand.173.36173.36 So to create backbone given another data set.177.20000000000002177.20000000000002 And the simplest variant here is you take a big image data set, such as image net, and183.0183.0 then you train a classifier, like we said, to predict some classes on it.188.44188.44 And then because an image net has a lot of images, then this is your backbone.192.92000000000002192.92 And then whenever you have a different task, you simply take the backbone, transfer it196.79999999999998196.79999999999998 over and then train the other basically, you continue training on the other tasks that's203.01999999999998203.01999999999998 called transfer learning.205.42205.42 The question is, how do you get a good backbone.209.39999999999998209.39999999999998 So if you train on something like image net, then this is of course a supervised tasks,214.92214.92 you have a very good learning signal, but even image net has like 1 million images.220.1220.1 But for example, the internet has many more images.222.88222.88 So what you could do is you could train on this much bigger data set that you collected228.04228.04 from the internet, let's just call it internet.231.74231.74 But there you don't have labels, right.233.62233.62 So what you'll have to resort to is instead of supervised learning is self supervised237.92237.92 learning, where you have an image and maybe you rotate it to the right.241.98241.98 So here is our cat, he rotated to the right.245.48245.48 And then you have a classifier that predicts that these this image was rotated to the right,252.23999999999998252.23999999999998 and then that will become your backbone.254.5254.5 These self supervised methods, they work very well.259.32259.32 There is a different number of them, for example, Moco, things like this.264.08264.08 And there's also a number of techniques that do supervised pre training, and then transfer268.74268.74 learning, you can maybe watch my video on big transfer, which is a very large attempt274.76274.76 to do to pre train a backbone for visual tasks.279.28279.28 Alright, now, you can see right here that the sort of direction is that the more data,286.71999999999997286.71999999999997 the better.287.71999999999997287.71999999999997 So that's sort of the idea here that image net is a big data set, we can train a really291.58291.58 good backbone.292.58292.58 But you know, the internet is an even bigger data set, we don't have labels.296.5296.5 So there's a trade off.297.82297.82 But we potentially can train an even better visual backbone to then transfer learn with303.03999999999996303.04 this paper goes into a different direction.306.22306.22 They say, look, if you go in this direction, right here, you get more images, but you get312.0312.0 less information per image.314.18314.18 So with image net, at least you have the label, right per image.318.3318.3 But if you simply take a photo of the internet, you don't even have to label you have to resort322.88322.88 to self supervised.324.88324.88 What if we go into the other direction, and we look for images that have very high quality331.1331.1 annotations, but maybe we don't have as many can we can we do the same thing?337.20000000000005337.20000000000005 Can we learn good backbones by trading off quality for quantity in this case, and their344.18344.18 quantity and quality trade off is they go for descriptions.349.52000000000004349.52000000000004 So they'll go for something like this, where you'll have an image, and you'll have a caption357.84000000000003357.84 for the image.361.47999999999996361.47999999999996 So they show these on a line here, semantically dense, semantically sparse, but their task366.15999999999997366.15999999999997 is going to be caption generation.368.56368.56 So they're back there, their task is, given an image, I want to produce a caption.375.64375.64 And there are data sets that you can train this from in a supervised fashion, which of380.76380.76 course, these are very expensive to create.383.12383.12 I mean, if you want to create an image net data set, then you have to label each image.388.84000000000003388.84000000000003 But if you want to create a caption data set, that's even harder because human really needs393.6393.6 to sit down, look at the image and in image net, everything is like one class.399.76399.76 But here you need to look at the image.401.6401.6 And then you'll have to come up with like an adequate description.404.48404.48 Here the adequate description is an orange, sorry, an orange and white, an orange and410.52410.52 white cat near a plate, and the white cake.414.32414.32 Okay.415.59999999999997415.59999999999997 So that's, that's the caption right here.417.91999999999996417.91999999999996 And of course, the caption is ambiguous.419.82419.82 So you'll have to collect multiple captions per image.423.59999999999997423.59999999999997 And you'll have to make sure that the humans that do this do a good job and so on.427.32427.32 So these are very, very expensive data sets, but they are very high quality.432.0432.0 If you think of what does what does a single label, let's just take image net image net437.41999999999996437.42 has a single label per class.440.64000000000004440.64000000000004 Let's say this is cat or cake for that matter.443.62443.62 It just sort of gives you very few bits of information.447.86447.86 But if you consider the text here, an orange cat and a white cat, an orange and white cat,454.06454.06 you know that there is a cat, right?457.44457.44 You know that it's one cat, you know what its color is orange and white, then you know463.32463.32 that there is a white cake, right?465.58000000000004465.58 So do you know the other object?467.76467.76 And you know the relation, they are near each other.471.32471.32 Okay, same for here, a brown and white puppy.475.28475.28 So this is one object and the description of the object.479.14479.14 There is a there are apples, there is a green lawn, and you the relations between them are484.84484.84 also clear, the puppy is lying on the green lawn, and looking at the apples.490.47999999999996490.48 So the information in captions is so much more dense than just labels.496.40000000000003496.40000000000003 And that's the that's the backdrop here to say, hey, can't we can't we do?503.84000000000003503.84000000000003 Can't we pre train a backbone from maybe a small data set, but that has so much information,510.16510.16 like a caption date, image caption data set?513.4513.4 Okay, so their method is nothing more, they train image captioning, and then they use520.0520.0 the visual backbone for transfer learning.522.16522.16 So this is the model, there is an image, the image goes into this visual backbone right527.8527.8 here, which is a resonant 50.529.42529.42 So this is a very, very standard convolutional neural network.534.68534.68 And that gives you these features.536.44536.44 So these features are seven by seven by 2048.540.98540.98 This is the standard output of a resonant 50.544.96544.96 And then from this part on, they do a linear projection, such that they can now input it550.8000000000001550.8000000000001 into a language model.552.24552.24 So they have visual features.554.84554.84 And now they feed those into the language model.557.96557.96 And the language model is just a transformer, actually two transformers.564.44564.44 So one transformer, they're both auto regressive, one transformer tries to predict the caption571.26571.26 in a forward way, and the other transformer tries to predict the caption in a backward576.48576.48 way.577.48577.48 And that's down here.578.48578.48 So in this direction is backward, because the caption has been reversed.582.84582.84 If you don't know what a transformer is, I've made several videos on transformers.586.58586.58 The first one is attention is all you need.589.24589.24 And that's sort of the same, the same kind of transformer they use here.594.24594.24 So as you can see, right here, you have this multi head attention, the layer normalization602.12602.12 attention from the decoder.604.0604.0 Now the difference between the original Vasvani attention is all you need transformer.609.8609.8 And this one is that in the original transformer you had, for example, if you had a machine616.48616.48 translation task, you would have the French, maybe a French sentence over here.621.6621.6 And then you would have the beginnings of German sentence here, right, this is what627.72627.72 you have already produced.629.0629.0 And now you're asking, what should the next word be?632.6800000000001632.6800000000001 And the architecture was such that there is a decoder transformer right here, and that638.36638.36 there is an encoder transformer that encodes whatever you already had.644.62644.62 And then at some point, there is this cross attention, right, there is the signal from649.48649.48 the decoder going into the encoder, and the encoder incorporating that and then at the654.9200000000001654.9200000000001 end, right here, the encoder would predict or the entire transformer would predict what660.4660.4 the next word will be.662.12662.12 The only difference right here is that the decode this, sorry, I mix this up, this is668.8000000000001668.8000000000001 the decoder, this is the encoder.671.6671.6 The only difference right here is that this encoder is no longer a transformer.677.3000000000001677.3 What is this ResNet?680.5999999999999680.5999999999999 This ResNet 50?681.8399999999999681.8399999999999 Okay, because now you don't have an image as a you can think of it like a translation686.68686.68 task, you want to translate from images to text, okay, so your input is going to be an692.88692.88 image.694.0799999999999694.0799999999999 And the signal is going like it would go in the original transformer into the decoder,699.4699.4 it would come from the image.701.5999999999999701.5999999999999 So from these visual features goes here.704.4799999999999704.48 So in this drawing, this thing is going in here.711.5600000000001711.5600000000001 And then you simply predict the next word, and you do it in both directions.715.28715.28 And the reason you can do it in both directions here, this wasn't is not the case, of course,721.2721.2 if you have a a decoder, like a standard transformer test, because you don't need to do inference728.14728.14 at this, you just need to do training and training you can do using teacher forcing.733.34733.34 And so you can do this in a bi directional way, you don't need you don't need this at739.32739.32 inference time.740.4740.4 So at inference time, you simply cut off this part right here.746.5400000000001746.5400000000001 That's your visual backbone.748.12748.12 Okay, and these features here, those are going to be the features that you then train your753.84753.84 task on.755.12755.12 And sometimes you fine tune this or sometimes you keep it frozen, you can choose that.759.76759.76 All right, so red convolutional network to encode the images that gives you features,766.16766.16 visual features, those visual features go into two transformers, both try to predict771.56771.56 the caption of the image one in a forward motion, one in a backward motion.778.0778.0 And you train it to predict as accurately as possible the gold standard captions that784.28784.28 you have in your data set.786.48786.48 That's it, if you train this model, well, that means the model can produce accurate790.88790.88 captions for these images, which means that it has learned something meaningful about796.72796.72 the image to the degree of course, that the original caption that was in your data set801.6800000000001801.6800000000001 was a good descriptive caption.805.04805.04 But we're just we're going to assume that the in these data sets, this is the case.809.16809.16 All right, that's what they do.812.04812.04 So interesting thing here is that in their standard in their standard, in their standard818.88818.88 setup, they only have one of these transformer layers.822.42822.42 So of these things right here, they only have one.825.86825.86 And that's like, I think it's like 2000 units wide, but or sorry, the hidden dimension is832.16832.16 2000 units or 2048.834.54834.54 But they only have one layer.836.3399999999999836.3399999999999 So what that means is that this transformer is not very powerful.841.38841.38 So most that you force most of the power to come from the visual encoder, the visual encoder848.24848.24 had basically has to do most of the work.851.12851.12 And then the the transformer is going to simply be a very shallow language model on top of857.96857.96 that.860.5860.5 And that, of course, makes your visual backbone even better.864.24864.24 All right, we can pretty much skip the rest.867.78867.78 That's the idea.868.78868.78 Like that there's nothing more to it.870.08870.08 You train this from the beginning, you don't use any pre trained, whatever you train this873.96873.96 from scratch.875.3000000000001875.3000000000001 And then you use this.877.08877.08 And in the first experiment, they simply train a linear classifier on top of that representation.883.0600000000001883.0600000000001 So they freeze the backbone.885.0400000000001885.0400000000001 And then they use a linear classifier.887.4200000000001887.4200000000001 And they compare this to baselines.890.12890.12 So one of the baseline is image net supervised, where you use the same backbone.895.88895.88 But you train it on image net in a supervised fashion.899.36899.36 Okay, and then you use that backbone to transfer out of the text.902.2902.2 It's kind of like what big transfers us but just on the regular 1000 class image net baseline.911.52911.52 Then you have the sort of the unsupervised pre training once so moco so perilous perilous.919.64919.64 I want to go into pearl, but moco is this momentum contrast, which is one of these supervised926.08926.08 methods that has been shown to work really, really well.929.72929.72 And this is also moco en is trained on image net, but now without the labels, because moco937.0937.0 is unsupervised.938.64938.64 And moco coco is trained on the cocoa data set.942.36942.36 And the cocoa data set is what this paper here, the vertex paper uses cocoa is this949.14949.14 image captioning data set.951.26951.26 Now what's important to note is that cocoa has about 10% only of the images of image959.02959.02 net, so it's considerably smaller.963.3963.3 Now let's see how these things fair.968.2968.2 Right here, you can see on the x axis, the number of images, okay, the number of images974.8974.8 that the data set or that the pre training method trains on.979.42979.42 Now of course, some of these are going to be capped because for some data sets, there983.76983.76 are just not more images available, right?988.16988.16 So they're going to be capped here, the ones that are training on cocoa, and the ones that991.4991.4 are training on image net are going to be capped here.994.16994.16 You can already see that the vertex outperforms the image net supervised baseline by pretty1003.81003.8 much when you only give it this many images.1006.481006.48 Okay, so the way you do it is, in this case, you simply train these models.1011.56000000000011011.5600000000001 Now the brown one is when you take one caption per image, but the data set actually has more1018.641018.64 more than one caption per image.1020.56000000000011020.5600000000001 So when you use more than one, you can still boost boost your performance a bit.1026.081026.08 And that works way better than when you do this supervised pre training on image net,1033.641033.64 which would get you here with about the same amount of images.1037.481037.48 Now when you use all of image net, you can see here, you can get to a similar performance1042.84000000000011042.8400000000001 right here, but you have to use a 10 times bigger data set to get there.1047.961047.96 Right, so this already shows you sort of the advantage here.1053.60000000000011053.6000000000001 Now also consider the difference to the unsupervised ones.1058.12000000000011058.1200000000001 So if you look at the same amount of images, the unsupervised, the self supervised baselines1063.04000000000021063.04 are even lower.1065.561065.56 But if you go to more images, they sort of get closer to image net.1071.681071.68 And in their own papers, there are there are some evidence that if you self supervised1077.15999999999991077.1599999999999 train for long enough, you can actually surpass image net supervised pre training, but I'm1084.61084.6 not so sure that that's really the case.1089.11089.1 But you can see here the trade off between higher quality information.1096.61096.6 But smaller data sets versus lower quality information, but more data per data set.1107.91999999999981107.9199999999998 And yeah, if I guess if you were to if you were to pre train these self supervised methods1112.081112.08 with lots more data in a self supervised manner, they would maybe end up even higher than1119.041119.04 image net.1122.321122.32 Now this graph here is sort of the same thing where they also train a linear classifier.1126.921126.92 And you can see right here that now the image net supervised baseline is outperforming vertex1132.321132.32 by a lot.1133.321133.32 So what's happening here?1134.321134.32 Now this here is actually this is on image net.1137.281137.28 So the task here that you transfer learn is image net.1141.441141.44 Here it was like a neutral task Pascal VOC.1144.721144.72 None of these methods have trained on Pascal.1148.321148.32 They simply have trained on their own data set.1150.721150.72 These have trained on Coco.1152.241152.24 This has trained on image net and then they have transfer learned to Pascal.1156.67999999999981156.6799999999998 Now the task is actually the transfer learning task is image net.1161.321161.32 So naturally, the the thing that was pre trained in a supervised fashion on image net is going1168.961168.96 to have a huge advantage in this task because it basically has already learned the task1173.91999999999981173.92 beforehand, whereas the vertex, it has pre trained on Coco, not on image net.1181.60000000000011181.6000000000001 And you can see, if you give it the same amount of images for pre training, it can actually1186.84000000000011186.8400000000001 it's it's fairly close to the image net baseline.1190.241190.24 So that's pretty respectable right there.1192.761192.76 Now, again, of course, if you use more images on the same data set that you then train for,1198.881198.88 then of course, the the image net baseline is going to outperform it.1203.56000000000021203.56 But so pretty cool to see here that in this smaller image regime, and also consider this1210.441210.44 down here, if you go even an order of magnitude lower, it's really shining that if you have1216.541216.54 higher quality information, and you make use of it, you don't need as many images.1223.321223.32 And now we knew this for a long time.1225.281225.28 But this now is showing the same for transfer learning for visual transfer learning.1234.89999999999991234.8999999999999 So this was when we froze the backbone.1237.961237.96 And then we trained a linear classifier on top, they go, they make a short excursion1245.61245.6 here and show how different parts of their model affect their final performance.1252.21252.2 And they find that, for example, the by captioning, which I believe is the is forward and backward1260.60000000000011260.6000000000001 captioning significantly helps, for example, compared to only forward captioning.1267.061267.06 And they also find that it significantly outperforms other pre training tasks that they could do.1274.62000000000011274.6200000000001 And they also investigate whether how big their models should be.1278.461278.46 So here, this is their baseline model.1280.641280.64 Oh, I was I was wrong, actually, they the it's one layer of width 1024.1291.28000000000021291.2800000000002 You can see as you make the layer bigger and bigger, that generally helps.1296.80000000000021296.8000000000002 But I guess they decided against it because the gains are too little to, to afford to1303.21303.2 make it worth.1304.54000000000021304.5400000000002 And also, if you make the network deeper here, you make the transformer have more layers,1310.41310.4 the performance goes up, but again, the gains are marginal.1313.681313.68 So I guess they're going to leave it away.1315.32000000000021315.3200000000002 So their baseline, as you can see, is these resnet 50 with the one layer of 1024 size.1325.56000000000021325.5600000000002 So this is now the last task, it's the fine tuning task.1330.981330.98 So this is what most people would do is they would train a backbone, and then they would1336.41336.4 fine tune it on a different data set on or on a different task where they don't have1341.521341.52 much labels.1343.08000000000021343.0800000000002 And here the situation looks a bit different.1346.241346.24 So if you look at, for example, a task on cocoa, so there are several tasks on cocoa,1353.04000000000021353.0400000000002 one of them is image captioning, which they use for pre trade for pre training.1358.221358.22 If you do other tasks on cocoa, you can see right here, that compared to the supervised1366.01366.0 baseline, this vertex, it performs about the same, or maybe a bit worse.1374.11374.1 But what you can see is it performs significantly better than, for example, Moco that was only1382.21382.2 trained on cocoa.1383.921383.92 So again, this shows that if you have the same data set, higher quality information1388.521388.52 makes it worth it.1389.961389.96 And it's even better, as you can see, on Moco that was trained on ImageNet.1395.081395.08 It's just not quite as good as the supervised baseline.1399.43999999999981399.4399999999998 But all of them, of course, are better than just a randomly initialized network that is1403.861403.86 trained from scratch.1404.861404.86 I mean, that's the entire point of transfer learning, that you are better than simply1409.961409.96 learning from scratch.1411.41999999999981411.4199999999998 And this shows throughout this experiment, except in this LVIS masking task, where they1419.461419.46 do outperform the other things, the other methods significantly.1426.761426.76 Now the lower numbers on this task also means that the task is harder than these tasks right1433.641433.64 here.1435.31435.3 And therefore, there are more gains to be made.1437.241437.24 And therefore, you could hypothesize that the bigger, the more quality information that1442.781442.78 you input can be used in a better way.1446.21446.2 So maybe more complex also, the more complex a task is, might also have an influence on1451.821451.82 how well the transfer learning works, if you come from a high quality transfer learning1457.681457.68 task versus a low quality transfer learning task.1462.21462.2 Yeah, so lastly, compare here with the, again, with Pascal VOC object detection, and these1473.661473.66 iNaturalist classification, where I believe this is also a transfer learning task with1479.71479.7 fine tuning.1481.71481.7 And as you can see, they can also hold up against the supervised baseline, or even outperform1488.82000000000021488.8200000000002 it at some times the green triangles mean that they outperform it by a significant margin.1494.86000000000011494.8600000000001 But then on this task right here, they again lag behind.1498.941498.94 So I think the point of the paper isn't really to show that that this is the best thing ever.1507.36000000000011507.3600000000001 But the point of the paper is to show that you can go about pre trainings, basically,1513.421513.42 the common assumption is that you need more and more and more and more data for your model1518.481518.48 to learn about the data set.1521.561521.56 And they conclude here, no, actually, you can do with with very few data points, as1527.31527.3 long as they have high quality annotations.1530.581530.58 Okay, so I think that's the point of the of the paper that and they don't always outperform1536.65999999999991536.6599999999999 the other baselines and whatnot.1539.841539.84 But they keep they keep the performance the same, which basically means that this is an1544.821544.82 option.1545.821545.82 Here is a pretty cool result where they visualize the attention of their image captioning model1552.39999999999991552.3999999999999 because they train an image captioning model.1554.861554.86 And you can really see that the image captioning model learns something meaningful about the1559.17999999999981559.1799999999998 image.1560.17999999999981560.1799999999998 So when it's a bird flying, the attention is mainly on the bird, as you can see, then1566.81566.8 over the the attention widens out over the image, air.1572.121572.12 So over the air, the attention is here in the sky and on the on the ocean.1576.541576.54 And then it goes near the ocean.1579.821579.82 And then the attention is on the ocean itself.1584.261584.26 As you can see, so they have a bunch of these images and they're they're pretty cool here1588.221588.22 a dog so focused on the dog riding on and then you can see the attention going down1594.941594.94 because on is riding on means probably there's something below the dog.1602.661602.66 A surfboard.1604.361604.36 Now the attention is fully on the surfboard in.1607.661607.66 So as soon as you say in the attention as you can see it widens out.1611.941611.94 So I think that's, that's fairly cool, fairly cool demonstration that the model understands1617.421617.42 sort of the the in relation, namely, if it is focused on something, and that something1624.221624.22 is in something else, then it widens the attention out to see what it is in, okay, the ocean,1631.31631.3 and then it focuses the attention on the ocean.1634.421634.42 So that's, that's a pretty, that's a pretty cool result.1637.741637.74 I guess we already knew this because we could train image captioning models before, it's1642.62000000000011642.6200000000001 just to show that it actually makes sense to use them as a pre training task for backbones.1650.061650.06 Now, what's the future of this, the authors here in their introduction, they make a claim1655.981655.98 that this has a good future because they here they only train on this small data set, right?1663.041663.04 It's smaller than ImageNet, as you can see here, and they already get the same performance1668.13999999999991668.1399999999999 as if you train on the whole ImageNet data set in a supervised fashion.1672.221672.22 Of course, they're also supervised, but they have 10 times less images.1677.89999999999991677.8999999999999 And they they say something to the effect of you do you know, it would be pretty easy1683.341683.34 to collect more data for this task because the internet is full of images.1688.061688.06 And mostly these images have like some text with them.1693.37999999999991693.3799999999999 They you know, they have these descriptions or they have text around it, people write1697.581697.58 something about the images, you could like mind Twitter and then the responses when someone1702.261702.26 posts an image might tell you something about the image.1705.541705.54 But this definitely counteracts their, this definitely counteracts their notion that these1712.061712.06 are very high quality labels, right?1715.221715.22 Their entire point here was that the annotations these, these data sets with these these image1721.86000000000011721.8600000000001 captioning data sets like Cocoa, they have very, very high quality annotations.1726.781726.78 So this this text here is very high quality is really a descriptive text of the image1733.061733.06 that tries to capture what a human can see visually in the image.1739.141739.14 And as soon as you go out to the internet and collect a text around images, that's not1744.721744.72 going to be the case that information is again going to be quite low quality.1749.86000000000011749.8600000000001 And so I doubt that the performance here would hold up or that the claim you can easily,1754.981754.98 you know, you can easily create more data for this task holds up.1761.21761.2 So that's a bit my worry about the future of this, but it's definitely cool.1765.62000000000011765.6200000000001 And definitely shows these quality quantity trade off very well.1770.62000000000011770.6200000000001 All right, that was my two cents to the paper.1773.51773.5 I invite you to read it and tell me in the comments what you think about it.1779.51779.5 And I'll see you next time.1806.1\"}"}