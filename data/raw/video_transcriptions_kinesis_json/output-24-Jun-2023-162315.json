{"data": "{\"value\":\"0.0 Hi, here's a question for the whiz kids among you. Is this system here controllable at a point xe with asymptotic control ue?11.011.0 I'll give you 10 seconds. Okay, 10 seconds are over. So to solve this, it's actually pretty easy.17.017.0 All you need to do is first differentiate the system with respect to its internal variables, which are the x's, to obtain the Jacobian A.25.025.0 Second step, differentiate the system with respect to its control variables, which are the u variables to obtain the matrix B.32.032.0 Look, this is a zero, like this is not hard. Then evaluate A and B at the point that you want and the control point that you want. Pretty easy.41.041.0 Calculate the controllability matrix. Come on, that's nothing. Another zero.47.047.0 And at the end, you calculate the rank of the controllability matrix. Now if the if n minus D is zero, the system is controllable.57.057.0 And optionally, if you want, if you feel like it, you can output the control feedback matrix as an equation three, which gives you this here.65.065.0 Now what's equation three? Equation three is super duper simple.72.072.0 It's just this little sort of integral thing, inverse matrix trace transposed exponential function, outer product thing.84.084.0 Come on, what's the matter with you?88.088.0 Okay, so if you found you can't do this just on the spot, then you are in the same category as most people. But interestingly, apparently, according to this paper, a deep learning system can.104.0104.0 So today we're going to look at deep differential system stability, learning advanced computations from examples by Francois Charton, Amaury Hayat and Guillaume Lempel of Facebook AI Research and \\u00c9cole de Pont Paris Tech and Rutgers University.122.0122.0 So at this, in this paper, these authors basically propose that you can learn these complex mathematics with a model that has no clue about mathematics. In fact, it is a language model.136.0136.0 And it can output the solutions, for example, whether or not a system is controllable, which are sort of binary solutions, but it can also output actual solutions as in numbers or as in the matrices that you would need to obtain from these problems.155.0155.0 So that's pretty cool. And it is built upon this other paper called, I think, some deep learning for symbolic mathematics or something, I have made a video on it, if you search for it, and I'll also link it in in the description.173.0173.0 And you can go check that out, because that's sort of the basis. So in this previous paper, I think it was from partially the same authors, they have investigated language models into integrating functions.186.0186.0 So you have some sort of function, you're trying to find the integral, and they've tried to do that. Now they go a lot further.194.0194.0 So they look at these differential systems, which are characterized by these differential equations. So if you've never seen differential equations, it's basically an equation where the derivation of some variable is characterized by the variable itself.215.0215.0 So the the, the gradient, if you will, or the coban, the derivation, according to some input variable here, it's most often it's time and physical systems is a function of that variable itself, and partially also other variables.231.0231.0 So you can have systems of differential equations that all depend on each other. And there are a number of questions about these systems, these are very relevant in like physics or engineering, control theory, and so on.245.0245.0 So they investigate different problems that you can solve with these. They investigate specifically problems where we already know the solutions, but the solutions require very complex mathematical manipulation, such as, as you've seen, calculate the integral of something, take the trace, calculate the rank, invert some matrix.270.0270.0 So all of these mathematical steps are required to solve these problems, if we were to teach them to math students or engineering students. And this paper basically says, if we just input the problem into a big language model, and, and ask it to output the solution, it can do it.290.0290.0 So basically can learn to do all of these things. So that's pretty, pretty surprising, because you don't program it to do any math. So the first problem they look at is this local stability problem.304.0304.0 So, in, I don't, I don't really want to go into, into much of the actual mathematical problems, but we'll look at the first one to just give you an idea of what these sort of problems are.317.0317.0 So, x e is an, if x is an equilibrium point, it means that all solutions, if all solutions converge to x e when their initial positions are close enough, the equilibrium point is said to be locally stable.335.0335.0 Okay, this problem is well known if f is differentiable in x e, and answers provided by the spectral mapping theorem.343.0343.0 So, in that case, you'd have a system, maybe we can draw one where you'd try to find local points of stability. So this point here would be maybe a local point of stability.357.0357.0 If you consider this as sort of an optimization landscape, because if you go from here, if you go a little bit away from it, you're always sort of pushed back. If this, if this is a, if the system is gravity, sort of, sort of.374.0374.0 So if these differential equations sort of describe that the height here is the force which with which you're pulled down, then this thing here would be a local point of stability.388.0388.0 The question is, if I give you a system that's described by these differential equations, can you tell me whether or not it is stable at some point?399.0399.0 Okay, and there is a spectral mapping theorem, which says, if you have the Jacobian matrix of f at this point, the matrix of its partial derivatives relative to its variable.411.0411.0 And if you take lambda to be the largest real part of its complex eigenvalues, if lambda is positive, then this is an unstable equilibrium.423.0423.0 If lambda is negative, then it's a locally stable equilibrium. So an unstable equilibrium, likewise, would be the point here on top, which means that if you're exactly at the point, then you stay there.437.0437.0 But as soon as you're a little bit off, you drift away from it. That would be an unstable equilibrium.444.0444.0 Okay, so there are complex steps involved in deriving the solution, then they list them out here, just to show you how complex this is, this is not meant to teach you.455.0455.0 You don't have to like understand or be able to apply this, this is simply meant to tell you how complicated it is to arrive at a solution.463.0463.0 So first, you need to differentiate each function with respect to each variable and obtain the formal Jacobian.469.0469.0 So they do this here for this example system, which is this system right here. This is a system of two equations, two differential equations in two variables.481.0481.0 Okay, so if you drive the Jacobian, that will give you a four entry Jacobian.488.0488.0 So each one of these is one of the equations derived with respect to one of the variables.494.0494.0 You can do that, right? But it requires fairly complex mathematical knowledge, like knowing that the derivative of the sine here is the cosine and knowing that this cosine doesn't matter for this particular entry, because it's in x two.509.0509.0 And here we drive by x one. So that's already very challenging.516.0516.0 Second, you need to evaluate the Jacobian at that point. So first, you've done it symbolically.524.0524.0 Now you actually need to put in the numbers at the point you're interested in, which will give you this thing right here, which is a numerical matrix, whereas this was a symbolical matrix.535.0535.0 Then you need to calculate the eigenvalues, which is mean, you have several methods of that. You have several methods of computing the largest eigenvalue, you could do power method, you could do decomposition.551.0551.0 There, there are numerous ways, but none of these is like particularly easy, right?558.0558.0 And then lastly, you need to return the minus max of the real part, which is the speed of convergence of the system.567.0567.0 So not only do you need to be able to tell whether it is stable, which is if this is negative, you also need to be able to say, or if this is negative, sorry, you need to be able to say whether it is locally stable or locally unstable.586.0586.0 And since this is larger than zero, it's locally stable. And this would be the decay rate 0.441.596.0596.0 This is what you're asking this model to output, right? Now we'll quickly go over the other things, but not as as in depth.606.0606.0 But this is in control theory, where you're trying, you have almost the same thing, you have a differential equation. But now, in addition to these variables, you have these control variables, which you have power over and you trying to decide, can I control the system with the appropriate function?624.0624.0 And in order to do that, that's kind of the problem that we had at the beginning, I know it's not.631.0631.0 Oh, yeah, it is. So what you need to do is, again, differentiate the system with respect to its internal variables, differentiate the system with respect to its control variables, evaluate A and B, calculate the controllability matrix with the with one of the functions above, calculate the rank, and optionally, evaluate this equation number three that we saw before.656.0656.0 Now, the last task is equally complicated. It relates to the stability of partial differential equations using the Fourier transform. And again, to obtain this, it is a five step, intricate process, where that is a mix of symbolic, complex manipulation, and numerical evaluation of that symbolic things.680.0680.0 And here, you need to simply output two bits, one bit says whether there exists a solution, and the other bit says whether it vanishes at t to infinity.694.0694.0 So what are they expecting here? What they're doing is they're going to build a data set that is composed of these things. And I think they do it one by one.704.0704.0 So they take one of the tasks, and they're going to build a giant data set of these things, since they all have solutions, right? You can build a data set with labels, because you can actually build a you can build software that does these steps, because you program mathematical knowledge into the software, but it's custom made for that particular problem.726.0726.0 And they're simply trying to, they're not trying to beat that program, they're simply trying to investigate, can a language model that knows nothing of math, do this simply by learning from data.740.0740.0 So they're going to try to build a data set, or they are building a data set. And they do it in the same way as this previous paper, which say we generate random functions by sampling unary binary trees and randomly selecting operators, variables and integers for their internal nodes and leaves.760.0760.0 So they use any combination of plus minus times divide x blog, and so on. So all these functions can appear in these things. And they basically they build a tree where they say, Okay, we go from plus, and then here is five, and then here maybe is minus sin of x, and here over is x of y.784.0784.0 So that would be five plus sign of x minus x of y. So they build trees like this by randomly sampling. And then they simply feed this to their mathematical program to obtain a solution, y.803.0803.0 And then they feed all of this into that's now a training data point. This here is x, and here you have y. And they generate a giant bunch of these things. And they feed them into the into the model.819.0819.0 They say, here is a seek to seek models, not even sure what kind of models they use. I think they just they use transformers as well. So they use standard transformers, I believe. So yes, in all experiment, we use the transformer architecture with eight attention heads, we train our models with the atom optimizer learning rate, blah, blah, blah, we vary the dimension, and the number of layers.844.0844.0 So that's going to be interesting to see how the size of this language model influences how well this model can solve these things. So as you can see here, they build these data sets for local stability, they include systems with two to six equations, which is already fairly, fairly complicated, and would take a human quite a while to do this.868.0868.0 They say we generate a data set with over 50 million systems. So it's a pretty dense sampling of this space. And I feel this is one of the important components here.880.0880.0 They do make sure that none of their tests, so they do a train test split, and they do make sure that none of their test examples is in the training data set, though they claim that they never actually have to remove anything, they just check and the search space, the space of these trees here is so large, that it never happens that the that a test sample or almost never happens that a test sample is in the training data set.908.0908.0 Alright, so they generate these things. And here are the results. So for local stability, it is trained to predict this lambda that we saw at the beginning, the largest part of the eigenvalue of the Jacobian, corresponding to the convergence speed of equilibrium, we consider that predictions are correct when they fall within 10% of the ground truth.934.0934.0 And here you can see that their best model achieves 96% if the degree of if it's two equations. So if the degree of the system is two, it achieves in 96% accuracy. So in 96% of the time, the convergence speed is within 10% of the true convergence speed.956.0956.0 That's fairly crazy, right? That's pretty good.960.0960.0 And here, the exact prediction of local convergence speed to a given precision. So how many digits actually match of that conversion speed, so it's not only 10% off, they also measure how many digits match.976.0976.0 And here, you can see that as you up the degree of equation, sorry, here, the performance drops off, as you can see, less and less and less. And also as you lower the number of layers in your model, or lower the dimensionality, the performance drops quite significantly.1000.01000.0 So that means the language model sort of is doing real work here.1007.01007.0 And here you also see that if it's degree two, the convergence speed has pretty even goes to two, three or four digits often. But as you now increase the degree, this accuracy drops off fairly quickly.1026.01026.0 Alright, let's keep that in mind. So the surprising thing is that it actually works. And it works in surprisingly big amount of the time.1036.01036.0 Now, I don't know, you could bicker about this 10% and how bad or how easy this is, and so on. But it is fairly, it is fairly complicated problem. And to be within 10% of the solution seems quite remarkable.1052.01052.0 And the same things here happen with the other tasks. So here they say, they predict controllability in the autonomous case. So in the control theory, they predict these two things, whether it's controllable, and then they output this K matrix that we saw before.1074.01074.0 Yeah. So here, you can see that if they have high enough dimensions and high enough layers with sample systems with three to six equations, they achieve again a 97% in the prediction of whether the system is controllable or not.1094.01094.0 Now remember, this is a binary prediction, but still, it requires a good understanding of math for a human to solve this. Again, we see this drop off with dimension and layers.1111.01111.0 But you know, the these, this, this number here is pretty good compared to the 50% you'd have for you to compare to the 50% you'd have from random guessing.1126.01126.0 Also interesting is when they look at is this correct, this correct, sorry, is the feedback matrix correct. So this matrix that you optionally have to output, they find that if they analyze whether or not that's within 10% of the true one, they see that pretty, pretty quickly this accuracy drops when they up the degree.1150.01150.0 Of course, the matrix, as I understand it has more entries at degree six than at degree three. So maybe that's understandable. But it drops off pretty quickly.1163.01163.0 But what is true is whether they call this correct feedback matrix. So from the feedback matrix from the entries in that you can read out whether it's the system is controllable or not, if all of the eigenvalues, I believe, are negative or positive, or all the values are negative or positive.1184.01184.0 So basically, by saying whether or not these things are positive or negative, you can read out the controllability. And if they check whether that property holds, then that is, is fairly well.1197.01197.0 So they argue here that this shows that it doesn't, it doesn't predict the the matrix they want. But the matrix that it predicts has the appropriate properties to solve this other task right here.1213.01213.0 Okay, that's experiment two. Experiment three, as you can imagine, is quite different, sorry, quite similar in that they investigate these partial differential equations.1226.01226.0 In the Fourier transform, the model is given differential operator and an initial condition is trying to predict if a solution exists, and if so, whether it converges to zero when t goes to infinity.1240.01240.0 The space, the dimensions between two and six. So the random guessing here would be I guess 25%. Because it's two bits, you need to output and this model performs extremely well, even up to this dimension six, there is a drop off with dimension, but it still does perform very, very well.1261.01261.0 Now they go into the discussion a bit. And they and this, this is the part that in this paper, interests me, like how do you interpret these results, apparently, you give these mathematical things to a language model that has no clue of math.1278.01278.0 And just by looking at examples, it learns to produce correct solutions. And if you want to teach that to a human, the human would have to go through all these steps, right? So something is happening here. And we'll want to find out what and the discussion is maybe they try to explain a bit why they think this happens.1300.01300.0 They say, we studied five problems of advanced mathematics from widely researched about it. In three of them, we predict qualitative and theoretical features. In two, we perform numerical computations. According to mathematical theory, solving these problems requires a combination of advanced techniques, symbolic and numerical that seem unlikely to be learnable from examples.1323.01323.0 Yet our model achieves more than 95% accuracy on all qualitative tasks, and between 65 and 85 on numerical computations, such high performances over difficult mathematical tasks may come as a surprise.1339.01339.0 One way to generate data set of problems with their solutions consists in sampling the solution first, and deriving an associated problem. For instance, pairs of functions with their integrals can be generated by sampling and differentiating from random functions.1356.01356.0 So here they hedge against there's this criticism. And this was mainly a criticism of their other paper, which they already addressed in their other paper was if you want to find out if you want to create a data set where you have the function, and then the label is the integral of the function, then there is no common solution to derive these integrals.1380.01380.0 Sorry, the derive is a, there is no common solution to integrate functions. I mean, you can do it numerically, but there is no common symbolic solution to integrate any function. And that's why what you can do if you want to produce a data set, you start with the integral already.1398.01398.0 And then you differentiate that to get to get a thing. And then you know that if you integrate this, you should get back your original function. But this biases the data set because the sampling is now not over these functions, but the sampling is over these functions.1419.01419.0 And that might lead to this distribution here being biased. So they hedge against that, which I don't care because it clearly in this paper, and they say in this paper, data sets for all considered tasks are generated using a forward approach by directly sampling as a result, potential bias caused by backward generative model do not apply here.1441.01441.0 And they studied problems from three to so they hedge against this argument that they could have a biased data set, which I don't think anyone reading this paper would leverage against them.1452.01452.0 Yeah, so1455.01455.0 in.1456.01456.0 So here, they basically say how good they are how surprising this is all of this requires math. This part is irrelevant because it hedges against an argument that I don't think is reasonable against the paper. And then the last thing in their discussion is an objection traditionally raised is that the model might memorize a very large number of cases and interpolate between them, which I think we know in language model happens often.1482.01482.0 Right. Oh, by the way, have I shown you how they encode this into the language model? I have not.1488.01488.0 This is the I guess this is the craziest part. They don't even put the numbers there.1495.01495.0 Wait, wait, they don't even put the numbers there. They actually put the as I understand it, they put the string tokens here. Right. So they put the string tokens of the math, and then even like compose.1511.01511.0 So that number 142, they would put as now there's an integer and then the token one, the token four and the token two. Okay.1522.01522.0 And the decimal point representation is the sequence float three dot one, four E in negative one. So this is it's really just a string, there is no, like the model would even have to learn the decimal representation of numbers to get that this four here is actually1546.01546.0 not not just a different token than two, but it's 20 times larger because it is in the position one in front of two. So it's not two times larger, like four is the two.1557.01557.0 But because four is, you know, one digit away from two, it's 20 times larger. And then this here is actually 50 times larger than this. So it seems like a quite inconvenient way to input data into the model. And yet the model is super accurate, right.1573.01573.0 And we already know that these language models, what they tend to do is they tend to memorize the training data or abstracted in a way that they can sort of interpolate between fuzzy versions of the training data.1587.01587.0 Here they say this is unlikely, sorry, this is unlikely because first because the size of our problem space is too large to be memorized.1597.01597.0 So say for all considered problems, we did not get a single duplicate over 50 million generated examples. Second, because in some of our problems, such as non autonomous control, even a model with a one layer and 64 dimensions obtains a high accuracy and such a small model would never be able to memorize that many examples, which is true, right?1620.01620.0 So this is, this is a fair defense against you're just interpolating training data. But I think the kind of broader, the broader scope of this criticism would be something like your model is just kind of learning the pattern regularities of the textual data that you feed in.1640.01640.0 It's not actually learning math, it's just learning sort of, okay, there is like a cosine. And if there's a cosine here, followed by an exponential function, that often leads to like a very low number of this lambda, right.1655.01655.0 And then if a very similar thing comes, if it comes across a very similar thing in the test sample, even though it's not exactly the same thing, it will map it to like a similar place in the label space. I mean, this is literally machine learning. This is literally regression.1671.01671.0 But I think the more the broader scope of this criticism is that what your model might be doing might simply be sort of a very simple regression on these tokens or on the these context dependent tokens, rather than this internal mathematical reasoning.1693.01693.0 And I don't, while it is true that it's probably not memorizing any examples, this still doesn't. And while it is also true that they did not get a single exact duplicate, what would be interesting to know is how many, like approximate duplicates.1714.01714.0 So can you basically solve the problem with a nearest neighbor approach? That would be my question. Can you solve the problem with a nearest neighbor approach over their training data set?1725.01725.0 Because that means you basically don't need the mathematical knowledge.1730.01730.0 They say third, because for some of our problems, we know from mathematical theory that solutions, i.g. the real value of eigenvalues cannot be obtained by simple interpolation. And I mean, that is also a valid defense.1746.01746.0 But I think the argument goes further than just simple interpolation. What we mean by interpolation is not we interpolate the real values of the eigenvalues. What we mean by interpolation is sort of interpolation in the regression space of these tokens.1763.01763.0 Like we know that if we go from a sine to a cosine, maybe the sine of the output flips at the end. And that's what we mean by interpolation.1776.01776.0 Like when we see two equations that are very similar, like x squared plus 4x minus the sine of x, and then we see x squared plus 5x minus twice the sine of x.1796.01796.0 What we mean by interpolation is that we now get a test example that says x squared plus, let's say, 4x minus 3 times the sine of x.1810.01810.0 Then what we would interpolate is sort of these things. Or yeah, I'm making a bad example right here.1819.01819.0 Maybe I should go with x squared and this is x third. I know these things aren't exactly equal. But this in the middle would be sort of an interpolation in token space.1834.01834.0 And if you train the language model, it will recognize that maybe I can interpolate whenever the coefficient here is just different or I can interpolate when there's just, you know, if there's like a log x here, that doesn't really change anything.1848.01848.0 So I can interpolate between the two, but I might not be able to interpolate when the exponent here is different.1854.01854.0 So if you give a training data set, you teach the model where it can interpolate and where it can't.1860.01860.0 Now, again, it's not able to remember the training data, but it will be able to sort of abstract it and store it fuzzily and abstract the patterns from it, which is good, right?1871.01871.0 That's machine learning. But there's no evidence here that this does any mathematical reasoning.1876.01876.0 So up until now, all that has built up is sort of if you read the abstract, can advanced mathematical computations be learned from examples?1890.01890.0 Neural networks can learn advanced theorems, complex computation without built in mathematical knowledge.1897.01897.0 All of the story here, all of this, this showing of, hey, look at what all what steps is required to solve these problems.1907.01907.0 And even this discussion here basically says, hey, you need mathematical complex reasoning to arrive at the solutions.1917.01917.0 And then in the conclusion, in the conclusion, they say, it seems that our models have learned to solve these problems.1929.01929.0 But that does not mean they learn these techniques we use to compute their solutions.1934.01934.0 Problems such as non autonomous control involve long and complex chain of computations, yet even small models, that means one layer transformers with 64 dimensions achieve high accuracy.1944.01944.0 Most probably, our models learn shortcuts that allow them to solve specific problems without having to learn or understand their theoretical background.1954.01954.0 Such a situation is common in everyday life. Yada, yada, yada.1960.01960.0 So here in this in this paragraph here, they sort of counter their whole narrative of the paper.1968.01968.0 And that's, I guess, that's sort of to it's fair, right? They criticize their own work, which is good for research.1974.01974.0 It's also to hedge against criticism. And it's to be a bit real. This it's a it's a good paper, right?1981.01981.0 Because it's a nice and interesting story. And then at the end, you also say, look, this might actually not be all that what it's made up or what it seems like to be.1992.01992.0 And I, I agree with this statement right here. It's that probably the model learns shortcuts, and the shortcuts might be just in a way of pattern matching, the pattern matching, whatever patterns you extract from the training data, you pattern match that and you relatively simply interpolate between those matched patterns, not between the training data itself, but between the match patterns.2017.02017.0 And therefore, you can arrive at approximately good solution. So what I would have liked to see from such a paper, right, they say that we leave that to future research, after making really kind of big claims in the introduction and the abstract.2032.02032.0 They have taken three different problems here, right? There's this local, this local stability, then there is this, this control theory.2044.02044.0 And then there is this stability, there are three different problems. And okay, they try to show that they can apply this to a diverse range.2052.02052.0 But what I would have expected from a paper like this is they even they even spell out, here are four things that you need to do to solve this if we were to teach this to a human, right?2067.02067.0 Now, if you have trained a model, and you evaluated it, it is really good at this task for which you thought you need, you know, to do these four steps.2077.02077.0 What would be really interesting is to now introspect your model and see can, can I somehow show that my model has in somewhere in the intermediate layers has this quantity right here, and is not just nearest neighboring in some learned pattern space.2097.02097.0 That would be an actually interesting research question, right? So rather, in my mind, rather than having three different things where they all you know, they demonstrate the same thing over and over and over again, that this actually works.2110.02110.0 It would be a much more interesting question to introspect the model and parse out can cannot, for example, you can see can I reconstruct this quantity from the inside of the model, when the model isn't specifically trained to give me back this quantity,2126.02126.0 I know this quantity would be a step on these on the path of the solution, right? If I want to get the solution, I almost have to calculate this quantity.2137.02137.0 Can I parse this out from the middle of the model somewhere, when the model isn't explicitly trained to give me this, if I can, then I can really make the point that the model does something like this and learn something like this from data.2152.02152.0 If I can't, that will be more of an evidence that the model is simply sort of pattern matching close enough seen examples in the training data.2162.02162.0 So that's a bit of my of my criticism right here is that they, they show it works, which is pretty cool.2169.02169.0 But then they, they don't do the sort of interesting experiments of these of this introspection right here, which is a bit sad, but you know, they leave it for future research, which I guess is going to be themselves.2185.02185.0 And that's how you make two papers.2189.02189.0 So now I don't want to be too critical. It's a very cool paper. And I invite you to check it out and leave a like and subscribe and leave a comment of what you think of this kind of research of this paper, and whether or not you think I'm totally wrong.2205.02205.0 That's entirely possible. Okay, I'll see you next time. Bye bye.2223.0\"}"}