{"data": "{\"value\":\"0.88 Hi there, today we're looking at pre trained transformers as universal computation engines7.127.12 by Kevin Lu, Adita Grover, Pieter Abbeel and Igor Mordach. On a high level, this paper argues that14.5614.56 pre trained transformers, specifically transformers pre trained on language modeling, are doing21.221.2 something called universal computation. And the way they prove it is by transfer learning these29.5229.52 transformers to completely new domains. So not language modeling, they do things like x or tasks37.8437.84 or c for 10. So computer vision, they transfer learn these transformers to these completely44.0844.08 new domains, and they don't just do it in a regular transfer learning way, they freeze almost50.0850.08 all of the parameters of that transformers, specifically, they freeze all of the attention55.255.2 and all of the feed forward layers in the transformer. Therefore, they only fine tune about60.060.56 point 01% or so or point 1% of the parameters of the model. And they show that on these specific68.5668.56 tasks, these frozen pre trained transformers, as you can see right here, are competitive,74.9600000000000174.96000000000001 if not outperforming a transformer that is fully trained from scratch on these tasks. And it also82.8000000000000182.8 mostly outperforms LSTMs that are fully trained from scratch on these tasks. So this is pretty89.9289.92 interesting. And it gives rise to a number of sort of questions about what happens in these96.2496.24 transformers. So we're going to look at what the claims are, and what the let's the evidence brought102.72102.72 forth by this paper is about why language pre trained transformers are universal computation109.52109.52 engines. And yeah, I'll have some comments on my own, as always, if you do like content like this,116.64116.64 share it out, leave a like and tell me what you think is going on here in the comments.121.75999999999999122.39999999999999 Right, so the abstract reads, we investigate the capability of transformer pre trained on natural128.8128.8 language to generalize to other modalities with minimal fine tuning. And they say in particular134.64134.64 without fine tuning of the self attention and feed forward layers of the residual blocks. So as you141.27999999999997141.27999999999997 know, or as you might know, a transformer is built approximately like this. So what you have is you148.72148.72 have input, so you have the positional embeddings, and you have the input embeddings. Now, if it is155.2155.2 a language model, that is simply one vector for every word or word piece. If it is an image model,162.0162.0 like in the vision transformer in the VIP, it is you simply take the image and you make it into170.56170.56 these patches. And then each patch patch, you simply unroll the patch into one long vector.178.08178.08 So you simply unroll the pixels. And that is a patch and that in the sequence of such patches is184.88184.88 your inputs. Now, what follows is these self attention blocks. And this is the majority of192.79999999999998192.79999999999998 the transformer is L times the self attention blocks, you always have a attention layer. And200.56200.56 if you if you don't know what an attention layer is, I'm sure you'll find some video on YouTube206.0206.0 that explains it. This is followed by layer norm. This is followed by a element wise feed forward214.8214.8 layer. And it is again followed by a layer norm, you also have the residual connections, as you221.60000000000002221.60000000000002 can see right here. And then all of this is followed by an output layer. And the output228.48000000000002228.48000000000002 layer is very task specific. In language modeling, it's obviously classifying into the vocabulary,235.28235.28 so into one of whatever the 30,000 possible continuations in computer vision, it might be241.68241.68 classifying into the classes of the data set. So for example, in image net, you'd have 1000 classes249.20000000000002249.20000000000002 or 21,000, depending on which, which version you use. So what they're saying is they are not256.40000000000003256.96000000000004 fine tuning, they are freezing the multi head attention. And they're also freezing the feed263.84000000000003263.84 forward layers. Now these make up like 99 some percent of the transformer. So what they get is272.32272.32 they get a frozen pre trained transformers and frozen specifically refers to these parts I marked278.32278.32 in blue. In fact, they just keep the attention and they keep the feed forward layers as they come out286.08286.08 of the of the language pre training, and then they train the things on different tasks. So293.35999999999996293.36 these tasks are as follows. There's bit memory, they consider a bit memory task where the model299.2299.2 is shown five bit strings, each of length 1000. Afterwards, the model is shown a masked version304.96000000000004304.96000000000004 of one of the bit strings, where each bit is masked with probability point five, and a model310.48310.48 is tasked with reproducing the original bit strings. So you give it you give it five bit316.40000000000003316.4 strings in sequence. And then you give it a sixth one that is kind of corrupted. And the model must323.44323.44 figure out which one of these five it is. And then it must successfully reproduce that bit string. So330.71999999999997330.71999999999997 if it figures out it's probably number three, the model has to look at the overlap between the335.35999999999996335.35999999999996 strings and then where there's the most overlap, it needs to copy over that string or the non342.56342.56 overlapping parts. So this is a fairly complicated task for a model like this that is just trained349.28000000000003349.28000000000003 with backprop, right? There is bit soar, where you have two bit strings of length five, and you need356.16356.16 to compute the element wise x or this is a long standing difficult task for neural networks,362.72362.72 we know that there is list ops where you get a sequence like this, and you must compute the368.0368.0 result. So it's acting a little bit like a calculator. So now it turns actually out that373.12373.68 if you think of the bit bit memory, that's already pretty similar to language, right? bit soar,379.2379.2 maybe not list ops, you were going to see that these models perform fairly poorly on the list ops386.56386.56 task. And then the last one is computer vision. So mnist and c410 is the classic like vision393.76393.76 transformer domain where but still they take the transformer that's pre trained on language400.8400.8 and simply fine tune the positional embeddings, the input embeddings, the output layer, and the407.52407.52 layer norm parameters. That's all they do. And the last one is c410 from the long range arena,413.59999999999997413.59999999999997 where instead of forming patches like this, in the long range arena task, you simply take every420.24420.24 single pixel into as its own kind of, so you don't do patches anymore, you do your own role pixel by428.48428.48 pixel, that is significantly longer vector for the model to to compute over. So it's going to436.0436.0 make the task a bit more difficult, because you completely lose all localization information.441.28000000000003442.08 And the last one is this remote homology detection. It's a task from protein folding.447.68447.68 Okay, so how do these? How do these things do you've already seen this here in the overview,454.88454.88 namely, if you train these things on these bit tasks, a bit memory or bit soar, you can see that462.56462.56 a if you the frozen transformer here reaches 100%, so does the full transformer. So what that shows471.04471.04 you, it's not necessarily which one's better, it's just that both are are able to completely476.16476.16 solve this task. Well, for example, an LSTM is not that we have no idea here what the size of484.32000000000005484.32000000000005 the LSTM is, I don't think they stated anywhere. So the comparison with an LSTM, it is cool to see492.32000000000005492.32000000000005 that the LSTM doesn't get this relatively simple task, but it also might just be a function of how498.56498.56 large the LSTM is and how much rigor goes into training one. Nevertheless, the LSTM can't solve506.56506.56 it. And that's because the LSTM takes in a sequence as just one at a time, and it needs to512.32512.32 sort of remember in its hidden state, what the individual elements are, and it can't go back,519.44519.44 right, the transformer, it can always look back, the LSTM needs to remember everything. And I think526.08526.08 that makes it much harder to do these kind of sequence tasks. I already told you list ops,531.6533.2800000000001 they all perform badly. But interestingly, they perform equally badly. So the full transformer539.5200000000001539.5200000000001 here is no better than the frozen transformer, which is very interesting. And if you look at547.2800000000001547.2800000000001 MNIST and CIFAR-10, actually all of the other tasks, you'll see that the frozen transformer is554.24554.24 not worse than the full transformer. In fact, it's sometimes better. And that is going to be558.5600000000001558.5600000000001 an interesting, an interesting thing also to look at. So the whole paper is actually just ablation565.04565.04 studies into this phenomenon, like why does this happen? And it's very cool. And the result is573.44573.44 going to be so the authors claim that there is something special about language pre training579.92579.92 that already primes the transformer to be receptive to these new tasks. Now, there are two different588.9599999999999589.52 possibilities if you if you think what's happening here. Actually, let's first go to the ablations and596.0596.0 do the discussion at the end. Because once you see what is happening, you'll, you'll be able to603.92603.92 form your own opinion. What I would like to record remind you though, is that they do,610.64611.4399999999999 they do train these layer norm, sorry, they do train the layer norm parameters, right. So when I620.4620.4 saw this, when I when I saw this, and they said, well, we only train the input embeddings, because625.36625.36 of course, it's a different modality. So adjusting the input embeddings makes sense, right, and the630.3199999999999630.32 position embeddings, maybe two, and the output layer, because we have a different task that makes635.2800000000001635.2800000000001 sense to and the rest, we freeze, but we also adjust the layer norm parameters, right. But we641.84641.84 don't adjust the attention. My immediate thought was you probably probably tried doing it without650.48650.48 the layer norm parameters at the beginning, they probably tried just adjusting input and output654.96654.96 embeddings. And that probably didn't work too well. And in the ablations, you're actually going to see660.48660.48 this. So and there, I think this hinges on the fact and we've seen this with transformers before,668.5600000000001668.5600000000001 I think they're called adapter layers. So if you have your kind of transformer layers, one after674.32674.32 another, what you can do is you can build in these adapter layers that have very few parameter that679.44679.44 are kind of compressing and uncompressing the data. And that's a way you can fine tune the686.72686.72 transformer. So this kind of goes in and out again in dimensionality. That is a way you can adapt.693.2800000000001693.2800000000001 And we know that these things are very possible with transformers that you can sort of have the700.32700.32 transformer ready and then only adjust very few parameters to transfer learn. And I think the same707.2800000000001707.28 is going on here. Now what the the authors sort of hint at is that in in the schematically, if you717.12717.12 have the transformer, you have the attention part, which is sort of the cross information routing723.6723.6 part, right. And then after that, you have the feed forward part, which is element wise like this.730.3199999999999730.32 And then you sort of have a layer norm part and the layer norm part. What it essentially is in736.96736.96 terms of learnable parameter is that you take one element here or even one channel or one layer and743.84743.84 this depends on the exact type of norm. But you in the input signal, you have two parameters that750.88750.88 you learn. So your output of the layer norm is going to be a normalized x. So this is a normalization757.36757.36 and you do it either over the batch or over the layer or something like this. In layer norm, you761.6800000000001761.6800000000001 do it over the layer, and you have two parameters that you can learn. One is a scaling, and one is767.36767.36 an offset. And I think, you know, by learning these, you can adapt and this is this is, I think775.84775.84 these two things have a lot of relation to each other, even though the authors say, we don't learn782.24782.24 any of the attention, I can by influencing this a and this B right here, and this y then goes into790.64790.64 the next layer of attention. I can very much influence how the attention works, right? If the797.92797.92 y is then in the next layer from the y, I construct the W, sorry, I construct the keys, queries and809.2809.2 values keep of this particular element, and that decides what information gets routed where and so817.36817.36 on. So I have very much an influence over the over the attention in the next layer. By adjusting825.0400000000001825.0400000000001 this a, I might not have a direct influence, like I can only if of course, if I want to change830.6400000000001830.6400000000001 something in an element in the key, an effect of this because I have to change the y, I can838.6400000000001838.64 change the y as a whole is going to be there also change something in here. But certainly843.4399999999999843.4399999999999 backprop will figure out some way I can make this happen. Okay, so I, I think this this whole notion852.16852.16 of we don't influence the attention at all. It's not as clear cut. It's true, they don't change the859.04859.04 attention parameters. However, they are very, they are able to influence how information is routed865.12865.12 by changing the signal itself in these layer norm parameters. Also, they here they call it zero shot,871.84872.72 they say, improves performance and compute efficiency on non language downstream tasks.877.76877.76 In particular, we find that such pre training enables the frozen pre transformers to generalize882.8882.8 in zero shot to these modalities, zero shot, I think that's a bit of an it's a bit of an over890.32890.32 claim. Like I get it use you pre train, whatever how many few percent, like only fine tuning898.24898.8000000000001 point 1% of the total number of parameters of the transformer model, and none of the self attention904.88904.88 parameters. I don't think it's entirely fair to call this zero shot on less I completely have912.4000000000001912.4000000000001 overseen and misread the paper, which of course is possible because I'm just one per person918.48918.48 reading a paper. Okay. So again, we fine tune the output layer, the input layer, the layer norm927.36927.36 parameters and the positional embeddings. My claim is this here does most of the work. Like we know933.6800000000001933.6800000000001 we already know that for example, for CNNs, we can do we can take a randomly initialized CNN and by942.48942.48 just adjusting the batch norm parameters, we can already gain a non trivial result. And I think the950.72950.72 layer norm here is doing a lot of the work, of course, the input and output layer as well,955.44955.44 we also know that we can take like a randomly initialized neural network and simply training960.0960.0 an output layer can already also give us a good performance. This is all stuff they do in this965.2965.2 paper. However, I think the layer norm does a lot of the a lot of the crucial work here,972.5600000000001973.5200000000001 too. But there are still some interesting things that come out of these experiments,977.5200000000001978.08 because it's not just that. Okay. So, as I said, the paper is a big piece of ablation studies.986.08986.08 Oh, yeah, that's what I forgot. The interesting thing, of course, is that the fully trained991.44991.44 transformer isn't better, right? That's the interesting thing, like if you fully train996.24996.24 a transformer on the same tasks, and this is due, I think, and I think the paper agrees,1002.16000000000011002.1600000000001 due to the fact that we are in sort of the low data regime, at least for the things here that1007.92000000000011007.9200000000001 are like the natural data sets, like MNIST or CIFAR-10, we don't have too many, we don't have1014.40000000000011014.4000000000001 too many data points. So training a big transformer with all the parameters could even be counter1020.721020.72 productive, because we're just going to overfit or shoot ourselves in the foot. All right, let's go1026.321026.32 through these experiments. Can pre trained language models transfer to different modalities?1031.36000000000011031.92 And the answer here is going to be yes, absolutely. So their base thing is like a GPT-21038.321039.3600000000001 model that is trained on language. And it's so interesting, right, that if you transfer it to1045.041045.04 these tasks, and you can see, right here, you compare it, so these are the results from figure1052.241052.24 one, this is just what you saw in the bar diagram. Again, it's pretty interesting that these fully,1059.21059.2 the frozen pre trained transformers match the performance of the full and outperform the LSTMs1066.481066.48 on these tasks. Pretty cool. So in some tasks, you can see right here in the homology, they even1072.321072.32 outperform the fully trained transformers. The second one, what is the importance of the pre1079.61079.6 training modality? So here, they're going to compare what if we just randomly initialize1084.961084.96 a transformer and then keep just keep we freeze the same layers, but they're not trained, they're1089.91999999999981089.9199999999998 randomly initialized, or we pre train it on this bit memory tasks, it's just this one task, or we1097.841097.84 pre train it on ImageNet, ImageNet 21k. In fact, we so we pre train instead of on language on images,1104.87999999999991105.4399999999998 or we pre train on languages, this is this FPT is pre trained on languages, which one is going to be1112.63999999999991112.6399999999999 the best. So this is to counter people, they're making the claim that language modeling has a1119.281119.28 specific, specific property that language is sort of a good task to pre train these transformers1128.63999999999991128.6399999999999 better than other modalities. So you can't just pre train the transformer on any old task. That's1133.761133.76 what they're saying here, that language is somehow special, or the best out of these ones. So in1140.161140.16 order to demonstrate that you can see right here, the this is the language one, the randomly1145.841145.84 initialized one already kind of underperforms throughout here. So actually not that much1153.121153.12 in these things here. But you can see on MNIST or on C410, it it does not perform too well all across1160.561161.12 the bit memory one obviously performs well in the bit memory task that was pre trained on.1168.01168.0 But also it kind of sucks on the rest of these tasks. It's okay. In MNIST, it's the performance1174.87999999999991174.88 is kind of shaky, and the vision transformer is better. But it still lags behind except on C410.1184.241184.96 Because, you know, being pre trained as a vision model might, you know, it seems like it's okay1191.84000000000011191.8400000000001 that it performs well on image modeling. The whole point here, though, is to generalize to1198.56000000000021198.56 two domains out of your pre training thing. And on these domains, the language one is better than1207.91999999999981207.9199999999998 all the other ones. Now, the question, there is multiple questions here. I think it is a bit too1215.61215.6 early from just this paper to say that language modeling has this special property, right? What1222.321222.32 think might also be an explanation is, for example, how difficult is your pre training task? Now, when1229.21229.2 you look at language modeling, you can look at simply how many classes does it have. So the1233.91999999999981233.9199999999998 number of classes is in language modeling, something like 30k, like these vocabularies are fairly large,1240.961240.96 random, it's absolutely nothing. These bit memory tasks is so you have two classes.1249.041249.04 And in the vision transformer, you have 21k classes, but you only need to apply it once1256.15999999999991256.1599999999999 per sequence, right? You only have to have one output. Whereas in language modeling, you need to1260.81260.8 output every single, so every single token is a classification. So in fact, the this is not1269.921269.92 necessarily more classes, but it is, let's say more training examples per training data point that1276.39999999999991276.4 you get because every token is a training example, essentially. So it might not be a language thing,1284.41284.4 it might just be how how hard the task is in terms of number of classes and how much training data1291.28000000000021291.2800000000002 you have available. I think there are a lot of variables that they haven't necessarily1296.80000000000021297.52 controlled for here. And it might be a bit too early to say language modeling is the task, though,1303.521303.52 what I'm completely prepared to accept is to say language modeling is a good task. In fact, it's1308.41308.4 the best task out of these ones. But I think the it could be a cool, it could be cool to research1317.441317.44 more in this direction and say, okay, can we find a better task? Can we find a task that is even more1322.87999999999991322.8799999999999 complex? And that depends on what is really going on here. So I see two possibilities, possibility1330.081330.08 one, why this even works is to say that somehow, natural signals are all somehow equal. So pre1342.561342.56 training on language somehow makes the transformer the attention layers just adjust themselves to the1350.63999999999991351.1999999999998 sort of natural signals that we see around us. So when we feed in an image recognition task or any1356.39999999999991356.4 other task that kind of humans care about in the natural world, the transformer is already sort of1361.84000000000011361.8400000000001 prepared about what that could entail like about the types of computation. And then second of all,1368.881370.24 and this, this is different, this is simply with enough complexity, you see, there is simply1377.761377.76 what I'm going to say, computational, computational utility, computational utility.1386.87999999999991388.8799999999999 What I mean by that is that there are simple when when you pre train on a task, certain types of1396.81396.8 computation are going to be important for that task. And the more complex and the bigger your1402.961402.96 model, the more sort of print computational primitives you can encode into the attention1409.84000000000011409.8400000000001 layers. Now, when you encode these computational primitives, it's not necessarily of course, it has1416.881416.88 something to do with the type of signal. But I think what's up, what could be happening is that1422.481422.48 these transformers, they simply they prepare a lot of good features that are just useful to compute1430.481430.48 different stuff like X or like remembering things, and so on. I think this could definitely be the1438.641438.64 case that in these attention layers, there are these just computational primitives encoded. And1443.921443.92 if you pre train on a task, and the harder the task is, the more of these primitives need to be1449.12000000000011449.6 encoded. And what you do when you adjust the layers in between is simply that you1456.881456.88 recombine these primitives in a better way. But sort of all of the computational primitives are1463.21463.2 already there. I think I think the two are not necessarily even exclusive. And I think the paper1468.481468.48 hints at both might be playing a role right here. I don't think they say exactly the same thing.1475.921476.72 But this would also give sort of meaning to this word of computation or universal computation engine.1483.04000000000021483.04 There is that that these transformers, and we might even extend that to probably any machine1488.87999999999991488.8799999999999 learning model, if we could scale it up and train it correctly, probably evolves or trains to have1496.321496.32 these computational primitives inside of it. And that's why we can adjust it with just a little bit.1502.561502.56 Now they're going to claim there is something about language pre training later on in the1510.63999999999991510.64 first part of the course. So first of all, they say how important is the transformer architecture.1517.36000000000011518.0 And here they simply say, if we take a randomly initialized transformer, and compare it with a1523.761523.76 randomly initialized LSTM, we freeze, we freeze the attention layers, and then we just do our1529.761529.76 frozen training, then the transformer performs a lot better than the LSTM here in most actually1536.961536.96 all of the tasks. However, this is a very shaky comparison, of course, because how do you fairly1543.21543.2 compare a transformer architectures within LSTM architectures? Do you control number of parameters1548.41548.4 number of computation? Speed? I don't know. Okay, so I don't know what's fair. Next, does language1557.681557.68 pre training improve efficiency over random initialization? The answer is yes, it converges1563.521563.52 much faster if you pre train with language. And do the frozen attention layers attend to modality1571.841571.84 specific tokens. So here, they're just going to look at the first attention layer. And they see1578.01578.0 that the attention matrix, for example, in this bit XOR task attends. So here are the two, here1584.561584.56 are the two, this is string number one, this is string number two. And in the output from here,1589.521589.52 you need to compute the, the XOR, you can see that the attention first is it's on the, on the first1597.61597.6 one, and then it's also on the second one, right in the output, it always looks at the corresponding1603.441603.44 position. So here, you can see clearly that the attention matrix already attends to the correct1610.87999999999991610.8799999999999 things for the task, which is cool, because we've never trained the attention, right. But it's I1616.481616.48 think that goes into my claim that, look, we are still able to influence the attention matrix,1623.281623.28 even though we don't train the attention weights, we are able to influence it by training these in1628.481628.48 between parameters. The same goes for these bit memory tasks, you can see the attention matrices1634.481634.48 are very much attuned to the task right here. Next one, does freezing the transformer prevent1644.161644.16 overfitting or underfitting? And here they, they train this frozen transformer, and they compare it1652.41652.4 to training a transformer that just has three layers. So they say, our general finding is that1661.12000000000011661.1200000000001 in contrast to their fully trained counterparts, FPT models underfit the data, which lends them1667.281667.28 to further improvements by increasing model capacity. So if you compare it to a three layer1674.241674.24 transformer, the three layer transformer does outperform the 12 layer frozen transformer.1682.721683.44 However, it does so by reaching a much higher training accuracy. So overfitting is much more1689.21689.2 of a problem if you fully train the transformer. However, if you use this frozen transformer,1694.321694.32 you're probably underfitting, as you can see right here. So you could technically1700.15999999999991700.1599999999999 scale up and gain more power with this frozen fine tuning.1706.241708.0 Does performance scale with model size? Yes. So you can see as you increase from small to medium1716.01716.0 to large as you increase the number of layers, the performance increases. However, the performance1722.15999999999991722.16 also increases for a randomly initialized one. So it just seems to be like, the more parameters,1727.681727.68 the better it's the same. And here is something I find interesting. Can performance be attributed1733.681733.68 simply to better statistics for initializations? Here, they're going to, let's say make the point1738.881738.88 that there is something about language model pre training that actually makes the transformer1744.641744.64 conducive to all these tasks. And you can't just reach that by better initialization, which is more1753.681753.68 point one from here than point two, because point two, you could just reach by initializing in a1760.881760.88 better way. Like these, we could, we could characterize these computational primitives.1766.241767.5200000000002 And we could build them in from the start, whereas natural signals, we can't characterize them,1772.961772.96 otherwise, we wouldn't need machine learning. So what they're going to do is they're simply going1778.241778.24 to take a fully trained transformer, which they call an Oracle. And then they they're going to1785.441785.44 compute the mean and the standard deviation, so that the Gaussian from those, and then they're1792.01792.0 going to initialize this new transformer. So they're going to take the pre trained, which1799.60000000000011799.6 they have, they're going to do default, which is the randomly initialized one, we've already seen1805.841805.84 those one as well. And then they're going to take a randomly initialized one, but not randomly with1812.15999999999991812.1599999999999 a default randomization, but randomly with the statistics they got from the Oracle. So this1818.561818.56 transformer is going to be randomly initialized, but it has the same statistics as the as the full1826.081826.08 transformer, or as a trained transformer. So the statistics are correct. And that does not seem it1832.63999999999991832.6399999999999 seems to help a little bit, as you can see, but it does not seem to help. In fact, here it even it1839.361839.36 even hurts. However, I think that's a bit of a weak experiment. And I think there is still a1844.87999999999991844.8799999999999 possibility that we could initialize these transformers much better if we could, if we could1851.761851.76 correctly capture the essence of these computational primitives that are there in that are learned by1860.081860.08 gradient descent, I think if we can capture those in a theoretically sound way, we might be able to1866.721866.72 initialize or if we could just Yeah, if we could find like a not a natural language, but if we1873.521873.52 could find a synthetic pre training task that is just so hard, but it completely initializes all1881.041881.04 of these computational primitives, that might still be better. And that's going to be the1885.521885.52 ultimate experiment that differentiates between option one, natural language pre training is1890.87999999999991890.8799999999999 somehow important because of grammar and natural signals, or option two, what we're doing is just1896.561897.12 inputting computational primitives into these layers. Does fine tuning self attention and1903.841903.84 feedforward layers further improve performance? And the answer is actually no, it degrades,1909.121909.12 you can see right here, this is worse than this. And that's because probably of overfitting,1916.561916.56 if you fine tune the whole transformer, you're going to fall down. And now here is where it1923.361923.36 really comes in that, you know, these tasks, they are in the low data regime. I know, if you go back1929.67999999999981929.6799999999998 five years, that sounds ridiculous. But right now they are these things will overfit if you train1936.01936.0 everything. And here it comes, which parameters of the model are important to fine tune. And you1942.561942.56 can go look at the you can go look at the look at the table, it's in the appendix, but they say1950.41954.0 in particular, we find orthogonal initialization, wait, we run ablations.1959.121959.12 Here, we generally find the layer norm parameters to be most important, the layer norm parameters,1966.95999999999981966.9599999999998 right. And that sort of gives, it gives a gives credence to the fact this is not. So the, I think1976.23999999999981976.2399999999998 what what they're doing, yeah, these layer norms, they carry a lot of the weight of these things1982.95999999999981982.9599999999998 right here. It's still pretty cool, because they're very, very, very, very, very, very, very,1989.041989.04 very few parameters that you need to fine tune. And, okay, now they do a bunch of more ablations,1997.041997.04 like only training the output layer, which gives non trivial performance, but not a good enough2002.242002.8799999999999 performance. So, and yeah, for some reason, I have another set of the paper right here. But this was2011.62011.6 essentially the paper, it's very cool. And the paper is super, I think it's well written, and2017.842017.84 it's easy to read, because it's like, hey, here's a phenomenon we've discovered. And now we're just2023.282023.28 going to investigate all kinds of things that explain this phenomenon, we're going to rule out2028.82029.4399999999998 some stuff, some hypotheses, and we're going to arrive at some kind of conclusion in here.2034.482035.36 And yeah, that was my two cents to this paper. I hope you enjoyed it. It's a bit of a shorter video2040.63999999999992040.64 and bye bye.2048.6400000000003\"}"}