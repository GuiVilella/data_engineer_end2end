{"data": "{\"value\":\"0.0 If we just do behavior cloning using this data, you know, won't cut it like we don't have enough data.6.06.0 Hello there. Today we're going to look at this right here. This is an agent in Minecraft that's trying to build a waterfall.19.019.0 So the goal is to go up a mountain, find a good spot, put down some water, turn around and then take a beautiful picture of the waterfall.29.029.0 That is one of the four tasks of the mineRL Basalt competition. This is what we're going to talk about today.38.038.0 And not only are we going to talk about the challenge, the competition, as you can see, make waterfall is one of the four subtasks.45.045.0 We're actually going to talk to the winning team, to the Kairos team in just a second.52.052.0 This is just the intro. I want to tell you a little bit about what's going on.56.056.0 So that later in the interview with the authors, you can follow if you don't know what Minecraft is or sort of the basics of these competitions.65.065.0 If you do, feel free to skip ahead. This is just going to take five to 10 minutes right here.71.071.0 So I want to show you another one to give you a little bit of the impression of what these agents can do.78.078.0 I haven't actually looked at many of them. I don't know what's going to happen right here, whether that's successful or not.85.085.0 These are the actual videos that the judges saw that that were part of these competitions.93.093.0 So the competition is human judged. There's no reward function.98.098.0 It's literally you just give 10 videos to a human and they're supposed to rate how good these things are, how human like they are and so on.107.0107.0 Ah, it missed the waterfall a little bit right there. Let's see whether I can turn around.112.0112.0 Yeah, it can not spot on, as you can imagine, and not spot on in any of the 10 things, but good enough to win this competition.123.0123.0 So how did this team go about this? If you don't know what Minecraft is, Minecraft is this game that it looks like, you know, it's it looks like it's from 1990 or so.134.0134.0 Everything is made of blocks, but it is a really cool game. It's a completely open world game.139.0139.0 You can do anything and everything. You can craft items. All of these blocks you can destroy and build up somewhere else.147.0147.0 You can collect items and craft new, better items from it. For example, you can craft a pickaxe with which you can mine things, mine stone.156.0156.0 From that you can build like an oven, a smelter and smelt iron ore. From that you can build iron tools and so on.164.0164.0 This world is completely procedurally generated. So there is there's no the level is never the same.172.0172.0 And that's one of the things that makes these challenges so hard. And the other thing is just the sheer amount of freedom that you have right here.182.0182.0 So the agent now has spent quite a bit of time looking for a good place to build the waterfall.187.0187.0 It looks like it got stuck right here. That must that that's kind of one of the failure cases I imagine. Or it's going to get out.198.0198.0 It's going to get out. What what a what a clench, clench, clench play there.204.0204.0 It looks like here it's a good spot for waterfall. Yes. Put it down. Walk away from it. Turn around. Snap picture with the sheep in it.214.0214.0 Beautiful. So this has actually led to a paper as well by the winning team called combining learning from human feedback and knowledge engineering to solve hierarchical tasks in Minecraft along with open source code that you can check out.232.0232.0 So you can retrain their agent. You can look at their code and you can improve it. It's MIT licensed. Therefore, you know, all good to go for you.243.0243.0 So what did this team do that gave them the winning submission, the challenge in itself is you're given the tasks in just a short string.254.0254.0 So there's not a reward function or anything like this. The short string literally is, for example, the find cave, it's the agent should search for a cave and terminate the episode when it is inside one.267.0267.0 That is the entire description of the task. As I said, no reward functions. You do get 40 to 80, I believe playthroughs 40 to 80 human demonstrations for each task.281.0281.0 Not all of them completing the task though, and a bit of a code base. And that's it.287.0287.0 This team came up with the following solution they built at the core, they built what they call a state machine, but I want to start somewhere else I want to start from how they used the human demonstrations.300.0300.0 So they had humans and demonstrations of humans solving this task. And then they trained a navigation policy. This is trained via behavior cloning.310.0310.0 So you try to make an agent that just kind of clones the human movements. They did cut out all of the sort of interacting with the environment things from the human demonstrations, such that it was just only navigation going from point A to point B.328.0328.0 This is a policy that they can activate at any time. So as you can see right here, this gives rise to these two one of what they call learned or engineered subtasks.340.0340.0 So you they have a stack of these subtasks. One of them is this navigation subtask that is obviously learned. They have other ones that are just hard coded.349.0349.0 For example, when it's time to actually place the waterfall at that point when you think you're at a good point to build a waterfall, this movement of stacking up the blocks and then putting the waterfall on top, that is a hard coded policy.364.0364.0 So these subtasks are hard coded partially and partially learned, and they're controlled by this state machine.372.0372.0 On top of that state machine, which we're going to get to in a minute, the state machine itself is controlled by this state classifier.381.0381.0 So the state classifier is a thing that they came up with.388.0388.0 They take pictures from the game, frames from the game, and they collect additional human labeled data where for each picture they let the humans label, for example, is this inside a cave, which you can see right here, that's inside a cave.402.0402.0 If you play Minecraft, you'd know. Is there danger ahead, which means kind of a large body of water that you should avoid or something like this?410.0410.0 Do you have animals, which is relevant for some of the tasks? So they build up the state classifier, which is also learned, and that state classifier is now going to control this state machine.421.0421.0 I'm not sure if they actually have it somewhere for one of the tasks in the paper. They do have it in the accompanying presentation.430.0430.0 The state machine controls what the age or which sub policy is active at any given point. Let's see. It's not here.441.0441.0 Well, I can maybe maybe I can I can draw it a little bit you're going to see in the presentation.446.0446.0 So you start and then you, for example, if it's the make waterfall task, you go you get to a point where you want to ask, is there a good spot to place the waterfall is a good spot in sort of the view of the agent?463.0463.0 If no, then you go to the explore sub policy. And if yes, then you go to the go there, the go there sub policy is activated.478.0478.0 These are these sub policies that we saw are either learned or hard coded.484.0484.0 For example, the Explorer one, you can imagine, maybe it's just sort of walking around until the state class classifier tells you that there is actually a good spot.494.0494.0 So what makes the decision between no and yes, that is exactly this state classifier, this trained state classifier, at some point, it will tell you, ah, now you found a good spot.505.0505.0 And then you can switch policy. So from there, if after the go there, you get to another decision point, and the decision point might be like, are you in front of a big wall?518.0518.0 If yes, use the jump policy. If no, use the walk policy or something like this.525.0525.0 So, as you can see, the state machine itself is hard coded. So the humans came up with, what do we need to do to complete the tasks, but the individual steps, they can be either learned, or hard coded policies.542.0542.0 And that's how they go through fulfilling these tasks, they use the state classifier to always tell them what specific sub task here should be activated at any given point, controlled by the state machine.556.0556.0 And, you know, with that, they finish the task. One additional thing that they sometimes need is this estimated odometry.565.0565.0 This is where they just look at the actions they've performed so far. And they build this overhead map of the agent. As you as the agent walks through the environment, they're able to sort of remember things.580.0580.0 For example, this here is has animals. So they're remember, they're going to remember locations of animals, of bodies of water, and so on. And that allows them later if on in the later stages, if they need to go back to something, they can efficiently find it again.597.0597.0 For example, in the waterfall sub task, they have to go away from the waterfall, turn around to put the waterfall inside of their field of view, and then take a picture or finish the episode.611.0611.0 That could be controlled by this overhead map that they build up. It's pretty interesting. All the while they only have access to the image of the simulator, they do not have access to like the F3 menu, or anything like this.625.0625.0 All they have is the image, they do have some information on their inventory and their current item, but not much more than that.633.0633.0 Alright, that was it from me. If you're interested, read this paper, it's a pretty good write up. And also it has a lot of evaluation, they did a lot of human evaluation as well, computing these true skill ranking scores and so on to compare their system and do various ablations.652.0652.0 It's really interesting. But now I want to give over to the interview part of this. Let me know how you like these more interviewee style of ways of presenting papers.663.0663.0 This one is obviously a very, very applied paper, very visual paper. But yeah, let me know what you think and now enjoy.677.0677.0 Hi, everyone. Welcome. Welcome. This is this is a really, really awesome opportunity. Right here. I'm joined by the winning team of the mine RL basalt challenge 2021 by David Watkins, Nick way to which and Vinicius Gooks who managed to somehow lock their way into winning this competition.700.0700.0 No, I'm kidding. I'm kidding. This is really awesome. I've seen the videos of your agent. And congratulations, first of all on winning and welcome to the channel.715.0715.0 Thanks for having us. Yeah, thank you very much for having us. We're excited to talk about the work.721.0721.0 So if you could describe in your words, the challenge itself, the challenge is about just sort of a bunch of tasks and then humans rate these tasks.735.0735.0 How have you what make what made you decide to take part in this challenge? Even how did you find it? Did you just stumble across each other? How did you form your team?745.0745.0 Like what was your interest in this? Well, I can say that so we all work together. So that's it wasn't like we kind of find each other.756.0756.0 We've had prior experience working together at the Army Research Lab. And, you know, I think Vinicius was actually the one that stumbled upon this challenge.766.0766.0 And what we liked about this challenge was that it's, you know, it's different from most other machine learning challenges out there, different from other AI competitions.775.0775.0 And the fact that, you know, you don't have an objective function to optimize over. Right. So it immediately makes it harder.781.0781.0 You know, the challenge, again, like it's in Minecraft with these very free form, you know, almost lifelike tasks where really you just have a description, a human readable description of what that task is.793.0793.0 There's no reward function, no objective function. So automatically means you can't just apply standard reinforcement learning techniques and you have to employ some sort of clever measures and potentially learning from humans, which is really what the core of the challenge is about learning from humans.812.0812.0 And that's actually, you know, each of us have machine learning backgrounds and the research that we do is kind of human guided machine learning.820.0820.0 So this challenge is almost like perfect for us. Like, oh, this is this is a great challenge. We knew it was going to be hard. But yeah, that was kind of the calling for us.830.0830.0 And just so for I will have introduced this, but the challenge was there were four tasks and every task was just given, if I understand correctly, like a very short description of what to do.844.0844.0 So, for example, find cave is the agent should search for a cave and terminate the episode when it is inside one. That is that is all.856.0856.0 And all you have as an input, if I understand this correctly, is the screen. Right. Not nothing more.863.0863.0 Well, you do have the screen and you do have your inventory and the item that you have currently equipped and the screen 64 by 64 RGB.874.0874.0 That that is a horrible resolution. But you do not you do not have because in Minecraft for people who play it, there's F3, right?883.0883.0 You can press it. You see your coordinates. You see sort of your bio and so on. Not you have none of that. You have to sort of do everything from from the screen alone.894.0894.0 And you're given 40 to 80 human demonstrations, if I know this correctly, but not all of them successful. Right.902.0902.0 Now, is that that was a surprise for us as well when we were using those demonstrations in our agent and we realized, like, look at this guy. He just walked around and through the snowball to end the episode.914.0914.0 How is that even useful? Like, it was a surprise for us as well.918.0918.0 And and sometimes you get some items. So one of the challenges, for example, is to it's called create village animal pen, where it is after spawning in a village, build an animal pen next to one of the houses in a village.935.0935.0 Animal pens must contain two of a single kind of animal. You're only allowed to pen chickens, cows, pigs or sheep. Don't harm the village.943.0943.0 And you're in this case, you'd be given also some sort of a fence and fence gates in order to build the pen.951.0951.0 So it's not like you would have to go collect resources, but the task is still quite challenging.957.0957.0 Exactly. Yeah. You don't have to collect any resource or build anything. You were given everything on your inventory, but like completing all those tasks was already a huge challenge.968.0968.0 So, yeah. And especially given that, again, to remind people, the reward here is not some function you can compute.979.0979.0 The reward is at the end, it's given to human raters. The human reads the description and then the human decides how well did your agent perform it.988.0988.0 And most striking, I find this in a third task that is build waterfall, where the goal is that you have to, I can maybe read the description, after spawning in a mountainous area, the agent should build a beautiful waterfall.1002.01002.0 That's part of the description, a beautiful waterfall and then reposition itself to take a scenic picture of the same waterfall.1011.01011.0 The picture of the waterfall can be taken by orienting the camera and then throwing a snowball when facing the waterfall at a good angle.1018.01018.0 So there is even an essence of sort of a subjectivity, judgment, beauty and so on in it.1025.01025.0 So that just, you know, that is the challenging part, I think here. What was your first, you saw this, you thought, I want to do this challenge, we want to do this challenge.1034.01034.0 What was your first try? Like, what was the first thing you threw at the problem?1039.01039.0 Well, I can speak a little bit about it. Like, at least me, myself, like when I read the challenge, I had no idea how to approach it.1048.01048.0 Because I was thinking, OK, we have a few demonstrations, but like from my experience, research and everything, I thought if we just do behavior cloning using this data, you know, won't cut it.1059.01059.0 Like we don't have enough data. And then we like it took us like a month to solidify like an approach. We thought about behavior cloning.1069.01069.0 We talked about Gale. We thought about like, OK, let's hardcore this whole thing.1076.01076.0 We definitely thought about different approaches. And then I guess in the end, it was a mix of everything.1081.01081.0 And that's what you make clear. So there is a paper about you wrote a paper about your approach as well. And the paper's title is combining learning from human feedback and knowledge engineering to solve hierarchical tasks in Minecraft, sort of pointing out that the best approach will be one where learned elements are mixed with hand engineered elements.1104.01104.0 How did you so my question is sort of how did you come about this? Was this an iterative process or did you you said you scrambled with a bunch of things at the beginning?1115.01115.0 Did you add and add and add? What was your what was your process? What was the first thing that maybe you realized, ah, this works now a little right.1124.01124.0 And then how did you build up your your end solution?1129.01129.0 Well, so I can add a little bit to that. So, you know, we were motivated like the nice thing about the competitions were motivated to try to do well.1139.01139.0 And and so we we knew from the beginning that we didn't want we want to take a different approach.1146.01146.0 Probably a lot of people would just try to apply and machine learning, you know, throw a lot of compute at it.1153.01153.0 And, you know, we kind of realized that really, if we want a solution that is a little less just academic and more that works for this particular application, we're going to need to really use everything right, including, you know, try to inject our own domain bias about the problem into the framework, into the solution.1174.01174.0 So that really led us to these, you know, OK, well, we could have a hierarchy of different modules. Some of those are hand engineered. Some of those are learned, you know, the things that we can't engineer.1186.01186.0 And then we can have like, you know, a state machine where we we know the agent should be doing this.1192.01192.0 So, you know, let's let's not have the the, you know, our own machine learning component learn the things that we already know how to do from scratch. Right. And just make the job harder.1204.01204.0 Let's add that information to the agent and let's, you know, save the learning for the things that we can't easily do. Right. And then have them work together.1213.01213.0 Yeah, I think you make this clear and I'm just going to share a screen for a bit right here. You make this clear in sort of this diagram, which is an overview over your system.1225.01225.0 And at the core here is this state machine. You want to maybe talk a little bit about why a state machine might make sense right here.1235.01235.0 For example, this here is the state machine for for the waterfall task.1243.01243.0 I can talk a little bit about it. So if you saw like those tasks. So, for example, let's talk about the build waterfall tasks since we have the diagram open.1254.01254.0 There's there's really like a hierarchy of subtasks that needs to be complete in order to, you know, to finish this whole task. For example, for the make waterfall.1267.01267.0 You first you need to find a good spot to build your waterfall. Right. And that that means you need to climb up somewhere.1274.01274.0 You need to be like at the edge of a cliff. Right. And then you have to actually build a waterfall. You know, you got to equip your water bucket and, you know, pointed down through the water bucket. Right.1286.01286.0 And then hopefully this waterfall will be beautiful. Right. Assuming you got like a good spot.1292.01292.0 Then you have to go really far away from this waterfall and then position your camera just right to get like the best, you know, the best view of this waterfall and throw a snowball to finish it.1303.01303.0 Right. So there's this whole hierarchy of tasks. It needs to be completed like one step at a time.1309.01309.0 And there's like this logical order. So the state machine was our approach to make sure that the agent would actually follow this order, you know, without coming back and forth.1319.01319.0 Like if you do like, for example, some just an end to end machine learning approach, the agent might, you know, let's say go find a spot and then we'll go back, take a picture, you know, come back again.1331.01331.0 Right. Equip the water bucket to build the waterfall. So the state machine was our solution to make sure the agent would follow kind of this logic for each task.1342.01342.0 And I think you profit from the fact that all of these tasks can be sort of described quite well in this state machine fashion, as I think a lot of, you know, if you play Minecraft as a human, that's sort of the same thing you do.1357.01357.0 Right. You if you want to beat the ender dragon, you okay, first I need to do this, then this, then this. And it's quite the same thing with a few decision nodes in between.1366.01366.0 And these decision nodes here in the in the green, those are now decided by a classifier, if I understand this correctly.1374.01374.0 So you build this, this little interface here where humans could rate, you were allowed in the competition to collect a little bit like a limited amount of different human feedback.1388.01388.0 And you chose among other things, you chose to have humans label different images from the game with such a with with such maybe you can describe it a little bit, what were you interested in? And why did you choose to put the additional human labeling into this task and not any other task?1411.01411.0 Well, like, why did you prefer this? Something important to keep in mind is that you're allowed to include 30 megabytes of additional data in this competition.1420.01420.0 And the Minecraft simulator is such that if you were to record a bunch of actions or steps that the player took and try to replay them, it's not currently designed to handle RNG the same way every time.1434.01434.0 So if I go break a block, that block is going to fly differently depending on the state, the internal state of the random number generator.1445.01445.0 And we have no control over that. So you can't seed it necessarily. We can't seeding it just doesn't work.1452.01452.0 So we couldn't just collect more demonstration data other than videos. And that would eat into 30 megabytes very quickly, as I'm sure you could imagine.1462.01462.0 So dividing up each of the tasks into a bunch of shared states made the most sense to us.1470.01470.0 It's something we've used in previous research to handle navigation tasks before.1476.01476.0 And it works reliably. And I think there's a lot of research in making state classifiers work really well.1483.01483.0 So it was more just us as a team, while we're watching TV, labeling a bunch of Minecraft screens.1492.01492.0 The most difficult part, of course, though, is it's 64 by 64.1496.01496.0 And there are many situations where maybe you want to recognize that there's an animal in the frame and it's a chicken and it's the small white blob, but it could be confused with a flower.1505.01505.0 And you're kind of fighting yourself to make sure that this actually works.1510.01510.0 And so there were some different strategies we were looking to employ to make sure that the state was classified correctly.1518.01518.0 But it worked pretty well.1521.01521.0 Cool. And I think people can see here maybe at this graphic, but you have such things like, for example, good waterfall view, which makes sense, right?1530.01530.0 This is a subjective thing of the reward function.1533.01533.0 So it makes total sense to include that in the human annotated data and not code a heuristic.1541.01541.0 But you also have things like a danger ahead, which you then use.1547.01547.0 So I think once you know which node you're in, right?1555.01555.0 In this state machine, very often the blue blocks right here, which are the actions, the blue blocks involve going somewhere.1565.01565.0 Right. For example, if has mountain, then, you know, if you don't have a mountain, find a mountain.1571.01571.0 If you do have a mountain, go to the mountain.1574.01574.0 And that part means that your Minecraft agent has to go from point A to point B.1581.01581.0 And that's where you build a specialized navigation, navigation subroutine.1588.01588.0 And you said right now you've already done this in the past.1592.01592.0 Can you tell maybe a little bit in general, what does it take to make agents navigate around?1601.01601.0 So can I just mention one more thing about the state classifier?1607.01607.0 Sure.1608.01608.0 So with the state classifier, like David and Anisha were saying, it's really the core of the state machine.1615.01615.0 So we knew we wanted, you know, it's the thing that makes the drives our entire solution.1620.01620.0 So it has to be, you know, more or less somewhat accurate.1623.01623.0 And we needed a lot of data. So we actually collected around, I think, eighty eight thousand labels, which sounds like a lot.1631.01631.0 But of course, you know, that type of manual annotating, no one really wants to do, you know, as machine learning scientists,1638.01638.0 we'd rather spend that time trying to, you know, code up a solution to do that instead of doing it ourselves.1645.01645.0 But what we did, we tried to make it as easy as possible by, you know, we're not HCI experts, but, you know,1652.01652.0 we tried to come up with a kind of intuitive labeling interface to make it as quick as possible to kind of1660.01660.0 you know, like one demonstration that's three minutes long at a, you know, FPS of 20 frames per second.1669.01669.0 You know, that's a lot of images. And we try to take advantage of the fact that the images are somewhat correlated through time.1677.01677.0 Right. So the way we designed our labeling interface is kind of just a step through each image of the trajectory.1685.01685.0 And if you hold down a button, let's say one of the buttons is, you know, there's nothing ahead, it's just open fields.1693.01693.0 So you can just hold down that button and it's going to traverse, you know, through the demonstration until something else comes up.1699.01699.0 And then you can just move a different button. So very quickly, you know, you can, you know, label five thousand images in one trajectory1706.01706.0 in like less than a minute because you're just holding down these buttons instead of like, you know, showing an individual image1712.01712.0 and then selecting the label and then the next image and selecting the label. I think that really allowed us to get it.1718.01718.0 It sacrifices a little bit of accuracy. Maybe when you're transitioning, you might miss, you know, get a few misclassifications,1725.01725.0 but you're able to get a lot more more labeled images.1730.01730.0 I think this is a recurring theme sort of in real world tasks, the efficiency of data labeling when you include humans.1741.01741.0 I've just recently watched sort of Elon Musk's appearance on Lex Friedman. And before that, I've commented on Karpati's talk about the autopilot there.1752.01752.0 It's a thing that you see again and again that the easier you make it for humans to annotate data, the more benefit you have later.1761.01761.0 It's almost an unfair like multiplier that you have on your system. I think it's neglected currently by academia.1770.01770.0 So it's pretty cool that you thought about this as well.1775.01775.0 Yeah, I think it is neglected because it is not easy and takes a lot of time.1780.01780.0 And like manual labor, nobody wants to do manual labor, but definitely having like high quality labeled data labeled by humans makes totally the difference.1793.01793.0 So and now we'll let's go to the navigation subroutine. How do you navigate? Wait, that is here.1805.01805.0 So you have a navigation policy which essentially says the agent needs to go from A to B.1812.01812.0 And what does it take to build that? Like it seems very complicated in a game so complicated as Minecraft.1821.01821.0 So, well, so the behavioral cloning part, right? So that part is, you know, unfortunately just very simple.1829.01829.0 It's not any secret sauce or anything complicated.1833.01833.0 You know, we again just prefacing by this, you know, was a competition and we had a deadline.1839.01839.0 We had so much more that we wanted to do with this particular part, right?1843.01843.0 For the solid navigation part, we wanted to do something way more than just standard behavioral cloning, you know, things like generative adversarial imitation learning, you know, trying to have better architectures.1857.01857.0 In the end, we didn't have enough time. We were scrambling. And for this component, we just did behavioral cloning.1864.01864.0 But the way that we did that is, you know, as you can see in this model, it's like, okay, the agent only has the image as input and its output, you know, are more or less just the direction keys.1875.01875.0 So it can go forward, it can turn left, it can turn right, it can strafe left, strafe right, and then it can move its camera.1882.01882.0 And really the way that we did that is we just we had all these demonstrations for each of these tasks.1889.01889.0 We kind of the only kind of trick that we applied was that we realized, right, this is just a navigation component.1895.01895.0 So we only want to learn to imitate the part of the demonstrations that we're navigating.1901.01901.0 Right. So let's just chop off that demonstration just to that navigation part and then feed that into our navigation policy.1910.01910.0 And so that's basically what we did was, you know, any time where the agent was building, like building the pen or the village or the waterfall, we cut those segments out.1921.01921.0 The remaining segments are where the agent is just trying to go from one point to the next.1927.01927.0 We kept those in and use that as our training data for the behavioral cloning module.1933.01933.0 And in this model here, it says image input. Do you also give the model access to, let's say, the results of your state classifier and maybe the current state machine state or something like this?1947.01947.0 So the agent knows where to go or do you rely on behavior cloning for the entirety of navigation?1955.01955.0 Yeah, that's a really good point.1958.01958.0 It's our this particular navigation policy is just terribly simple. It's really just the image input.1966.01966.0 It's being driven by the state classifier in the sense that it allows, you know, the state classifier decides when to start and stop the navigation policy.1977.01977.0 But we're not feeding in any information directly from the state classifier or other other more interesting information that that certainly would help.1986.01986.0 If we had more time, we could probably do that. It would make sense to do that.1990.01990.0 But right now, the state classifier just decides when to start that navigation policy and when to terminate the navigation.1998.01998.0 I think I just just went and I had a little bit on top of that.2003.02003.0 Like the main reason we didn't add anything else on this is because we didn't have.2009.02009.0 So like the so this navigation sub task policy was trained from the demonstrations provided by the competition.2017.02017.0 So that data didn't have any like state machine. So the state machine was everything on our side.2022.02022.0 So we really only had access to the actions that the agent took right and the camera data.2030.02030.0 And again, like I think the using that demonstration data provided by the competition to train only the navigation sub test made sense because let's say think about it.2042.02042.0 Let's say we want to do an end to end behavior cloning.2045.02045.0 Right. And then you were doing the fine cave task and the fine cave task.2050.02050.0 At some point, the human will throw a snowball when the agents inside the cave.2054.02054.0 Right. And that's only one data sample.2057.02057.0 And the whole episode has about two to three thousand.2060.02060.0 So you have one same sample to throw in the snowball on over three thousand samples.2066.02066.0 But to find the cave, it took a lot of steps. And this is all really useful for navigation.2073.02073.0 So we did this like Nick said, this preprocess to remove all those actions, leave only the navigation part and use that to train this navigation sub task.2084.02084.0 And I think that was pretty helpful to in our approach.2089.02089.0 So it's fair to say that, for example, you're here and you are your has mountain classifier says yes, then the state machine would simply activate the navigation.2105.02105.0 Does it? Yeah. But it doesn't it doesn't necessarily tell it where to go.2108.02108.0 You just rely on the fact that your demonstration in your demonstration, people have generally gone towards the mountain and therefore the navigation policy would have learned that implicitly.2121.02121.0 Exactly. Let me I guess let me explain this diagram a little bit.2125.02125.0 So what you said is correct. So the green diamonds are decision nodes.2129.02129.0 Right. And that's that's based on the output of the state classifier.2133.02133.0 Right. So like has mountains, you know, if it's over, let's say 90 percent confidence, we'll take that as a yes.2140.02140.0 Right. And then we go to those blue rectangles and each blue rectangle is a sub task.2148.02148.0 And those sub tasks can be either learned or coded or like hard coded.2153.02153.0 So, for example, go to go or find go actually find go was learned from the human demonstration.2161.02161.0 So we would not say like something like, oh, go to this coordinate like we didn't have.2167.02167.0 Right. We would just use the human the policy that was trained from human demonstrations to navigate, let's say, going up the mountain.2176.02176.0 Right. And then let's say on that part of the diagram where you have the dash line, you know, there's a green diamond there written at the top.2186.02186.0 So let's say if the state classifier detect that we're on top of the mountain.2191.02191.0 Right. Then we would switch to this place waterfall sub task and this place waterfall sub task was hard coded.2198.02198.0 So that was not learned from the human demonstrations. And what the sub task does is basically point your camera down, keep the water bucket and throw it.2206.02206.0 You know, that's kind of placing the waterfall. So those blows are a mix of learned sub tasks and hard coded.2214.02214.0 What my question is a little bit. You have, for example, this danger ahead state.2220.02220.0 Right. But you don't feed any state to the navigation policy. Where is the danger ahead used inside the state classifier somewhere?2231.02231.0 Like you say, if there's danger ahead, then we don't even want to activate navigation.2236.02236.0 Exactly. So that's something that it's like a safe critical sub task that takes priority over everything.2244.02244.0 So it doesn't matter if you're looking at the mountain, whatever you need to do, if there's danger ahead, just avoid it.2250.02250.0 Right. So it's like a sort of a safe override that's always on no matter which sub task we're doing, if you're following the human or not, because, you know, just avoid danger.2261.02261.0 Because our first iterations of Asian and even the final one still does some time when you fall on one of those lakes, you just can't escape.2271.02271.0 It's just too hard. Like sometimes you're like two blocks tall.2276.02276.0 Then it's hard to like teach the Asian to break the blocks and jump, like do all those things that us humans do pretty well for the Asian is pretty hard.2285.02285.0 So our Asian got stuck a bunch of times. Then we had to add like some safety sub tasks to help a little bit the Asian to escape those things.2297.02297.0 And at some point, you also built in this odometry estimation because you only had the image and you thought it would be maybe you can explain this.2313.02313.0 What led you because it's not a straightforward thing to include. Right. If I think about how would I solve this task?2320.02320.0 What is the odometry estimation? What is it for and why did you include it?2328.02328.0 I can talk about it. So like you mentioned at the beginning of the video, we could not like in Minecraft, we do know where the Asian is like when you were playing the game.2338.02338.0 You can press like F3, you can see everything. Right. But in the competition, we were not allowed to use that. Right.2344.02344.0 So we had some ideas. OK, let's use the simulator. But we were not allowed to do that.2350.02350.0 But we're thinking like, what do we know about this problem? Right. So we do have access to the actions that the Asian took. Right.2358.02358.0 And we do have access to the image. Not only that, we know a little bit of Minecraft.2363.02363.0 So we know that the simulator runs at 20 frames per second. So each frame is one over 20, 0.05 seconds.2372.02372.0 So we know this this this time interval between each frame. Right. And from Minecraft, we know that, for example, the walking distance is actually, I think, 4.4 point thirty two meters per second.2386.02386.0 So we had this information from the wiki. So let's say if the Asian send the command to move forward.2394.02394.0 Right. And not considering inertia or anything. Right. We could assume that in one frame, the Asian walked four point thirty two times, 0.05.2404.02404.0 Right. So like this velocity times this DT, this time interval.2408.02408.0 So we know how much the Asian walk in the X direction. Right. And then we had the actions.2416.02416.0 We had access to the actions for the camera control. So we could estimate the heading.2423.02423.0 So just based on the actions that the Asian took and knowledge of the simulator. Right.2430.02430.0 We're able to sort of estimate velocity X, Y and heading. And then we integrate that over time because, you know, your time interval.2439.02439.0 So you can come up with estimates of X, Y and heading for the Asian. And that's what you see on this kind of this black diagram on the right, which I can explain everything in more details to you.2453.02453.0 You know, but I mean, you build this sort of map almost like this is an overhead map of the agent in its environment annotated with.2464.02464.0 First of all, what you've done so far, right, your position that's that's been going on.2470.02470.0 Maybe if this here loads this year is different trajectories.2474.02474.0 But you also annotate this map with various things that you find like whenever your state classifier says something.2483.02483.0 Where is this information used? I guess it's you said it's not in the navigation because that it doesn't get any additional features.2492.02492.0 Where is the information that you estimate from from this this overhead map? Where is it used?2500.02500.0 The best example for this is to make waterfall task. So when the agent places a waterfall, you know, something we were thinking is maybe we'll try the behavioral cloning.2510.02510.0 But often, you know, the behavioral cloning doesn't really stay still very often because they really learned.2517.02517.0 Well, the navigation sub policy. So instead, we we sort of use that heading estimation to move the agent away a fixed amount and then rotate around to look at it.2529.02529.0 So there are just certain tasks that is really important that whatever the final view is, the line with some landmark in the environment that we don't have a ground truth information for.2543.02543.0 Yeah, so it's really the odometry is mainly used in various places in the state classifier.2549.02549.0 I mean, start at the state machine in some of the subtasks like David saying. Another example is the animal pen, right?2556.02556.0 The challenging part of that task is you really have to build. You first got to find an open location, then build the pen.2563.02563.0 And then you have to leave that pen and go find the animal somewhere.2567.02567.0 Right. They could be anywhere and then lure them back to the pen. So you have to remember where you built that pen.2575.02575.0 And so that that's, you know, the odometry comes into play for that place.2580.02580.0 So we were using the state classifier to kind of classify, OK, here's an open location.2586.02586.0 Now we switch to pin building mode. OK, the pen is built. Let's go find some animals.2592.02592.0 We remember the location of that pen, you know, based on our estimated odometry.2596.02596.0 And then once we find some animals, then we try to go back to that location.2601.02601.0 And just to say that the try to go back will be a hard coded policy that takes as an input the remembered location of the pen and your guess of where you are in relation to that pen.2615.02615.0 Exactly. Yeah. So, yeah, at that stage you have a X, Y coordinate of the pen and you have an X, Y and had the estimates of your position.2625.02625.0 Right. So you can basically compute the angle between like where you're looking and where the pen is. You can compute this angle.2631.02631.0 Right. And the policy was literally kind of close this angle and then keep moving to kind of reduce this distance over time and go back to that location.2640.02640.0 So the simple policy. There are a few limitations, though, on the odometry side, which I just want to comment just to don't say this was like a God dear approach for that.2649.02649.0 So, for example, since we only use the actions, right, if you think about it, the odometry is just seeing the actions.2658.02658.0 Right. And then, OK, the agent is moving forward. So we've seen this moving forward action.2664.02664.0 Right. So we're integrating that over time, increasing the distance and everything.2667.02667.0 Right. Well, what if the agent gets stuck like behind the rock, behind the tree and it is still moving forward like in Minecraft?2675.02675.0 You can still kind of walk forward sort of sliding. Right. But you're still stuck in place.2679.02679.0 But the odometry does not know that. Like we had some some ideas to integrate like different in the pixels.2687.02687.0 Right. Using this camera data to know when when the agent is stuck. So we can order that.2692.02692.0 But we didn't have time to do that at the end. But this approach, our current approach still works for a short, short distance.2699.02699.0 Right. So, of course, the longer you walk, you know, like the the drift will be just higher on this estimation.2706.02706.0 But for short distances actually actually works pretty well. And I guess it's sorry.2713.02713.0 I was going to say that a slam approach in a 64 by 64 image that's only RGB is incredibly challenging and probably not the right approach for this particular challenge.2726.02726.0 And it might also be fair to say that you said you had a lot of ideas.2733.02733.0 I guess if you were to go further, you'd probably let's say try to come up with a navigation policy that's both learned but also controllable in some way.2745.02745.0 Try to come up with an odometry estimation that takes into account the picture which could recognize when you're stuck and so on.2752.02752.0 I think there's there's a lot of stuff to improve, but I'm very impressed by sort of your your your pragmatism of, OK, this works well enough.2760.02760.0 Let's go on. Was there was there moments when I guess there's moments in every project when when you're or what was the moment when you most thought, ah, this is not going to work.2775.02775.0 Let's give up. Like, did you have a moment like this and what did you do?2783.02783.0 You guys want to comment on that? Well, there's there I guess a lot of those moments we if you go back to the main overall diagram, we definitely like had went back and forth on on what should the solution be?2800.02800.0 You know, we were still toying around at some points with with, you know, a more, you know, end to end approach in some places and whether we should put our eggs in that basket or whether we should do this kind of approach.2815.02815.0 Ultimately, you know, this is the one that we landed on and we we designed this.2821.02821.0 The nice thing about this approach is it's hierarchical, but it's very modular. Right. And the idea is that each of these subtasks, you know, their individual modern modules that we can improve upon or replace.2833.02833.0 And so, like, you know, if we had more time, some of the things that we would do is start to try to replace some of these hand engineered subtasks with more learning based subtask and or, you know, replace the navigation module with a more advanced learning module that uses more information.2852.02852.0 One of the things we spent a lot of time on that never made into or at least was was kind of using generative adversarial imitation learning as our core algorithm for learning the navigation module.2866.02866.0 And, you know, with Gale, it's it's basically using again. And as we found out, like everybody knows, GANs are notoriously difficult to stabilize, including GANs for Minecraft. And it didn't ultimately end up making it.2885.02885.0 So we had to revert back. So that was one of our scenarios where we're like, oh, this is this is definitely not going to work. You know, we spent a ton of time doing that and we had to kind of, you know, replace with our with our backup, which is just, you know, standard behavior.2900.02900.0 So go ahead. Also, the at one point, my brothers are very good at Minecraft and the Minecraft speed running community is a pretty big thing. So at one point we were considering why don't we just get somebody to play Minecraft really well?2919.02919.0 But that stupid Minecraft simulator limitation and also, you know, it's it's one thing to get a bunch of people to play the game better than maybe the demonstrators were playing. But that also means that, you know, that data won't necessarily be very rich because they can't play the game well and label the data at the same time.2940.02940.0 And I think it comes back to this problem of labeling data really conveniently is difficult, especially when you're driving the agent simultaneously. So it becomes a very difficult challenge to use human data when the the amount of data you can actually collect is small.2962.02962.0 And this being Minecraft, I think I like I'm fascinated by this because I wonder how much world knowledge is in inside a human when they play Minecraft and how much is sort of learned because the world is different, like literally different every time.2978.02978.0 And I can learn Minecraft by just watching someone do it a few times, right? I can perfectly not perfectly, but I can well generalize to other worlds. Is that because I've watched someone I've done it a bunch of times? Or is that because I know from my life what sand is and what water is and how it behaves?2998.02998.0 And I think that I don't know. Yeah, I think I guess the main advantage of like, you know, humans is that, you know, we've lived, you know, 20, 30, 70 years already right in the real world.3013.03013.0 And then Minecraft tries to mimic that. So we humans have a huge kind of baggage that we can use.3020.03020.0 But we have to always remember, like, those Asians, they start from scratch. They literally start from nothing. Right. We had to collect data to teach what danger was for those Asians.3030.03030.0 Like, like, had to teach, oh, don't jump in the water, you know, don't don't drown there, you know, things like that. So that is very challenging as well.3041.03041.0 And I have I have your sort of so for videos that you upload, and they have side by side the agent view the classifier, but also the odometry estimation.3057.03057.0 Do you want to maybe so this is for example, do you have one that is your favorite of these four?3063.03063.0 Probably the waterfall, I think, will look pretty nice. So this is the build house was pretty challenging.3071.03071.0 This is 30 seconds. I'm going to I'm going to slow it down to like point two five right here.3078.03078.0 Do you maybe? Oh, sorry. Yeah, I can't. Oh, yeah. I can like call anyone to comment a little bit on what's happening right here.3084.03084.0 So which state is it in? What's happening? Yeah. So so this is a video of the agent solving the make waterfall task. Right.3093.03093.0 And then you mainly see in the screen in the screen two panels. So on the left side, that's the RGB.3100.03100.0 So this is like a camera view of the agent. Right. And on the right side, this black panel is the estimated odometry.3107.03107.0 So if we start there on top left, you see like action and then a huge tensor. Right.3114.03114.0 So that's the I think 12 or 13 actions that the agent was performing. So they're mostly binaries.3120.03120.0 So like move forward or not move back or not, you know, things like that.3124.03124.0 And below that, you see the raw output of the state classifier.3129.03129.0 So we had 12 classes or I guess 13 with, you know, the non class. And you see like the confidence of the classifier, you know, for classifying the state like this camera, this camera image.3142.03142.0 So you see like right now, you know, facing wall is pretty much almost 100 percent.3147.03147.0 I think it is from all the stone that the agent is seeing. So it thinks it is a wall. Right.3153.03153.0 And on the right side, the odometry. So we can start there on the on the top part there.3159.03159.0 You see a X, a Y and a heading. So X, Y. So that's the estimated position of the agent.3166.03166.0 So that's not the ground truth. So again, we didn't have the ground truth. Same with the heading.3171.03171.0 So that's estimated. And that camera angle there is like a vertical angle. Right.3176.03176.0 And then on the right side, you have like some time. So we kind of just have a keep track of time.3182.03182.0 And then you have a legend. So the legend there is for all the colors you see in the odometry.3188.03188.0 So the red one, the red dot is the agent. So right now it is down at the bottom of the screen.3195.03195.0 Whenever the way the agent walks around, it leaves like this trace. So that's the Y dashed line that you see on the screen.3204.03204.0 And then like right now, you see, for example, it just saw that cyan, I think, blob at the bottom there.3213.03213.0 That's when the state classifier detect that we are on the top of the waterfall.3218.03218.0 So you see that that's the last thing on the legend there. So basically, yeah, the agent walks around3225.03225.0 and some of the relevant states that we classify, we sort of drop a pin in the map.3230.03230.0 Kind of just to keep track of it. So in the video, the first like 25 seconds or so, what this is,3239.03239.0 it starts off basically with the navigation policy. Right. So the behavioral cloning module that we trained is in control3247.03247.0 and it's driving and it's basically trying to mimic all of the other human demonstrators that did this task,3254.03254.0 which is more or less kind of walk around and look for a good spot.3258.03258.0 And then when the state classifier detects like, OK, this is a decent spot, that's when you saw it switch to the,3263.03263.0 all right, let's build the waterfall. And then after build the waterfall, the state classifier switched to the now go take a picture sub task.3272.03272.0 And so that's basically what you see in this video. And one thing I'll say with this,3278.03278.0 the interesting thing with the navigation policy is, you know, this is something we kind of noticed3285.03285.0 and it's just a theory. We don't have any proof on it. But like the, you know, the agent jumps around a lot.3292.03292.0 But we think that's because the agent is mimicking the human demonstrators.3298.03298.0 So like so jumping for the sake of jumping, not necessarily to jump over stuff like, you know, there's there's some players faster if you jump.3308.03308.0 Yeah, yeah, exactly. And that's seen in the demonstrations or some players like like me.3312.03312.0 I just jump idly, you know, just a fixation. So I'm just like randomly jumping, not not to particularly jump over anything.3320.03320.0 You kind of see that in the agents behavior. So it's almost, you know, makes it more human like, at least in our opinion,3329.03329.0 versus, you know, a hard coded navigation policy, which mainly, you know, you might expect it to just walk without jumping unless it needs to jump right over something here.3340.03340.0 You know, the agent is kind of just more pseudo randomly jumping like a human would.3345.03345.0 And I thought that was pretty cool because, you know, another part of this competition that we haven't talked about yet is not just, you know,3351.03351.0 developing agents that can do the task the best, but also there was a sub thread to the competition of who can build the most human like agent,3359.03359.0 which we also won that prize. So, you know, this would potentially I mean, really our whole system,3369.03369.0 you know, is sort of aims at the human life because we added a lot of human knowledge to it.3374.03374.0 But like the behavioral cloning part, you know, that might also add to that because it kind of moves around more or less like it like a human would move around.3382.03382.0 And it looks a little less robotic, like if it were kind of a more engineered.3388.03388.0 Except like here when when it's like a good spot for a waterfall, you immediately point down and start like I guess this is the hard coded part.3397.03397.0 You see right now immediately point down, build a bunch of blocks, place the bucket.3402.03402.0 And then it's interesting. So this part here is hard coded as well. It's just like move the agent away.3408.03408.0 And we see the agent kind of slide on the left a little bit because I've noticed that later when it turns around, it sort of almost misses a little bit the angle.3419.03419.0 Right. So this is this could be this drift that you have in the odometry estimation.3423.03423.0 So it's trying to make a picture of the waterfall directly. It misses like a little bit.3428.03428.0 So I guess that would be that would sort of be the problems that you get in just having the just having the estimation from the action which you mentioned.3438.03438.0 Yeah. So, for example, when you throw the water down, right, sometimes the agent will float in the water and that will turn the agent a little bit left and right.3447.03447.0 But the odometry doesn't see that because the agent didn't command the camera movement. So it doesn't update your headings.3453.03453.0 So that can also cause problems later. But yeah, like you said, that part was hard coded like the place waterfall sub task was hard coded.3464.03464.0 But all the way up to that thing, up to that part was learned from human demonstrations, which is the navigation sub task.3474.03474.0 I think what you what you need to do is you just need to train the navigation thing on, you know, dream.3482.03482.0 So you just want to you just want to train it on like a bunch of videos of dream and then just see what happens.3488.03488.0 I would be so curious to see what happens.3492.03492.0 Well, that's what we wanted to do that initially is we thought, oh, look at all of this awesome data on YouTube that we could maybe try to learn from.3499.03499.0 But there's no actions associated with it. Yes. OK, true. You sort of have to estimate the actions almost a little bit.3506.03506.0 And you'd also have to like there's a lot of things you'd have to guess at what's actually going on.3511.03511.0 Which where do we crop the video? Right. There's all this stuff they have overlaid and it becomes more challenging to use YouTube data.3523.03523.0 But I see. OK. You wait. What was I was I going to? One thing that yeah, one thing that I was a little bit like a tiny bit dissatisfied with with this competition, obviously, it's already super duper challenging.3544.03544.0 Right. And Minecraft is so much more complicated than this thing. But there were these four tasks and you knew them ahead of time.3554.03554.0 Right. That's why you were able to sort of build the state machine. The descriptions were very clear ahead of time.3562.03562.0 Let's say that I come and I'm the organizer and I change the challenge for next year and next year.3569.03569.0 It's still the same thing. It's human rated. It's described in just like a simple string. But I won't tell you what the string is.3578.03578.0 Right. I won't tell you ahead of time. How would you how would you go about designing a system like this?3587.03587.0 Like what would you would you do? Would you try to go the same route? Or let's say you also had very limited resources like you had now.3597.03597.0 You can't train like a giant RL system. I think we would definitely be forced to go a different route, which I think would be good.3606.03606.0 You know, one of the things I like about this competition, again, is that it's you know, I think it's important for the field because, you know, it's these tasks.3614.03614.0 Again, that you can't just, you know, do this black box optimization over because there's no objective function.3620.03620.0 So you're forced to really try to learn from a human. Right. Or do something. Right. And, you know, we really took that to heart.3629.03629.0 We knew like, OK, in order to do wellness competition, we cannot just use the human provided demonstrations like the majority of the other teams.3641.03641.0 We had to add our own additional human input and feedback. And we did that with the design of our state machine and in the labeling, the human exhaustive human labeling that we added.3653.03653.0 But, you know, to take it a step further, really, I think the interesting thing would be to have a system where you have you learn from real time human feedback, which our system didn't do.3666.03666.0 Because, you know, well, one is that's more challenging and we didn't have time. And because all the the tasks are known ahead of time, you don't have to have real time human feedback.3678.03678.0 You can, you know, collect your human feedback or human labeling beforehand and then use it.3683.03683.0 But if you have now a new iteration of this competition where you do not know the the tasks ahead of time,3690.03690.0 then you now might need a system where your agent needs to learn from human feedback in real time and kind of interact with the human to kind of get that learning.3699.03699.0 Because, you know, you're just seeing what you need to do the task at competition time.3704.03704.0 So I think that would be really interesting and that would force more solutions to use something that that uses real time human feedback.3715.03715.0 What set you apart? If you you probably seen sort of the other teams that competed and so on.3722.03722.0 And I'm sure they were also they were also engaged and motivated and tried a bunch of things.3728.03728.0 What do you think was sort of the or maybe the most defining factor that let you win?3736.03736.0 Was it I'm sure there was a level of stochasticity in the evaluation, but, you know, you won I think not one, but two of the three subcategories even.3748.03748.0 So it must mean that you had a considerable, let's say, edge over most of the competition. What in your estimation was that?3759.03759.0 I have a guess. You guys can comment on that.3762.03762.0 I think in my opinion, I think our edge was actually using human feedback data.3769.03769.0 So like the other teams, if I remember correctly, I think number two used sort of improved algorithm that would improve on Gale.3778.03778.0 So that was kind of sort of full RL approach. The third team tried to use some of kind of learning from human preference, if you remember that paper.3787.03787.0 But they didn't use a human to rate the trajectories. They used like a heuristic.3793.03793.0 And we were the only team that actually used human data. So we, you know, we label a bunch of data.3800.03800.0 You know, we added our knowledge, our bias on the task and everything.3804.03804.0 So I think really using the human, I think, was the key factor that allowed us to win two of three of the awards.3813.03813.0 100%. Like, you know, yeah, we had a state machine approach with, you know, these modular hierarchical design.3821.03821.0 But really, we wouldn't have been able to do that if we didn't have, you know, this classifier that was generated with additional human feedback and human labeling.3831.03831.0 And so it's really the thing that stood us out. And like we said, it was, you know, the other teams, they just used the human demonstrations.3840.03840.0 And even the third place team, they used a simulated human, right?3847.03847.0 Instead of, you know, doing the hard work of actually getting that human feedback, they just defined this simple heuristic.3854.03854.0 And I think that right there is like, you know, the important thing, like the field, you know, sometimes can just like, oh, well, let's just, it's easier to kind of simulate out the human.3864.03864.0 Let's just, you know, come up with a better algorithm. But it really just shows like we should do a better job trying to incorporate human feedback because it's definitely, you know, valuable information and can really improve the way we develop our algorithms.3883.03883.0 And I think it's important as well to, when you look at Minecraft, it's very much feels like an open world sandbox problem, very similar to using a robot in the real world.3895.03895.0 And collecting real world data is about as difficult as I would say, well, it's a little more challenging in some ways, but challenging to collect lots of good, rich human demonstrations in this particular environment.3908.03908.0 And so if we were looking at this as a generalized approach to solving this kind of navigation problem, I think we would have used a similar approach for handling this on a robot.3920.03920.0 Where, you know, a robot going to go pick something up somewhere can be broken down into a bunch of discrete steps and we solve each of those steps really well.3929.03929.0 Whereas an end-to-end approach, we risk having situations where the neural network is doing something that we can't debug at all.3938.03938.0 And I think that hierarchical approach really let us debug each step really well, as opposed to the monolithic approach.3949.03949.0 Now, just to say, on the leaderboard website, there is a team that has like a better score than you. Is that an artifact of the one leaderboard or is it a late entry after the competition?3963.03963.0 So that's the public leaderboard, right? And it's an unofficial leaderboard. This highlights the other difficulty of this competition.3972.03972.0 Again, there's nothing that just automatically grade everything. You have to just get volunteers to literally just sit down and look at pairs of videos of different agents and see which one is better.3985.03985.0 Very, very arduous task, right? And the public leaderboard is just any random person with a web browser can go on and start rating all the people.3995.03995.0 We provided some ratings. It's completely unofficial, but it was just used to kind of determine who would go to the next round, so the top 10 teams.4006.04006.0 And then the competition organizers actually hired professional contractors, but actually had not just random people, but contractors go and do official evaluations to determine the winners.4021.04021.0 And on that one, that's where we won first place. But on the public leaderboard, we're not showing us first place because of the stochasticity of all the human raiders.4033.04033.0 I love that the professional contractors were probably like they had to know Minecraft, right? So they're like the most competent people in it were probably like some 13 year olds.4044.04044.0 A bunch of kids to watch some videos, give some ratings.4048.04048.0 Excellent. Yeah, is there anything you'd like to... that was my exhaustive list of questions that I had about this.4057.04057.0 Is there anything you feel is important to add for people to know if they want to do something like this themselves?4067.04067.0 I think during the presentation we had a slide about that. So this competition might happen again next year, or I guess this year already, 2022.4078.04078.0 So if you're really interested on that, make sure to go ahead and start playing with the MineRL package now, because it took us a long time to figure that out.4089.04089.0 I think I can speak for all three here. I think that was our first time working with the Minecraft package, like the reinforcement learning package.4098.04098.0 So it took us some time to learn all the, you know, how to work with their action space, observation space and everything.4105.04105.0 So if you want to like an extra edge this next year, you can maybe start playing with the package now.4112.04112.0 And I think that's it. Maybe play a lot of Minecraft. I think that helped.4120.04120.0 Yeah. What do you think, guys?4122.04122.0 You mentioned the paper that we have, but we also have made our code available for anybody that wants to try it themselves or improve upon our solution.4134.04134.0 I think the paper got the link to the code. Yeah, I'm pretty sure it's there. So go ahead, play with our code. Maybe make it better. Let us know.4144.04144.0 Maybe make some pull requests.4148.04148.0 Cool. Awesome. Well, in this case, thank you so much for being here and sharing this.4154.04154.0 It's really I love I like it. I think it's really cool when when things like this get get out into the well, not real world, but Minecraft world, which is close enough.4165.04165.0 It's incredibly hard task. And just from the videos I saw, I was surprised by, you know, just how far you can get with how little sort of resources and data.4177.04177.0 But it's just one last thing like the definitely, you know, after this first year's competition, the you know, this is far from solved.4186.04186.0 And I think the competition organizers realize that, too. So out of the four tasks, which are, you know, that you already mentioned, you know, basically advancing in difficulty, the fine cave and the make waterfall the easiest.4198.04198.0 Those are pretty much solved. The create animal pen and especially the build the village.4204.04204.0 None of those solutions came even close to really solving that.4208.04208.0 You know, I'm sure the human raiders are just looking at two really junk agents doing random stuff and trying to pick which one's better.4216.04216.0 Right. But, you know, it's still like on that build village task, but still a very simple task out of the range of tasks that you can conceive in Minecraft is still far from from solving.4228.04228.0 And I mean, yeah, there's there's no crafting yet. There is no fighting. There is no exploring. And this isn't even like this. This is where Minecraft starts.4239.04239.0 The actual game of Minecraft is where you sort of set your own goals. Right. And you try to achieve something new.4247.04247.0 Yeah, it's it's cool to see that there's still a lot of a lot of stuff to do. Awesome.4253.04253.0 Thank you so much for being here. And yeah, I hope to see you next year again.4263.04263.0 Thank you very much for having us, Yannick. Like I said, I watch a bunch of your videos. I really like your channel. I'm excited to see.4272.04272.0 Hey there, it's Yannick. I'm going to leave you with the submissions of the team to the competition that were actually judged by the human annotators.4281.04281.0 So you can see what the human saw and what it takes to win such a competition.4286.04286.0 We'll show you all the submissions for each of the tasks in parallel. Let me know if you like this video.4292.04292.0 Leave a like if you did and leave a comment if you have comments, suggestions, anything at all. See you next time.4313.04322.0 Oh, oh, oh.4344.04352.0 Oh, oh, oh.4371.04382.0 Oh, oh, oh.4401.04401.0 Oh, oh, oh.4420.04420.0 Oh, oh, oh.4439.04439.0 Oh, oh, oh.4458.04458.0 Oh, oh, oh.4477.04477.0 Oh, oh, oh.4496.04496.0 Oh, oh, oh.4515.04515.0 Oh, oh, oh.4534.04534.0 Oh, oh, oh.4553.04553.0 Oh, oh, oh.4572.04572.0 Oh, oh, oh.4591.04591.0 Oh, oh, oh.4610.04610.0 Oh, oh, oh.4629.04629.0 Oh, oh, oh.4648.04648.0 Oh, oh, oh.4667.04667.0 Oh, oh, oh.4686.04686.0 Oh, oh, oh.4705.04705.0 Oh, oh, oh.4724.04724.0 Oh, oh, oh.4743.04743.0 Oh, oh, oh.4762.04762.0 Oh, oh, oh.4781.04781.0 Oh, oh, oh.4800.04800.0 Oh, oh, oh.4819.04819.0 Oh, oh, oh.4844.04844.0 Oh, oh, oh.4863.04863.0 Oh, oh, oh.4882.04882.0 Oh, oh, oh.4901.04901.0 Oh, oh, oh.4920.04920.0 Oh, oh, oh.4939.04939.0 Oh, oh, oh.4958.04958.0 Oh, oh, oh.4977.04977.0 Oh, oh, oh.4996.04996.0 Oh, oh, oh.5023.05026.0 Thank you.5031.0\"}"}