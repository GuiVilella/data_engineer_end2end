{"data": "{\"value\":\"0.0 Hi there. Today we'll look at Bertology meets Biology, Interpreting Attention in5.925.92 Protein Language Models by Jesse Wigg, Ali Madani, Lav R. Varshney, Kaiming Xiong,12.04000000000000112.040000000000001 Richard Zucker and Nazneen Fatima Rajani. This paper is a investigative paper into20.0420.04 models that are trained on biological data, specifically into BERT models,25.6425.64 actually into a one specific BERT model that is trained on protein sequences. Now32.7632.76 it is trained to simply perform language modeling on these protein sequences, but39.48000000000000439.480000000000004 out of this language model you can then inspect this BERT model and read45.845.8 important biological data of these proteins, higher order data from the51.851.8 attention heads of the BERT model, which is pretty interesting. Basically means56.1656.16 that the information of these higher order functions is at some point encoded62.062.0 in the structure of the language of the protein sequence. So we're going to go68.2468.24 through what this means and how this comes about and what they did in order74.274.2 to investigate. I think this is a pretty cool investigative work and probably79.5679.56 very promising for future research. Yeah, as always if you like content like87.7287.72 this, consider sharing it out and leaving a like. Also tell me what you think in93.4493.44 the comments. So biology. Really quick for people who maybe have never heard101.96000000000001101.96000000000001 this. In your every cell you have this thing called DNA, which basically is a109.04109.04 an encoding of all of your biological functions. Now usually biological114.88000000000001114.88000000000001 functions are realized through proteins. So DNA is basically a building plan for120.84120.84 all of your proteins. This happens in the following two steps. First there is126.0126.0 this transcription step where RNA is built. This is basically a copy of your132.08132.08 DNA, but is only single strand as you can see right here. And then there is a137.12137.12 translation step that finally translates the RNA into the protein. What will end143.8143.8 up is just a sequence of these beads right here. Now these beads are what are149.54149.54 called amino acids. So a protein is simply a chain of these amino acids.155.12155.12 There are 20 different amino acids and the order of these amino acids in the161.62161.62 chain makes the function of the protein. Now specifically we know about these167.0167.0 proteins that it seems to be very important how their three-dimensional172.68172.68 shape is. So a lot of these different amino acids have different chemical177.52177.52 properties. Some are sort of I think negatively charged, some are neutral, some184.68184.68 are acids and so on. So they have very different chemical properties. So once189.16189.16 you build this protein and you kind of release it into the cell it will curl up194.8194.8 into a three-dimensional structure. So this one might be you know doing198.8198.8 something like this and sort of form a circle or something like this.205.56205.56 Just because these proteins here they kind of attract each other maybe210.88000000000002210.88000000000002 electrically and thus the protein forms a circle and the function of the protein216.60000000000002216.60000000000002 is very much related to its shape. So if it is a circle it could maybe you know221.92000000000002221.92 trap something else in here. So you really have to think of these things226.0226.0 like kind of tools. There are proteins that cut other proteins and they are231.07999999999998231.07999999999998 really shaped sort of like a scissor that exactly fits these other proteins237.95999999999998237.95999999999998 such that you can effectively cut them. So sometimes you can substitute an245.2245.2 amino acid for a different amino acid like this here. If it doesn't change the250.64250.64 shape very often you're fine. The protein function isn't changed.257.47999999999996257.47999999999996 But if you change a different amino acid that is sort of vital to the shape and262.64262.64 the shape changes then your protein very often loses function. So mutations in DNA272.0272.0 sometimes lead to mutations in protein. Not always because there is some278.03999999999996278.04 redundancy in this translation step from RNA. But if they do lead to a282.72282.72 different amino acid it doesn't actually mean that the function changes. So there288.12288.12 is sort of value in analyzing the sequence of the structure of proteins294.84000000000003294.84000000000003 rather than the structure of DNA. Of course it's also important to analyze298.64000000000004298.64000000000004 the structure of DNA but it is also it is equally important to analyze the304.56304.56 structure of proteins because they not all the information is in the sequence.311.44311.44 Not all the obvious information is in the sequence. So what does this paper do?318.24318.24 This paper goes and takes a model that has been trained on protein data. So if324.76324.76 you look at this protein it is simply a sequence of amino acids and these amino329.84000000000003329.84 acids they all have names. I think I have a table somewhere here. Yes. So these are335.76335.76 the different amino acids that exist and you can see a protein is simply a343.35999999999996343.35999999999996 sequence of these names. So usually they're abbreviated by like a three348.67999999999995348.67999999999995 letter abbreviation or just a one letter abbreviation. So a protein might be A, V,354.28354.28 M, M, V, A, G and so on. And this is just a string of text right? So what I can do364.0364.0 is I can train a language model on this. A language model is simply a model that369.2369.2 takes a piece of text and tells you what's the next piece of text. So what's375.08375.08 the next letter? What's the next word? In this case what's the next amino acid?379.15999999999997379.16 And we can use tools from NLP for that. Specifically we can train a BERT model.386.08000000000004386.08000000000004 Now BERT works a bit differently than a standard language model. BERT does390.84000000000003390.84000000000003 what is called masked language modeling. So you would take this string you would395.0395.0 feed it into a BERT model right here. And I've made an entire video on BERT if400.72400.72 you want to check that out. And what you'll do by inputting that you'll mask405.96000000000004405.96 out some of the tokens. So you'll maybe mask out this one, mask out this one and410.2410.2 then you ask the model to reconstruct those. You say that here is an M and here415.71999999999997415.71999999999997 is an A without seeing them. So the model somehow has to learn from the419.96419.96 surrounding amino acids what this amino acid could be right? So it has to427.67999999999995427.67999999999995 reconstruct this sequence. So the hope here is in natural language is that BERT434.44434.44 somehow learns something about language itself. By being able to reconstruct440.88440.88 these things it has learned something about language about which words appear445.0445.0 together and when. It might even learn very long distance relationships between449.88449.88 words just because it has to predict those. And the idea carries over to457.0457.0 biology. So we might hope that a BERT trained on an amino acid sequence will465.0465.0 learn something about the language of proteins about the amino471.12471.12 acid sequence. And our goal here is to be to ask can we somehow infer the 3D479.04479.04 shape of a protein which is the important part from its sequence right484.32484.32 here. So given its sequence can we infer the 3D shape? Now as I understand it491.44491.44 usually this has to be done in like a simulation. So you would you would build496.15999999999997496.15999999999997 this in a simulator and then you do like some sort of a molecule simulation to502.44502.44 see how this ends up in a 3D shape. You could train a model to just predict the507.84507.84 3D shape but in this case we're just interested what does the BERT model learn512.36512.36 about the 3D shape while only ever having been trained on predicting the518.8000000000001518.8000000000001 next or predicting the sequence of amino acids. So it's never been trained to to524.28524.28 look at the 3D shape. And that's our goal here. So specifically we'll look at two530.16530.16 different things. So here you can see examples of proteins and their high534.6534.6 level structure. So in in these proteins what you call the primary structure is540.2540.2 this sequence of amino acid. This is simply which amino acids are in in545.72545.72 which order. There is a thing called the secondary structures and we often551.6800000000001551.6800000000001 observe that spans of these amino acids like substrings form these what are558.2558.2 called these helixes as you can see here or these sheets. I don't know how they're565.2565.2 strands in English. We call them sheets or I think these are the alpha helixes571.24571.24 and these are the beta sheets and there is also a turns. I think this here might576.48576.48 be a turn. So there are these kind of secondary structures and then the582.1600000000001582.1600000000001 tertiary structure is how these this is still one this is one protein. This is587.76587.76 one unbroken chain of amino acid and you can see this here kind of forms this592.2592.2 double ring which would be its tertiary structure. Very important for597.9200000000001597.9200000000001 predicting the tertiary structure is to predict when two amino acids are close604.6400000000001604.6400000000001 to each other. So if we have a chain right here and the chain as we saw609.96609.96 before kind of turns and bends on itself then these two amino acids here are very615.6615.6 close in close contact and to predict which amino acids are in close contact622.8000000000001622.8000000000001 to each other helps you determine the the tertiary structure. So that's a628.5600000000001628.5600000000001 consequence of it. So we wonder does BERT know intrinsically which of these636.6636.6 amino acids are going to end up being in contact with each other without ever640.8000000000001640.8 having been trained to do it. The second thing we're interested in are binding646.24646.24 sites. So here, you might not be able to see, but we made this example before652.64652.64 where this sort of forms a loop and then I say can trap something here right like657.5999999999999657.5999999999999 another molecule and this is what what we would call a binding site. A binding663.1999999999999663.2 site is a one amino acid that maybe through the structure of the surrounding671.5200000000001671.5200000000001 amino acid as well but also through its properties and how it is exposed in 3D676.8000000000001676.8000000000001 shape acts as sort of a receptor for other molecules. It binds to other things.683.8000000000001683.8000000000001 It can, so think of your hemoglobin that that traps the the oxygen in your blood691.0400000000001691.04 or something like this. It is where a chemical reaction or a reaction with698.8399999999999698.8399999999999 something else will happen. That's a binding site and we are interested does703.7199999999999703.7199999999999 BERT, the BERT that is only trained on a language modeling objective know which710.5999999999999710.5999999999999 ones are the binding sites because you know that would be very interesting and716.16716.16 not something BERT was trained on. By the way, I particularly liked Richard721.1999999999999721.1999999999999 Soccer's tweet on this. I think he tweeted out, oh BERT trained only on727.24727.24 language model can predict binding sites and biological properties and formulated732.48732.48 it like it was you know like GPT-3 was formulated like if we train on Wikipedia737.6737.6 our model can do math. I thought it was kind of a satire headline. If we train on743.88743.88 Wikipedia our model can predict biology and also it can tie your shoes and cook749.16749.16 your dinner. Yeah but so it's trained on language modeling on biological data and755.64755.64 now that makes sense. So they're going to look at two different things or actually762.64762.64 more than two different things but they formulate this in an abstract way right767.96767.96 here. So what they'll look at is the so-called properties. The property F and775.52775.52 this property F can be for example that a amino acid is a binding site. The783.36783.36 property F can also be that two amino acids are in contact with each other. So789.2800000000001789.2800000000001 F always takes I and J. If in the case for example where this is the contact796.1600000000001796.16 property then it simply is the indicator function for when I and J are in802.92802.92 contact. And if it is a just a binding site then I think we are looking at J. At814.88814.88 the token level property we define to be an indicator that returns 1 if the819.3199999999999819.32 property is present in token J. Okay so whenever J is a binding site then that826.5200000000001826.5200000000001 holds. So what we're looking at are these attention heads in BERT. If you don't831.96831.96 know BERT has an attention mechanism which basically means from layer to836.7600000000001836.7600000000001 layer each token can attend to all other tokens. So here the amino acid sequence843.08843.08 I've drawn it twice and the next layer representation of this amino acid will848.84848.84 be able to gather information from all of the other amino acid through an853.5600000000001853.5600000000001 attention mechanism through a dynamic routing algorithm. I've made a video on858.2858.2 attention is all you need if you want to find out more how this works. Now what863.8000000000001863.8000000000001 we're interested in is the strength of these connections. So the hypothesis is870.9200000000001870.92 if molecule here is 1, 2, 3, 4, 5, and 6. If molecule 1 and 3 are contact sites881.04881.04 then maybe we will find a layer where this connection between 1 and 3 is very890.12890.12 strong. That would indicate that there is a connection site or that would896.12896.12 indicate that BERT has learned something about the connection sites. Okay if we901.5600000000001901.5600000000001 find this repeatedly so if we look at many many proteins and whenever we know906.88906.88 that there is a contact between two things and then we observe that the911.8911.8 corresponding attention is very high then we can be pretty sure that BERT has918.28918.28 learned something about contact between amino acids. Okay the same goes for924.6924.6 binding sites. So if 4 here is a binding site and then all the932.44932.44 connections all the attention that the higher layer gets from 4 so all the938.36938.36 information routed away from 4 is very strong that means all these other tokens943.32943.32 are paying special attention to the token number 4 to this amino acid. And948.4948.4 if we find that there is a big correlation with this being a binding site then we955.0955.0 can reasonably conclude that BERT has learned something about binding sites.960.24960.24 Alright so we're going to do a correlative analysis for proteins where965.0799999999999965.0799999999999 we know the binding sites where we know the contacts right we can analyze them970.36970.36 we can run simulations therefore we can know them. So we're going to look at this977.6977.6 quantity right here which is simply a normalized quantity. So we're going to981.5600000000001981.5600000000001 look at the attention in a given attention head so as you know BERT has986.9200000000001986.9200000000001 many layers with many attention heads and we're going to look at whether or992.76992.76 not this property is active and just normalize it by the total attention in997.6800000000001997.6800000000001 that head so that we get some kind of a percentage number. That's the first task1003.361003.36 we're basically going to look at how does the attention correlate with these1007.361007.36 properties. And the second task we're going to do is this probing task. So a1013.521013.52 probing task is like a linear probe in like a classifier. So what we're going to1019.361019.36 do is we're going to take a layer right here and even though it's an1026.641026.64 intermediate layer we're simply going to run it through a linear classifier and1031.281031.28 then decide is this a binding site for example or not. Is a given1037.63999999999991037.6399999999999 amino acid a binding site or not? Is a given pair a contact or not? So this is1044.841044.84 kind of a linear probe but this sort of takes a backseat in this paper. The1049.61049.6 analysis is really on the attention heads and what the attention heads learn.1053.63999999999991053.6399999999999 And that's already it. They take a pre-trained BERT model so there are1059.81059.8 these BERT models that are already trained on these protein databases and1064.81064.8 first they look simply can we find attention heads that correlate with a1072.441072.44 given amino acid. So here you see the attention to the amino acid. This is1079.61079.6 proline I believe and this is phenylalanine. Is that the same in1085.761085.76 English? Yes, phenylalanine and proline right here. So you can see that1095.481095.48 the plots here are there's almost no attention pretty much throughout the1104.681104.68 network that pays special attention to the amino acid proline except this head1111.21111.2 right here seems to have if you look at the scale over like a 70% of attention1117.161117.16 always goes to proline in this particular head. So this is layer 1 head1122.36000000000011122.3600000000001 number 11 focuses 78% of its attention on proline. Now this is not that special1133.881133.88 if you think about it because in language models as well in natural1137.721137.72 language models you might want to think that you have some mechanism in your1142.481142.48 neural network that's especially specialized on like a very particular1146.241146.24 word in the language because that might just be a often occurring very1151.321151.32 particular word. For example in English maybe the is very important or the word1158.281158.28 what these are like very indicative very often occurring words so it is1164.841164.84 reasonable to expect to find an attention head that pays a lot of1168.561168.56 attention to these things especially here where our vocabulary size is 201173.19999999999981173.1999999999998 instead of like 30,000 in natural language. And the same goes for this1179.95999999999981179.9599999999998 phenylalanine where you can see that in the last layer and in the1186.521186.52 first layer you have attention and also in the proline you have in the last1189.95999999999981189.9599999999998 layer. So why does this make sense? Because what we would expect from like1194.39999999999991194.4 single tokens these are not interactions yet these are not biological functions1198.681198.68 yet. So we know that in the lower layers of a neural network we have these kind1204.641204.64 of basic features basic feature extractors and here these basic feature1209.521209.52 extractors appear to be simply paying attention to one specific token in the1216.521216.52 vocabulary a lot. So they kind of these heads sort of specialize for single1222.161222.16 for single amino acids. And the same in the last layer so in the very last layer1228.161228.16 the task of the very last layer is to prepare for the classification task. So1235.881235.88 if you remember the BERT model you have layer layer layer layer and at the1241.21241.2 end you'll have to predict which ones are masked down here. So at the end you1246.161246.16 have to predict single amino acids again so if there's a proline masked here1252.08000000000021252.08 you'll have to predict the proline. So it also makes sense that the last layers1256.87999999999991256.8799999999999 would very much specialize to single tokens. So this does make sense. Now our1266.63999999999991266.6399999999999 question is going to be do we find the biological function where would you1271.841271.84 expect them? We would expect the let's say the tertiary, sorry the secondary1276.841276.84 structures which are sort of one level higher than the primary structures we1281.241281.24 would expect to find them maybe here and then we would expect to find the1285.481285.48 tertiary structures maybe somewhere here okay because these are most highest1291.01291.0 level. And then it goes it goes back again or maybe it's like we find the1297.321297.32 tertiary structures rather here and here again and then in the middle we'll find1304.641304.64 the the most high level the tertiary structures or yeah blue secondary. This1310.161310.16 drawing is getting too too too weird but there could be multiple scenarios but1317.281317.28 that could fit here but until now it sort of makes sense. So they do an1323.60000000000011323.6000000000001 additional investigation where as I told you sometimes you can substitute an1328.681328.68 amino acid and nothing really happens right and in fact that this probably1334.641334.64 happens in you right now you probably might have some mutation that changed1340.01340.0 some amino acid and you don't even realize because it's just it's fine. No1347.161347.16 notice. So the biologists can build these matrices of how much you can substitute1354.161354.16 proteins with each other. So here you see this BLOSUM62 substitution scores which1360.921360.92 are very I guess very high if you can substitute two protein, two amino acids1367.761367.76 with each other and the effect is negligible. And it's very low if it's the1376.321376.32 the other way around. Now this is interesting so far but you compare this1381.61381.6 to this matrix right here. This is the attention similarity. So what we'll do is1387.21387.2 for each two amino acids we take those two attention things, those two attention1393.761393.76 matrices and we'll calculate the correlation between the attention1397.87999999999991397.8799999999999 matrices. And our hypothesis is that the more correlated the attention patterns1404.321404.32 are between the two amino acids the more likely we are to substitute them1409.81409.8 because as a direct result of our language model our language model is1416.21416.2 it's reconstructing right these things. So our language model is going to treat1424.84000000000011424.8400000000001 if in natural language is like a a synonym right is our language model is1431.161431.16 going to treat synonyms very similar to each other because they're synonyms they1435.681435.68 can be exchanged. So a good language model should learn that they are almost1440.761440.76 the same and therefore the attention pattern is going to be almost the same.1444.84000000000011444.84 So a high correlation we hypothesize is a means that the function of the amino1453.521453.52 acid is similar and therefore we can substitute it easily. So this here is the1459.43999999999981459.4399999999998 matrix of the correlations between each two attention patterns of these amino1465.43999999999981465.4399999999998 acid. And if you compare the two right here they look extremely similar. Just1473.081473.08 have a look for a little while and you'll see that the patterns they do not1479.121479.12 match perfectly but they are very very similar. The dark spots are in the same1485.281485.28 places the light spots are in the same places. So this already makes a good case1491.281491.28 that the language model here has learned something about biology. Now what we want1498.041498.04 to do is we want to investigate higher order higher order functions. So here1505.961506.24 we're interested in these contact maps right. So how likely is it that two1513.761513.76 amino acids are in contact and we'll look at it through the lens of attention1518.63999999999991518.6399999999999 as we did before. So here is the percentage of each head of each head's1523.961523.96 attention that is aligned with contact maps averaged over a data set suggesting1530.01530.0 that head 12 4 is uniquely specialized for contact prediction. So look at this1533.84000000000011533.8400000000001 this this head here is just spiking. So remember before we said our analysis is1544.161544.16 whenever whenever we're basically measuring the correlation of two things1551.441551.44 being in contact because we know it from our simulator or from our data set the1556.961556.96 correlation of that with an attention connection being particularly strong. And1562.721562.72 we find it in this attention head right here. So this layer 12 head number four1571.241571.24 will always peek out whenever two things are in contact. Now you can see that it's1577.01577.0 it's not like always it's like 25 percent of its attention but significantly more1582.841582.84 than anything else right here. In fact if you group the things by this attention1589.841589.84 you can build the following plot. So you can see right here probability two1594.881594.88 amino acids are in contact as a function of attention between the amino acids in1600.681600.68 head 12 4 showing attention approximates perfectly calibrated estimator which1606.721606.72 would be the green line. So here we simply for each for each pairs two1612.641612.64 amino acids for each pair of amino acids we plot we make a histogram right here1620.241620.24 of what they're sorry not a histogram we plot the probability if they have the1630.081630.08 attention weight point nine we plot how likely is it that they are in contact. So1637.761637.76 this is this if we just look at the data and we simply take this attention weight1643.041643.04 as a measure as a predictor of being in contact we get the blue curve and the1648.39999999999991648.3999999999999 green curve would be if we could perfectly predict from this attention1654.281654.28 head what the probability of contact would be and you can see that the fit is1658.67999999999981658.68 fairly good you can't predict with super high accuracy but the fit is fairly good1665.36000000000011665.3600000000001 and you can see the general trend that as the attention in this head rises the1672.56000000000021672.5600000000002 probability of the two amino acids being in contact with each other also rises. So1680.761680.76 we can sort of confidently say that BERT has learned something about a higher1687.12000000000011687.12 level biological structure just from the language modeling1692.23999999999981692.2399999999998 objective. How can we interpret this? This must somehow mean that it is1699.841699.84 possible it is vital for reconstructing the sequence from its1708.01708.0 surrounding so if we delete this right here if these two are in1715.841715.84 contact in the 3D structure that makes probably probably means that this thing1723.561723.56 right here is a very good predictor of what was here. If we mask this out and1728.841728.84 we're asked to reconstruct which amino acid was there then it probably helps to1733.15999999999991733.1599999999999 look at its neighbors right it probably always helps to look at one's neighbors1736.91999999999981736.9199999999998 especially also in natural language but if these two are in contact they have1744.81744.8 very special connection to each other it's very you can basically1750.321750.32 read out from this one which one this was. This is sort of like if you have a1758.21758.2 sentence and you say1762.01762.0 I don't know I can't come up with one right now but if it's like da da da da da and then there is a name like1776.01776.0 Mark and then da da da da da da da and then there is him right and you would expect if I drop out1784.641784.64 if I drop out him then from the text right here you can probably determine that it is some sort of1794.241794.24 pronoun but then you go back and you see it's Mark okay so it's not like it's not like it or she it's probably he or him this is sort of the analogous structure right here in biology.1815.81815.8 The second thing we're looking at is these binding sites. Now these are single properties of different amino acids and we're simply looking at all the incoming or sorry all the other tokens that focuses their attention.1832.39999999999991832.3999999999999 Why is this important? Because these binding sites are central to the structure of the or to the function of the protein right.1841.481841.48 If this here is a binding site then that's a very central important point of the protein.1847.681847.68 So a lot of these other things are going to be determined by what this binding site is.1855.081855.08 This binding site needs to have a very particular function and therefore probably needs to be a very particular amino acid.1863.01863.0 And the other things here are sort of supporting this binding site because they form the 3D structure around it and so on.1869.521869.52 So you would expect a lot of attention to be put on this binding site.1876.521876.52 And what do we find? We find that it's a bit more murky than before.1884.081884.08 So you can see that the attention is kind of spread out.1887.561887.56 Percentage of each head's attention that focuses on binding sites especially in the deeper layers binding sites are targeted at much higher frequency than would occur by chance.1897.681897.68 Head 7-1 has the highest percentage with 34%.1902.921902.92 So also here you can see that it is spread out but this is because multiple heads are now focusing on these binding sites because probably binding sites come in different variations.1915.08000000000021915.0800000000002 So you'll have lots of heads specializing on attending to binding sites and they say it is much higher frequency than would occur by chance.1924.041924.04 And you can see here this head is the highest with 34% of its attention focused on binding sites.1931.721931.72 You can also see the general trend of the attention being rather in the later layers which we would expect from a tertiary structure.1941.121941.12 Now yeah it would be interesting here.1945.521945.52 Here you also see that actually most of the things are in the last layer which points to rather maybe lower level information because we reasoned before about the last layer or I was just wrong.1960.41960.4 But also in a general trend you can see that the attention is rather shifted towards the later layers because this is sort of a higher order function.1972.41972.4 If you look at the same calibration experiment you can see that the picture is not as clear.1980.881980.88 There is a general trend at the beginning but then it sort of flattens out.1984.761984.76 So you can sort of differentiate the very probably not a binding site from the somewhat probably a binding site but it's not a perfectly calibrated classifier.1997.41997.4 And that might just be because there are many things specializing in different types of binding sites.2003.882003.88 So you can't just go to this one head. So this is just for this one head.2008.962008.96 You can't just go to that one and expect that to classify all the binding sites because you might want to be you might want to combine all of the high ranking ones here to form a classifier.2025.682025.68 The last experiment they do is these linear probes which where they just go and they just build classifiers from different parts of the network.2034.722034.72 And you can see right here that what is predicted and how well they work.2039.882039.88 So each bar here is going to be the difference of performance.2043.962043.96 So this is differential performance of diagnostic classifier by layer sorted by task order in figure eight.2051.282051.28 Each plot shows the change in performance between the given layer and the previous layer.2056.842056.84 OK so a bar up shows it's performing better than the previous layer bar down shows it's performing worse than the previous layer.2065.62000000000032065.6200000000003 So you see right here that the these are the secondary structures right here and you can see that there is a lot of performance in the earlier layers right here and sort of not that high performance in the later layers.2080.74000000000022080.74 Whereas for the tertiary structures the binding site and the contact you can see that there is a bit of performance in places but it sort of tends to be more towards the middle certainly more at towards the middle of the end of the network than the the secondary structures which sort of makes sense with our hypothesis.2104.642104.64 You can also see this here where they show the percent of attention focused as a function of layer and the red is the center of mass and you can see that as the the secondary structures this their center of mass is at a lower layer in general than the tertiary functions.2127.742127.74 All of this is not perfect of course but it's still an open question I guess whether or not it's not perfect because we haven't built a strong enough language model yet.2140.582140.58 Do I want to say GPT-4 is now for biology and not for language or is it because there is really you need you really can't very well predict the these things just from a language model.2156.52156.5 I mean you should technically all the information is there but maybe the language model objective as such isn't able to capture that information.2167.92167.9 So yeah this this was the paper it's pretty simple they have in the appendix they have a lot of a lot of these additional experiments or full experiments I believe for all the amino acids and so on.2179.422179.42 And I invite you to check that out in general I like this kind of work because it's very applied it's and it can you know tell us something about the nature of both these language models and the biological things that we that we care about in biology.2201.142201.14 OK I'm just talking crap right now.2205.74000000000022205.7400000000002 Thanks for being here.2206.782206.78 I hope you enjoyed it.2207.92207.9 And bye bye.2209.34\"}"}