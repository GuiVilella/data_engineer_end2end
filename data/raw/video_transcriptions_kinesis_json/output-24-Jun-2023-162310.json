{"data": "{\"value\":\"0.0 Hi everyone, I'm Patrick and in today's video we are going to learn how to get started with Hugging4.84.8 Face and the Transformers library. The Hugging Face Transformers library is probably the most10.3210.32 popular NLP library in Python right now and it can be combined directly with PyTorch or TensorFlow.17.1217.12 It provides state-of-the-art natural language processing models and has a very clean API that22.822.8 makes it extremely simple to build powerful NLP pipelines. So today we have a first look at the28.40000000000000228.4 library and build a sentiment classification algorithm. I show you some basic functions and34.034.0 then we have a look at the model hub and then I also show you how you can fine tune your own model.39.1239.12 So let's get started. Alright, so to get started you should either install PyTorch or TensorFlow45.7645.76 first and then in order to install the Transformers library you just have to say pip install52.5652.56 transformers or there's also a conda installation command that you can find on the installation page.60.08000000000000560.080000000000005 So let's install it like this. So I already did this and then we can start using this so we can67.267.2 say from transformers and then we import a pipeline as first thing and have a look at this. And then75.4475.44 we also import some utilities that we need from the PyTorch library. So we import torch and we83.4483.44 import torch dot nn dot functional sf so we are going to use this later. And now we can start92.6492.64 using this pipeline. So let's say classifier equals and then we create a pipeline and we need101.2101.2 to specify the task that we want. So in this case, we want to do sentiment analysis. So we have to109.92109.92 call it like this. And you will find the different available tasks on the website. So here we can see119.92119.92 for example, we have this sentiment analysis, which is just an alias of text classification.127.92127.92 But for example, we also have a question answering pipeline or a text generation or a conversational135.6135.6 pipeline. So yeah, this is how we can define a pipeline. And what a pipeline does is that it142.64000000000001142.64000000000001 gives you a great and easy way to use model for inference. And it abstracts a lot of the things149.76149.76 for you. So you will see what I mean in a moment. So now we can just use this classifier and157.44157.44 classify some text by saying res for results equals and then we call this classifier and we166.24166.24 want to classify a example text. So let me copy and paste some example text for you. So we want174.64174.64 to classify we are very happy to show you the smiley face transformers library. And then let's181.76181.76 print the result and see how this looks like. So let's run the code. Alright, and as you can see,188.79999999999998188.79999999999998 we get the label is positive and the score is 0.99. So it's very confident that this is a195.51999999999998195.51999999999998 positive sentence. And as you can see, it only takes two lines of code with this pipeline to202.0202.0 create a sentiment analysis code. So yeah, this is exactly what we need. So we need to see the209.28209.28 label of the text if it's negative or positive. And we also get the score. So yeah, this is really216.0216.0 nice. And now let's have a look at some more things that we can do with this pipeline. So first222.16222.16 of all, we can put in more texts at once. So we can not just use one so we can give it a list. So230.16230.16 let's for example, use a list and then let's use another example text. So let me copy and paste238.32238.32 this one in here as well. So we also want to classify this we hope you don't hate it. And then245.84245.84 we get multiple results back. So let's call this results. And then we can iterate over this. So we252.56252.56 can say for results in results, and then we want to print the result. And now let's run this code262.48262.48 and have a look at how this looks like. Alright, and as you can see, for the second text, we get268.08268.08 another result back. So here the label is negative, and the score is maybe not that confident in this274.96274.96 case. So this text might be a little bit confusing, we hope you don't hate it. But basically, this is281.84281.84 how you can pass in multiple texts at once. And now so right now we only use the default pipeline289.84289.84 with the default model. But now let's have a look at how we can use a concrete model. And then also296.79999999999995296.8 how you can use a concrete tokenizer. So what we can do is we can specify the model name and say306.16306.72 model name equals and in this case, I use this tilt bird base on case and then fine tuned ssd to315.52315.52 English. So I will show you where I got this string or this name in a moment. But for now,323.28000000000003323.28 yeah, this is basically just a distal bird model, which is a smaller and faster version of bird,331.52331.52 but it was pre trained on the same corpus. And then you see that it also was fine tuned. And337.35999999999996337.35999999999996 this is just the name of the data set. So in this case, it's an English data set from the342.64343.2 Stanford sentiment treebank version two. And yeah, so now if we have the model name, we can349.76349.76 give this to our pipeline with the model argument. So we can say model equals and then we use this357.36357.36 model name. So now in this case, I can tell you that the default model for the sentiment analysis365.68365.68 task is already this model name. So this should do exactly the same. But later, we will switch this373.52373.52 and then have a look at how we can use different models. So first of all, let's run this again and379.44379.44 see that this is still the same. Alright, so we see this is still the same result. So this worked.385.6385.6 So now we just use this string to define our model. But now let's have a different approach394.32394.32 to define a model and then also a tokenizer. So this will give us a little bit more flexibility400.96400.96 later. So in order to do this, we want to say from transformers, and then here I import409.04409.04 a auto tokenizer class and auto model for sequence classification. And this is just a generic class420.96000000000004420.96000000000004 for a tokenizer. And this is also a generic class, but a little bit more specifics. In this case,428.08000000000004428.08000000000004 I want to have it for sequence classification. And then it will give me a little bit more433.92433.92 functionality specifically for this task. So don't worry about this right now, you can also find all441.12441.12 the model classes available in the documentation. So if you're interested, then have a look at this.447.12447.12 And also, if you use TensorFlow, then here you have to say tf, and then the name of this class,455.04455.04 but the rest is actually the same. So yeah, this is how you use TensorFlow. And now after importing462.88462.88 this, we can create two instances of this. So we can do we can say model equals and then we use473.04473.04 this class. So auto model for sequence classification. And then we use a function480.32480.32 that is called so let's say dot from pre trained, and then it also needs the model name. And we do488.56488.56 the same with the tokenizer. So we say tokenizer equals the auto tokenizer dot from pre trained,497.92497.92 and then it needs the model name. So this dot from pre trained function is a very important506.56506.56 function in hacking phase that you will see a lot. So you will see this later a few more times.512.24512.24 So now that we created this, we can also just give the actual model and not just the string to521.6521.6 the classifier or to the pipeline. So we can say our model equals our model and our tokenizer530.96530.96 equals our tokenizer. So now if we run this, we should still get the same results because these538.8538.8 are the default versions. And yeah, as we see, we still get the same result. But then later,544.64545.5999999999999 if you want to use a different model or tokenizer, then you know how you can switch this.551.1999999999999551.1999999999999 So just by using a different model and tokenizer here for the pipeline. So now instead of using557.1999999999999557.1999999999999 this pipeline, let's see how we can use this model and tokenize directly and do some of the565.1999999999999565.2 steps manually. This will give you a little bit more flexibility. So down here, let's first use573.2800000000001573.2800000000001 the tokenizer and see what this does. So first, let's call the tokenizer dot tokenize function.584.6400000000001584.6400000000001 So we say let's call this tokens and then equals tokenizer dot tokenize, and then the string or594.08594.08 the sentence we want to tokenize. So let's copy and paste this in here. And then once we get the601.5200000000001601.5200000000001 tokens, we can use them and get the token IDs out of it. So we can say token IDs equals and then we611.36611.36 again use the tokenizer and the function convert tokenizer to it's called IDs. And then it needs621.76621.76 the tokens. So this is one way how to do this. Or we can do this directly by saying token IDs equals,633.36633.92 and then we call this tokenizer like a function. And then again, we give it the same string here.642.96642.96 So now let's print all these three variables to see where is the difference. So first, we print651.84651.84 the tokens, then we print the token IDs. And then here, let's actually give this a different name.660.4000000000001660.4000000000001 So let's call this input IDs. So now let's run this and see how this looks like. Alright, so here's669.2800000000001669.28 the result. So as you can see, when we call tokenizer dot tokenize, then we get a list of678.16678.16 strings or the list of the words back. So now each word is a sorry, each word is a separate token.688.88688.88 And for example, this one is our smiley face or our emoji. So yeah, this is what the tokenize697.28697.28 function does. And then once we call this convert tokens to IDs, we get this one back. So now it706.9599999999999706.9599999999999 converted each token to an ID. So each word has a very unique ID. And this is basically the716.72716.72 mathematical representation or the numerical representation that our model then can understand.723.36723.36 So this is what we get after this function. And if we call this tokenizer directly, then we get a731.04731.04 dictionary back. And here we have the key input IDs. And we also have the attention mask. So for739.2739.2 now, you don't really have to worry about this. But let's have a look at the input IDs. So if we745.92745.92 compare the token IDs with the input IDs, then we see we have the exact same sequence of token IDs.755.68756.24 But we also have this one on one and one or two token. And this is just the beginning of string763.92763.92 and the end of string token. But basically, it's the same. So yeah, this is the difference between770.64770.64 these three functions. And then these input IDs, this is what we can pass to our model later to do779.36779.36 the predictions manually. So now like before, we can also use multiple sentences, of course,786.96786.96 to for our tokenizer. So for example, usually in your code, you have your training data. So let's794.64794.64 say x train. And in this example, let's just use these two sentences. So this is our x train. And803.6803.6 then we can and then we can pass this to our tokenizer. And let's call this batch. So this is812.24812.24 our batch that we put into our model later. So we say batch equals tokenizer. And then we call this820.3199999999999820.32 tokenizer directly with our training data. And then I also want to show you some useful arguments.828.0828.0 So we say padding equals true. And we also say truncation equals true. And then we say max length838.5600000000001838.56 equals 412. And we say return tensors equals and then as a string p t for pytorch. So this will850.7199999999999850.7199999999999 ensure that all of our samples in our batch have the same length. So it will apply padding and858.0799999999999858.0799999999999 truncation if necessary. And this is also important. So in this case, we want to have a865.1999999999999865.2 pytorch tensor returned directly. So I will show you later what's the difference if you don't use872.32872.32 this. But for now, let's just use this. And then first of all, let's print this batch and see how881.0400000000001881.0400000000001 this looks like. And then we see we get a dictionary. And again, it has the key input IDs889.9200000000001889.92 and the key attention mask. And then here it has two tensors. So the first one for the first898.3199999999999898.3199999999999 sentence and the second one for the second sentence. And the same for the attention mask. So904.4799999999999904.4799999999999 two tensors. So yeah, as I said, these input IDs are these unique IDs that our model can understand.911.76911.76 So yeah, now we have this batch. And now we can pass this to our model. So and let's do this920.96920.96 manually and see how we can call our model. So in pytorch, when we do inference, we also want to say927.4399999999999928.08 with torch dot no grad. So this will disable the gradient tracking. I explained this in936.08936.08 a lot of my tutorials. So you can just have a look at them if you want to learn more about this.940.8940.8 And then we can call our model by saying outputs equals and then we call the model and then here949.4399999999999949.8399999999999 we use two asterisks and then we unpack this batch. So if you remember here, this is a dictionary.961.04961.04 And here basically with this, we just unpack these values in our dictionary. So for TensorFlow,969.04969.04 you don't do this. So you just pass in the batch like this. But for pytorch, you have to unpack975.12975.12 this. And now we get the outputs of our model. So let's print the outputs. And as you might know,982.24982.24 this, these are just the raw values. So to get the actual probabilities and the predictions,989.8399999999999989.8399999999999 we can apply a the softmax. So let's say predictions equals torch, or we also have this998.24998.24 in F dot soft max. And then here, we say all puts dot logits. And we want to do this along1008.161008.16 dimension equals one. And let's also print the predictions. And then let's do one more thing. So1018.241018.24 let's also get the labels labels equals and we just get this by taking the prediction with the1027.281027.28 or the index with the highest probability. So we get this by saying torch dot arc max, and we can1036.63999999999991036.6399999999999 either put in the predictions, or we can put in the outputs and actually don't need this. But just1044.481044.48 for demonstration, let's use the predictions. And then again, dimension equals one. And then let's1052.87999999999991052.88 print the labels as well. And now let's actually do one more thing. So let's convert the labels by1062.01062.0 saying labels equals and then we use list comprehension and call model dot config dot1070.241070.24 id to label and then it needs the actual label ID and then we iterate so we say for label ID in1086.241086.24 labels to list. And now what this does, you will see this when we print this. So we print the1095.21095.2 labels. And now let's actually run this and see if this works. Alright, so this works. So as you can1104.721104.72 see, here we print the output. So these are our output. This is a sequence classifier output. And1115.60000000000011115.6000000000001 as you see, it has the logits argument. So that's why we used all puts dot logit. And then we1124.481124.48 get the actual probabilities. And then to get the labels we used argmax. So this is a tensor with1133.441133.44 the label one and the label zero. And then we converted each label to the actual class name and1142.161142.16 then we get positive and negative. So by the way, this function I think is only dedicated to a1151.441151.44 auto model for sequence classification. For example, if we just use a auto model, then I think1158.721158.72 it won't be available. So that's what these more concrete classes will do for you. It gives you a1166.81166.8 little bit more functionality for the dedicated dedicated task. So we see that the loss is none in1174.32000000000021174.32 none in this case. So if you also want to have a loss that we want to inspect, then we can give the1181.43999999999981182.08 loss or the not the loss, but the labels arguments to our model that it knows how to compute the loss.1191.43999999999981191.4399999999998 So we say labels, and then we create a torch dot tensor by saying torch dot tensor. And then as a1200.561200.56 list, we give it the labels one and zero. And now let's run this again. And then you should see that1208.081208.08 we should see a loss here. And yeah, now here we see the loss. And again, this labels argument is,1216.63999999999991216.6399999999999 I think, special to this auto model for sequence classification. So yeah, this worked. And now if1225.121225.12 we have a careful look at the probabilities. So first of all, we see we get label positive and1234.321234.32 negative. And here for the first one, this is the highest probability. So 9.997. And here for the1244.39999999999991244.3999999999999 second one, this is the largest number. So it took this one and this is 5.30. So if we compare them1253.61253.6 with the results that we got from our pipeline, then we see these are exactly the same numbers.1261.67999999999981262.32 So now you might see what's the difference between a pipeline and using tokenizer and model directly.1270.481270.48 So with the pipeline, we only need two lines of code. And then we actually get what we want. So we1276.39999999999991276.3999999999999 get the label and we get the score we are interested in. So this might be just fine. But1281.67999999999981281.68 then yeah, if you want to do it manually, you can do it like I showed you and you will get the same1286.881286.88 results that you can then use. So yeah, that's how you can use a model and a tokenizer. And yeah,1294.41294.4 so using the model and the tokenizer will be important when you for example, want to1299.84000000000011299.8400000000001 find you in your model. So I will show you roughly how to do this later. But yeah, so this is how you1307.921307.92 use model and tokenizer. And let's just assume we did fine tune our model, then what we can do,1316.56000000000021317.3600000000001 we can say save directory and specify a directory. So let's call the folder saved. And then we can1326.32000000000021326.3200000000002 call tokenizer. And then we can call dot save pre trained and then the location just the save1335.21335.2 directory. And the same with our model. So we can say model dot save, pre trained, save underscore1344.41344.4 pre trained, and then again, the safe directory, and then we can load them in another application,1352.721352.72 for example, tokenizer equals and then again, here we use this auto tokenizer class and then1360.241360.24 the from pre trained and then here, we can give it a directory. So this from pre trained, we can1368.721368.72 either give it a model name, or we can give it this directory. And again, the same for the model,1376.161376.16 so model equals and then we use this auto model for sequence classification, dot from pre trained,1384.961384.96 and then the safe directory. So this should work. And then you should get the exact same model and1391.761391.76 tokenize it back. And yeah, as you might see these model at these dot from pre trained functions are1401.12000000000011401.1200000000001 very important and you will use them a lot of time. Alright, so I think these are the basic1406.81406.8 functions you need to build a pipeline or to apply the model and tokenizer manually. And now let's1414.161414.16 have a look at how we can use a different model. So like here, you can either load this from your1421.60000000000011421.6000000000001 disk if you already have a pre trained model somewhere on your computer. But what you can also1428.56000000000021428.5600000000002 do is you can go to the hugging face model hub. So you can find this at hugging face dot co slash1436.08000000000021436.0800000000002 models. And here we have the model hub and you can search through different models. So for example,1443.84000000000011443.84 you could filter for the tasks. So in this case, we want to do text classification, which is the1451.43999999999981451.4399999999998 same as sentiment analysis. And then it filter is applies this filter. So you can see the most1459.19999999999981459.1999999999998 popular model is already this one. And then we can click on this and get some more information.1465.67999999999981465.68 And as you could see, so this is the exact same model name that we used in our code. So once you've1474.721474.72 decided for a model, you can click here and copy this name and then paste into your code. So let's1482.241482.24 say in this case, we want to use a different model. So in this case, I want to do sentiment1488.01488.0 classification with German sentences. So then of course, I need one that is trained on German. So1497.21497.2 you can filter here so you can search so I can either again search for this tilt bird and see1503.521503.52 what different versions there are available, or let me search for German. And then here,1510.81510.8 let's take the most popular one. So by Oliver gore, and then we see this is a German sentiment1519.681519.68 bird, and then we get more information. And sometimes we also see some example code, which1525.761525.76 is helpful. So yeah, this is nice. And now what we have to do is we want to click here and copy1532.39999999999991532.4 this will just copy the name. And then in our application, let me comment this out. And then1540.881540.88 let's again say model name equals and now I hit paste. So now it pasted this string here. So now1551.12000000000011551.1200000000001 we have this. And now here we can give our model and tokenizer the model name, so model name and1561.04000000000021561.04 model name. And now let's do this for some example texts in German. So let me copy and paste this in1569.21569.2 here. So basically, let me quickly translate this for you. So this says, not a good result. This was1576.241576.24 unfair. This was not good. Not as bad as expected. This was good. And she drives a green car. So1586.15999999999991586.16 basically, these three texts are negative. This one is rather positive, and this is neutral. So1594.881595.52 let's see if our model can detect this correctly. So now again, like above, we do the same steps. So1603.36000000000011603.3600000000001 we could copy and paste this. So let's copy and paste this. And then the same as above, we say1611.921611.92 with torch, torch dots, no grads, and then we call the model. So we say, outputs equals model. And1623.041623.04 then here we unpack our batch, then we have the model, then we want to have the label ID. So let's1631.521631.52 say label IDs equals and then we use the torch dot arc max function with the outputs and along1642.481642.48 dimension equals one and let me remove this one. And then we print the label IDs. So print the1651.041651.04 label IDs. And then we do the same as we do here. So we want to convert them to the actual label1659.361659.36 names by calling model dot config, ID to label label ID for label in here, we call this label1668.71999999999981669.36 IDs to list and then print the labels. And now let's run this. And actually, let's also print the1678.561679.4399999999998 batch in this case. And let's have a look at how this looks like. So let's run this. And I get an1687.67999999999981687.68 error. So here I forgot to say outputs dot logits like we did before. So let's try it again. And1695.84000000000011695.8400000000001 this is only two results. So of course, here in our tokenize that we want to use these texts. So1703.60000000000011703.6000000000001 let's call this x train underscore Sherman, and then let's use x train underscore Sherman here.1713.921713.92 And let's run it again. Alright, and as we can see, we get the labels 11100 and two and this is1723.36000000000011723.3600000000001 equal to negative, negative, negative, then two times positive, and then neutral. So yeah, this1730.161730.16 is exactly what I told you, the first three sentences are rather negative than two positive1736.56000000000021736.5600000000002 ones. And this one is neutral. So yeah, now our German model works as well. And this is how we can1743.441743.44 use different models. So we simply search the model hub. And hopefully there is an already1751.761751.76 pre trained version for the task we want. And then we can just use this year as our model name,1758.01758.0 and then we are good to go. Or if there is not a already pre trained version, then we have to do1764.08000000000021764.0800000000002 this ourselves and find you in our own model. So I will show you how you do this in a moment. But1770.561770.56 now one more thing I want to mention. So I want to talk about this return tensors equals PT. So1780.081781.28 if we here we print the batch and here the input IDs, and then we see this is a tensor. So right1789.761789.76 now it's already in the pytorch format. So we could use tensor flow here, or we just omit this.1798.721798.72 And if we omit this, then we don't have this in the tensor format. So now it is just a Python list,1808.41808.4 I think. But then what you could do is you could convert this so we can say batch equals and then1815.841815.84 we convert this to a tensor by saying torch dot tensor. And then we give it the we call this batch1824.961824.96 and this is a dictionary. So we can say batch and then access the key input IDs like we see here.1836.081836.08 And now we created a actual tensor out of this. And then we don't have to unpack it like this1844.881844.88 year. So now we remove this. And then if we run it again, then this should work as well. And yeah,1852.01852.0 this work too. So we get the same result. And here we printed our batch. And now we see this1857.521857.52 is a tensor directly. So yeah, be careful here to specify what you want. So it's actually if you use1865.841865.84 pytorch, then it's just simpler to use this as a return argument. So return tensors equals PT. But1875.281875.28 if you don't use this, then you know what you can do otherwise. Alright, so now we know how we can1880.881880.88 use different models. So yeah, try this out for other models in your language and see if this1887.60000000000011887.6000000000001 works. And now let's have another look at how we can fine tune our own models. So this is very1895.21895.2 important. And I already prepared some code here and I will go over this very roughly. But there's1904.08000000000021904.08 also a very great documentation about this. So we can go to this documentation page here. And you1912.15999999999991912.1599999999999 can also open this in colab. So either with pytorch or tensorflow code. So this is really1917.91999999999981917.9199999999998 helpful. So I encourage you to check this out. But now let's go over this briefly. So basically,1925.67999999999981925.68 there are five steps you have to do. So in this example, it's for pytorch. So we have to prepare1934.41934.4 our data set, for example, loaded from a CSV file or whatever, then we have to load a pre trained1942.481942.48 tokenizer and then call it with our data set. So then we get the encodings or the token IDs,1949.441949.44 then we have to build a pytorch data set out of this with these encodings. So if you don't know1957.36000000000011957.3600000000001 what the pytorch data set is, then I will have a link for you here where I explain this, then we1963.12000000000011963.1200000000001 also load a pre trained model. And then we can either load a hugging face trainer and train it.1971.60000000000011971.6000000000001 So this abstracts away a lot of things, or we can just use a native or normal pytorch training1978.881978.88 pipeline like in our other pytorch code. So yeah, this is what we have to do. So let's go over this1986.161986.16 very quickly. So in this case, we define our base model name. So we want to start with a1994.161994.16 distilbert base uncased version. But in this case, for example, not the fine tuned one,2000.42000.4 so just this one, then step one, we prepare the data set. So we write a helper function to2006.962006.96 create texts and labels out of the actual text. And here we downloaded some data set and put it2017.442017.44 in our folder. So I already did this here. And yeah, this is available at this website. And this2023.762023.76 contains movie reviews. So we want to find you in our models on movie reviews for sentiment2029.922029.92 classification. So here we create training texts and the training labels with our helper function.2037.442037.44 And we also do a train test split to get validation texts and labels. And yeah, then as a2045.682045.68 next step, we create or we define a pytorch data set. So this inherits from pytorch data set. So2055.282055.28 data set. So torch utils data, we import data set. And then we define this here. So again,2062.962062.96 I have a tutorial where I explain how this works. But basically, it needs the encodings and the2070.24000000000022070.2400000000002 labels and it stores them in here. So yeah, this needs the encodings. So for the encodings, we need2078.02078.0 a tokenizer. So again, we use this from pre trained function with the model name. And in this case,2086.322086.32 since we know we use the distil bird one, we can use this class. So remember before we used a2094.562094.56 generic tokenizer, this auto tokenizer class. And here we use a more concrete one. So we use the2102.162102.16 distil bird tokenizer fast, then we apply it to training validation and test set and get the2109.922109.92 encodings. Then we put them in our data set and create the pytorch data set. And then we import2119.75999999999982119.7599999999998 a trainer and the training arguments. So this is in available in transformers library. And then we2127.83999999999972127.84 can set this up so we can create the arguments. So here, for example, we specify the number of2136.16000000000032136.1600000000003 training epochs, the output directory, the learning rate and other parameters we want. And then we2142.882142.88 create our model, again from a concrete model class. And then with this dot from pre trained2151.60000000000042151.6 function, and then we set up this trainer and give it the model and the training arguments,2158.882158.88 and then the training set and the validation set. And then we simply have to call trainer train,2165.362165.36 and this will do all the training for us. And afterwards, you can test it on your test data set,2170.962170.96 and then you have a fine tuned model. So yeah, this is basically all you need. And then I also2177.62177.6 want to show you that instead of using this trainer, if you want to do it manually and have2183.442183.44 even more flexibility, you can just use a normal pytorch training loop. So for this, we use a data2192.642192.64 loader and we need an optimization. So in this case, we use a optimizer from the transformers2199.75999999999982199.7599999999998 library. And then here we specify our device, then again, we create this model, we push it to2206.242206.24 the device and set it to training mode, then we create a data loader and the optimizer. And then2213.27999999999972213.2799999999997 we do the typical training loop. So we say for epoch in num epochs, and for batch in our training2220.47999999999962220.4799999999996 loader. And then we do the stuff we always do, we say optimizer zero grad, we also push it to the2227.522227.52 device if necessary, then we call the model and we calculate the loss with this. And in this case,2234.79999999999972234.8 this is already contained in the output. So we can just access the loss like this, then we call2242.08000000000042242.0800000000004 lost backward and optimizer step and iterate. And afterwards, we can set our model to evaluation2250.16000000000032250.1600000000003 mode again. And yeah, this is how we do it in native pytorch code. And yeah, so this is basically2256.42256.4 how we do a fine tuning and then can find in our own models. And then afterwards, you can also2262.82262.8 upload them to the hugging face model hub if you want. So yeah, I think that's pretty cool. And2269.442269.44 yeah, that's all that I wanted to show you for now. I think that's enough to get started with2274.56000000000042274.56 hugging face. And I hope you enjoyed this tutorial. And then I hope to see you in the next video. Bye.2293.12\"}"}