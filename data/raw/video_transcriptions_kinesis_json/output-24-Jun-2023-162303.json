{"data": "{\"value\":\"0.0 Hi there. Today we'll look at Direct Feedback Alignment Scales to Modern Deep Learning Tasks5.85.8 and Architectures by Giulia Loney, Jacopo Poli, Francois Boniface and Florian Crisakala.13.1413.14 So this paper on a high level, it replaces the backpropagation algorithm in deep learning19.40000000000000219.400000000000002 architectures with this algorithm called direct feedback alignment, which is more biologically24.6424.64 plausible. The algorithm has been around for a while, but it hasn't yet been shown to be30.4430.44 applicable to really modern big deep learning architectures and then perform on par with37.3237.32 backprop on modern deep learning tasks. This paper, as I understand it, is the first one43.12000000000000543.120000000000005 to demonstrate that it can do that. So this is very much an engineering paper, an applied48.848.8 paper, and we're going to mostly go into direct feedback alignment as such. And I don't think57.6457.64 we're going to go too much into what the actual empirical findings are, because they even63.95999999999999463.959999999999994 though they're impressive, and it's a good piece of engineering, I think they can be68.3268.32 summarized pretty much by it, it works not yet on par with backpropagation, but into74.4474.44 a promising direction. Alright, as always, if you like content like this, consider sharing81.5681.56 it out and leaving a like and tell me in the comments what you like, of course, subscribe87.9887.98 if you aren't yet that that is, you know, essential. Otherwise, how are we going to94.7599999999999994.75999999999999 hear from me in the future? Okay, let's dive in. They say despite being the workhorse of100.75999999999999100.76 deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer105.84105.84 updates thus preventing efficient parallelization of the training process. Furthermore, its111.4111.4 biological plausibility is being challenged. Alternative schemes have been devised yet117.60000000000001117.60000000000001 under the constraints of synaptic asymmetry. None have scaled to modern deep learning tasks123.28123.28 and architectures. Here we challenge this perspective and study the applicability of129.08129.08 direct feedback alignment to neural view synthesis, recommender systems, geometric learning and135.72000000000003135.72000000000003 natural language processing. In contrast with previous studies, limited to computer vision142.72000000000003142.72000000000003 tasks, our findings show that it successfully trains a large range of state of the art deep146.8146.8 learning architectures with performance close to fine tuned backpropagation. At variance153.26000000000002153.26000000000002 with common beliefs, our work supports that challenging tasks can be tackled in the absence158.8158.8 of weight transport. So there's a lot to unpack in this particular abstract right here. So166.58166.58 first of all, what's the problem with backpropagation? Backpropagation, they have they have two172.24172.24 quarrels right here. First of all, it's preventing efficient parallelization of the training178.0178.0 process. So what does that mean? So in backpropagation, I'm pretty sure you all know, it's basic backpropagation,186.88000000000002186.88 but you have an input to a neural network. And the neural network has a bunch of layers.190.56190.56 So the input will travel layer by layer. And at the end, you'll get some output and your196.0196.0 output y hat, let's call it here what the neural network thinks the let's say it's a201.44201.44 classifier thinks that the class of this particular x should be now in the data set, you have207.4207.4 your true label, and then you compare that to your output label, and you can compute214.18214.18 a loss function. Now, the whole question of the backpropagation algorithm is how do I220.4220.4 need to change my layers of the neural network in order to make the loss as small as possible.226.64000000000001226.64000000000001 And for that, you can use backpropagation, that means you can take that loss and you230.64000000000001230.64000000000001 can backpropagate it down the layers in order to update each layer individually. So the238.12238.12 first problem they have here with the backpropagation algorithm, and it's not, I mean, it's kind242.96242.96 of a secondary problem, but it is that is sequential. So in order to update this layer248.78248.78 right here, you need to have already back propagated to this layer, and then you need254.08254.08 to back propagate further to this and to this layer. So it's a sequential task, you need258.88258.88 to back propagate down the layers again, whereas what is more plausible, but what would be265.56265.56 more efficient if if we could somehow update all the layers in parallel, but this is a271.16271.16 minor quarrel. The bigger one is that backpropagation isn't biologically plausible. We know that278.0278.0 in real neurons, you have your your dendrites, your inputs, and you have your axon, and the284.92284.92 signal only travels in one direction, we don't know of a feedback mechanism in true neurons291.76000000000005291.76000000000005 in the brain that would allow for information sort of to flow in the opposite direction,297.58000000000004297.58 there is there is information flowing the opposite direction, but it's it's, I guess,302.03999999999996302.03999999999996 I think it's too slow. And it's so it's not really, it can't be, there's no analogous310.08310.08 way of backpropagation, there's no nothing in the brain that would take the role of the316.44316.44 backpropagation algorithm. Specifically, if each layer is characterized by a weight matrix,323.84323.84 right here, what backpropagation does is it uses the transpose of that weight matrix to332.73999999999995332.73999999999995 back propagate. So these, these arrows to the front, right here, they use the weight340.2340.2 matrices, and these arrows to the back, they use the transposes of the weight matrices.346.35999999999996346.35999999999996 So the transposes of the weight matrices, sort of relay the information of what needs353.05999999999995353.06 to change, that would be the loss, what needs to change to make the loss as small as possible,358.48358.48 they relay this information down to the other layers. And we don't know of any biological365.36365.36 analog, analogy to this mechanism right here, this transpose, it acts as sort of a layer371.32371.32 inverse. And that is called weight transport. So weight transport means that you can you379.24379.24 can do something like the transpose of the weights basically, to carry to bring information385.52385.52 from the next layer back to this layer. And in biology, we don't have this. And in direct392.74392.74 feedback alignment, we don't have this either. So direct feedback alignment, the next thing397.16397.16 here in this abstract is the algorithm that they are going to apply here, direct feedback404.22404.22 alignment, and we'll go into what it is, but it is more biologically plausible, in that409.6409.6 what it does is it takes the loss somehow, and it distributes it globally to all of these416.28000000000003416.28000000000003 layers like this. And it does so without requiring these transposes and also without requiring424.64000000000004424.64000000000004 these sequential steps. So both of their proposed problems here would be solved by this. They433.96000000000004433.96 say they say that in contrast with previous studies limited to computer vision tasks.442.44442.44 So what people have tried to do is they have tried to apply this DFA algorithm to computer450.84450.84 vision tasks. But in computer vision, most architectures are CNNs. And as I understand456.58456.58 it, as far as I understand it, DFA can only right now be applied to linear layers. So463.84463.84 something that is w x plus b, and then a non linearity. It cannot even though you can write471.15999999999997471.15999999999997 to CNN as like a linear layer with constraints. As I read this paper, I think to interpret478.32478.32 that you can only apply DFA to fully connected layers or things that look kind of like fully484.79999999999995484.8 connected layers. So what they're going to do in their experiments is they're going to488.56488.56 take these big architectures like transformers and replace parts of them with the parts that495.22495.22 act as fully connected layers with with DFA updates. So well, they're not going to replace501.24501.24 the layers, but they're going to replace the back propagation part of it with DFA updates.506.04506.04 It remains to say that they still use back propagation at some places where they can't511.44511.44 replace the updates with DFA. And that means where the layer isn't, you know, a fully connected517.84517.84 layer or I guess it's too big, they somehow have to make it work. So often they will not522.16522.16 update, for example, the embedding layers, and things like this. Okay, so what they're528.64528.64 saying is they go away from computer vision tasks. Because if you go to computer vision,533.88533.88 and CNNs rule that world, right, you can only do for feed forward layers, fully connected540.76540.76 layers, you're going to lose already. And so, yeah, it's kind of an unfair fight in548.24548.24 that in that sense. But even in absence of that, they say we apply this to neural view554.4554.4 synthesis recommender systems, geometric learning and natural language processing. So these560.22560.22 are quite diverse tasks. And they're going to be quite diverse architectures that they're564.6564.6 applying it to, for example, in geometric learning, I believe they do graph neural networks.569.72569.72 And there, there, they replace the, the usually in graph neural networks, there are fully576.32576.32 connected layers that connect the two the vertices and the edges together and compute582.28582.28 properties of them. So that's a pretty good point for using DFA, right? Because what you're589.0589.0 looking for is state of the art tasks and architectures that still employ fully connected594.0400000000001594.04 layers because there your algorithm can shine. Okay, so that's it. And they're basically602.92602.92 going to show that this is performance is close to fine tuned back propagation. Right.608.1999999999999608.1999999999999 So what is DFA? What is this direct feedback alignment? And for that, I actually want to616.12616.12 jump papers right here and go to this other paper that describes DFA in a bit in a bit,623.48623.48 not more detail, but in a graphic fashion. So this paper right here, direct feedback628.5600000000001628.5600000000001 alignment provides learning in deep neural networks by RL Nocklund, sorry, Nocklund shows637.96637.96 some theoretical properties about DFA. Now, I don't want to go into the theory right here642.44642.44 or in the math, but I mainly like this paper for this particular graphic right here. So649.5600000000001649.56 in the back propagation algorithm, as you can see, you forward propagate using these653.92653.92 weight matrices, and then you back propagate using the transposes of the weight matrices.660.16660.16 Now one step after that is this thing right here. It's called feedback alignment. It's665.78665.78 not the same thing as direct feedback alignment. In feedback alignment, you simply say, Well,671.68671.68 I won't back prop using these transposes because I can't because that's not biologically possible.677.0799999999999677.08 What I'll do is I'll use other matrices. And these other matrices are going to be random684.9200000000001684.9200000000001 matrices. And by random matrices, we really mean a matrix that is of you know, the correct690.5600000000001690.5600000000001 shape, the same shape as this W transpose. But each entry is going to be sampled from697.98697.98 a like a random Gaussian, right? Now, I don't mean like the distribution of Gaussians, but704.96704.96 you fix this matrix once at the beginning of training by sampling from Gaussian, and711.64711.64 then you leave it there. And that's going to be the matrix that you use for relaying716.48716.48 the signal back through the layers. Now, you might protest and say, wait, that's not going722.48722.48 to work. Because specifically, this thing right here, it you know that you need to know728.52728.52 the weights here to know what you need to change in the lower layers, you need to somehow733.84733.84 have that information in there, how are you going to know what to change? And that's a738.88738.88 valid question. And I will give my opinion of why this works. Okay. In a second in two746.4746.4 seconds. First, this is feedback alignment. So simply use random matrices to back propagate,753.5600000000001753.5600000000001 so to say, and then you have direct feedback alignment and direct feedback alignment goes758.52758.52 a step further. Because in feedback alignment, you still do this in a sequential manner.763.84763.84 Direct feedback alignment simply takes whatever the top change should be the change to the770.76770.76 top layer. So how do I need to change the top layer, and it back propagates that in776.8776.8 a this global fashion to all the layers directly using random matrices, okay. And then this783.96783.96 IFA we're not going to look at today, because that's not relevant for this other paper.789.52789.52 But I hope you can sort of see the overview here. So let's go back. Scroll, scroll, scroll,798.9200000000001798.9200000000001 scroll, scroll, scroll, scroll. Okay, so here is the mathematical formulation of all of805.24805.24 this. And it pays to look at it to understand what's going on. So they characterize a neural810.76810.76 network right here as having n layers. Each neural network is the following each neural815.92815.92 each layer takes whatever is the output of the last layer, multiplies it by a weight822.24822.24 matrix. And that's going to be your a quantity, you put a through a non linearity to obtain830.68830.68 the next layers input. Okay, so the H is the output of this layer, and the input of the836.96836.96 next layer. At the very end, your last output is going to be your estimation of the labels.845.4000000000001845.4000000000001 So your last non linearity is probably going to be something like a, a softmax or something852.64852.64 like this. Okay. So how can we how can we have this as a concept in our heads? If you861.9200000000001861.92 have the neural network right here, what you want to do is you want to forward prop always867.64867.64 using your weight matrix w, and then your non linearity of that particular layer. And875.92875.92 then the last in the last layer, you get your y hat, as we saw before. Now, the question883.0883.0 is, how can we adjust? How can we adjust this w right here to make y hat more into the direction892.44892.44 of y. And here, it's in here, it's useful to think of the last layer as a vector output.900.32900.32 Like usually we think of the loss function. But in all of these algorithms, they always907.32907.32 start with the derivative of the loss function with respect to the last layer output, so913.5600000000001913.5600000000001 a y, and a y is here right before the non linearity. If you remember, this was f of922.2800000000001922.2800000000001 a y. And this here, I guess, is the softmax. So if this is a classifier, the a y here,930.22930.22 those are the lockets. And that's the output of your last layer. So it, instead of having936.7600000000001936.76 y and y hat, right, sorry, y hat right here, it pays to maybe think of the output as a948.48948.48 vector, and the desired output as another vector. And the desired output is of course955.3199999999999955.3199999999999 going to be one hot vector in the case of in the case of a classification. But it, you963.04963.04 know, if you think of it like this, then you will recognize, okay, I need to change if970.76970.76 this is my estimated output, and I want to achieve this output, I need to change it into976.5999999999999976.5999999999999 this direction, right to get more into the same direction as the output I want. The entire983.38983.38 question now becomes how do I tell the lower layers about this change right here? This990.0990.0 is the change that I want to make in the lower layers? How do I get the lower layers such997.12997.12 that they provide me with that signal with with the green signal instead of the red signal?1004.361004.36 So I need to propagate this blue difference. In the back propagation algorithm, you can1010.41010.4 simply ask the system, right? So we've built entire frameworks on being able to back propagate1017.481017.48 TensorFlow, pytorch, jacks, whatever. Because with back propagation, we can simply ask the1024.561024.56 system this question. So here is how should I change the weights of my layer to make the1031.081031.08 loss smaller? You can just ask that you can say, what's the gradient of the loss with1036.841036.84 respect to the to my weights, and the negative sign here is because you want to make the1044.01044.0 loss smaller. Okay, and that is going to be a straightforward calculation. How does that1050.761050.76 calculation go? It's going to involve this right here is the last layer's output. This1061.681061.68 right here, as you can see, over here is going to be this is going to be whatever comes back1071.41071.4 from the back propagation. So in back propagation, you always have to think of, if you want to1076.641076.64 update these weights, you need two quantities, you need whatever comes from the bottom or1081.761081.76 came from the bottom during the forward pass, and whatever comes from the top during the1087.481087.48 backward pass. And this quantity here is going to be the one that came from the top. And1097.84000000000011097.84 it's basically how you need to change the next layer in order to make the loss happier.1103.39999999999991103.3999999999999 And by using this right here, you pull it back to this layer. So how do I need to change1109.081109.08 this layer? And here you see that dreaded transpose of that weight matrix. This is what1114.67999999999981114.6799999999998 we can't do in biology. But this is what back propagation does. So it pulls back how you1120.65999999999991120.6599999999999 need to change the next layer, it pulls it back to this layer. So this quantity right1126.43999999999981126.44 here is basically how do I need to change the output of this particular layer in order1133.32000000000021133.3200000000002 to make the loss happier. And then you multiply it by the signal that comes from the bottom.1140.32000000000021140.3200000000002 And that will give you how you need to change your weights. Okay. So the green part is how1146.58000000000021146.5800000000002 does the output of the layer need to change and the multiplied by the blue part, it's1151.84000000000011151.84 how do the weights need to change. And of course, the non linearity is in there as well.1156.81156.8 But let's, let's just leave the non linearity away because it's really not important for1162.281162.28 this particular thing. So this is what backprop does. What does DFA do? DFA here, again asks,1172.67999999999981172.6799999999998 how should I change the weights of layer i and DFA says, Well, first, you need to compute1180.041180.04 this thing right here. This is you see the derivative of the loss with respect to a y1186.361186.36 now a y is the output of the last layer. These are in in our case, for example, your log1192.841192.84 it's okay, note that this is still a gradient. So it's not like we can't differentiate anymore,1199.721199.72 we simply can't do back propagation from layer to layer. Okay, so this is the quantity. How1206.521206.52 do we need to change the last layer's output? And we're going to take that and simply feed1213.841213.84 it through this random matrix, and then multiply again, let's leave this away, multiply it1220.721220.72 by the by this thing right here. So if I get my colors correct, like this, again, you have1227.941227.94 your neural network, you want to update these weights, the green is what comes from the1233.63999999999991233.64 top. Now it doesn't come from the next layer, but the green actually comes from all the1238.80000000000021238.8000000000002 way at the end. Sorry, you can't see that I still have to get used to that new frame1245.961245.96 of view. So the green comes all the way from the end, and the blue comes from down here.1253.441253.44 Okay, so this is weird, right? Because especially because this is just modulated by a random1261.28000000000021261.28 matrix. So how can this possibly work? That's the question. And I, you know, I had some1269.481269.48 thoughts, but I haven't read too much about it. So I might be completely wrong. Or this1273.561273.56 might be completely known in the community. I have no idea. I'll just give my opinion1279.441279.44 right here. So first of all, you have to see if to compare this to backprop. So what's1286.721286.72 actually changing is this green part right here, right? We agree that this is the thing1292.41292.4 that's changing. And what do we say, does the green part mean, the green part basically1297.81297.8 tells you how do you how should the output of this layer change? Okay, by adjusting the1306.681306.68 weights in the direction of the thing on the right side of the equality sign, you're going1310.521310.52 to change the output of the layer into the direction of that green part. Now, in backpropagation,1317.41317.4 the green part basically tells you how should the output of this layer change in order to1323.481323.48 make the losses happy as possible. Now, we don't have that anymore. Here, we simply change1330.521330.52 the output of the layer into the into the direction of a random transformation of the1338.921338.92 of the change we would like to have in the output. Now, okay, that's the, the first thing1344.60000000000011344.6000000000001 is we understand what's different. And we understand what the green quantity means.1349.60000000000011349.6000000000001 Green quantity means how should the output of our layer change? Okay, second thing. If1356.641356.64 you look at the last layer of a neural network that that logits layer, right, what does it1362.521362.52 actually do? Let's say we have that's a three dimensional last layer, which means you have1367.681367.68 three classes, right? If your last layer is three dimensional, you have three classes,1373.84000000000011373.8400000000001 each axis represents one class because you encode the classes as one hot vectors. So1379.881379.88 this might be C, the class label equals zero, this might be C equals one, this might be1386.681386.68 C equals two. If you have something that you forward propagate through your neural network,1394.12000000000011394.12 and let's say it comes out to be like this, what would you classify that as? Now you classify1401.19999999999981401.1999999999998 that as the whatever class has the the biggest inner product with that vector, which would1410.71999999999981410.7199999999998 be the C equals zero class right here. And what is this quantity going to be? How should1418.081418.08 you update this output in order to make the loss happier? Now that depends on your true1424.081424.08 label. But let's say your true label is actually the zero label. Now, what you want to do is1430.361430.36 you want to update that thing into the direction here, right, such that it is more aligned1436.43999999999981436.4399999999998 with the axis. So what happens if you pull that back through a random matrix? Now, the1444.19999999999981444.2 thing you have to know about random matrices like this is that they do approximately preserve1449.36000000000011449.3600000000001 distances and angles. So technically, if you pull this back, what you're going to induce1457.84000000000011457.8400000000001 is another coordinate system in that other space. Now, this can be a higher or lower1462.84000000000011462.8400000000001 dimensional space, I frankly, I don't care. But what you're going to induce is a coordinate1470.84000000000011470.84 system. And what do you pull through that B matrix? So this is the bi matrix, you fix1478.481478.48 it, right? This is really important, you fix it at the beginning of training, it's always1482.15999999999991482.1599999999999 the same, it preserves distances and angles approximately, you pull back that quantity,1489.61489.6 which is the Okay, my colors are all screwed, which is the green arrow over here, you pull1495.01495.0 back this green arrow here. So what does it mean? What so the output right here, the output1505.361505.36 vector that came from the lower layers, right? That's the forward propagated that through1510.241510.24 your network. So maybe in this layer, it actually pointed here, we don't know. But let's say1515.721515.72 it pointed here. If we pull back the green thing, it might point here. Okay, now, this1524.961524.96 is, since it's a random matrix, we don't know, we know that the angle is approximately preserved,1530.01530.0 okay, but you know, the end the lengths are approximately preserved with relative to each1534.561534.56 other. But it doesn't really tell you too much. So why is this useful? And to see why1544.521544.52 it's useful, you need to consider other inputs, we don't just input this one vector, we input1552.21552.2 a whole bunch of data. Now let's consider two other vectors. So first, I want to consider1558.481558.48 this this blue vector right here. Now, the blue vector is also going to have a label1565.421565.42 of zero. So what does the blue vectors update look like the blue vector is going to be pulled1572.281572.28 into this direction. And I also want to consider this red vector right here, the red vector1579.441579.44 is of class one. So what does the red vectors update going to look like, like this, right?1586.881586.88 And if I consider now the red and the blue vector in this space, right, let's just draw1594.21594.2 them at random, like so, okay, what I do know, actually, that's that's for consistent. Let's1603.041603.04 draw the blue somewhere here, and the red somewhere here. What I do know is that the1609.721609.72 angles and distances are preserved. So what is the green thing going to look like the1614.441614.44 update for the blue vector is going to be something like this. And the update for the1619.01619.0 red vector is going to maybe be something like this, you know, away from from those.1626.741626.74 So what is happening in that lower space, you'll notice that the two vectors that are1632.081632.08 supposed to be in the same class, this and this, they are going to be pulled together.1639.361639.36 Now the direction they're pulled in, that's determined by this random matrix. But we know1645.91999999999981645.9199999999998 they're going to be pulled together because they are pulled together in this space in1650.87999999999991650.8799999999999 the final space. Okay, and they're going to be pulled apart from the red vector, okay,1660.281660.28 because that red vector is going to be pulled towards a different class in the in the last1665.161665.16 space. And since the distances and angles are approximately preserved, it's going to1670.041670.04 be pulled away from these in this space. So what this induces, in my opinion, is some1678.11678.1 sort of it induces this coordinate system where if you make the last layer axis aligned1687.87999999999991687.88 because you want to classify it, it kind of clusters things that belong in the same class1694.21694.2 in these previous weight spaces, right? And because and if you do this layer by layer,1701.56000000000021701.5600000000002 so if you do this in layer k, and then you make the job easier for any layer k plus one1710.06000000000021710.0600000000002 that's in between here, right? Because they now the things in the same class are already1714.881714.88 together pretty okay. Now you map it through a weight and the non linearity they might,1719.921719.92 you know, intertwine a bit again, but there's there more together than they would be otherwise.1726.56000000000021726.5600000000002 So you make the job for the next layer easier, which means that the next layer can also can1733.441733.44 even better cluster things. And what you'll end up with in this last layer is the is a1741.681741.68 basically a class or next to last layer is basically a clustering, where everything that's1747.161747.16 supposed to be in the same class is together and far apart from each other. And since the1753.161753.16 last layer is the classification layer, it's going to have a really easy job separating1759.721759.72 those classes and performing good classification. So that's what I think is happening in this1766.481766.48 algorithm. So even though the layers don't know how to change to help the last layer,1773.281773.28 by the fact that these random matrices induce a clustering together, you know, by back propagating1780.721780.72 these updates here, it helps the last layer, make it makes its job really easy. And, you1788.961788.96 know, that's all the classifier needs. And I want to I want to show again, this is my1794.081794.08 opinion, this is not anything of value. It's just my hypothesis of why something like this1801.43999999999981801.4399999999998 could work. I want to show you in this paper that I've shown you before right here, they1805.961805.96 do actually do these experiments with DFA. And they show that you can see top row shows1814.041814.04 feature obtained with back propagation, bottom row shows features obtained with DFA. I think1820.121820.12 these are input and features. I'm not sure where exactly they are in the network. But1826.87999999999991826.8799999999999 you can see that this clustering clearly emerges. So oh, yeah, here, from left to right input1834.63999999999991834.6399999999999 images, first hidden layer, second hidden layer, third hidden layer. So you can see1838.87999999999991838.8799999999999 that the clustering from layer to layer in backprop, and also in DFA is better and better.1846.521846.52 So the reason why backprop is good, maybe it's just that because it also really induces1853.121853.12 clusterings like this, I don't know, maybe backprop does even does something on top of1857.681857.68 that. Because I mean, backprop has all the properties of this and more, right. But still,1864.63999999999991864.6399999999999 this this is congruent with my hypothesis of what's happening. So what do they do with1872.321872.32 it? They take this algorithm, and they apply it to these architectures. Now, let's, for1880.361880.36 example, look at one of them, this neural view synthesis with neural radiance fields.1887.521887.52 So neural radiance fields is a type of model to do this task of where you get a bunch of1894.15999999999991894.1599999999999 views of an object in 3d, or, you know, a bunch of views around an object, and you're1900.081900.08 supposed to render a new view. And you can see that the DFA parameter or the DFA updated1908.241908.24 nerve neural radiance field model is pretty close to the backpropagation updated one,1917.39999999999991917.3999999999999 you can see it's a bit more blurry, but it it works, right. And I think the this paper1922.63999999999991922.6399999999999 is really trying to show that look, this works, it doesn't work, you know, extremely well,1929.19999999999981929.2 but it works. And it works on a level that hasn't been seen before. So here, if you consider1936.08000000000021936.0800000000002 these results higher is better. On the synthetic data set here, even you see that if you have1941.961941.96 the same model with backprop, it performs better than with DFA. But the DFA for that1948.241948.24 model performs better than these other baseline models that have themselves been trained with1954.36000000000011954.36 backpropagation. So it's definitely in the direction of being competitive. And that's1964.081964.08 the same thing they show with all of these experiments. So they apply this to graph networks,1969.63999999999991969.6399999999999 they apply this to transformers. And as I said, it's not there yet, you see that. So1975.261975.26 in the transformers, they have these settings where in macro, they just use it DFA for the1979.71999999999981979.72 individual blocks and micro, they use it for each layer, and already told you that you1985.081985.08 still in the attention mechanism, you still have to use backprop within the attention1990.01990.0 mechanism. But it is much more of a plausible algorithm than the backpropagation through1997.841997.84 the entire network. And they show that if they appropriately tweak the hyper parameters,2004.522004.52 they do get into the direction of something that's performant, at least with this macro2010.02010.0 strategy. Now, this is nowhere close to this is nowhere close to what the to what the backpropagation2018.082018.08 algorithm achieves. But it's sort of it's sort of an indication that if the community2024.482024.48 could work as much on this as it has worked on backpropagation, then probably will make2031.842031.84 a lot of like we could we could push this to a place where it does perform on par with2038.02038.0 backprop or very close to it. So I do invite you to go and look at the experiments, they2044.082044.08 have a lot of lot of details on how they did it. And exactly how you have to change the2051.842051.84 architectures to make DFA work and the hyper parameters and so on. So that's really cool.2057.982057.98 And they have some more outputs right here of the view synthesis and so on. Yeah, if2064.22064.2 you are interested in that thing, I again, I don't want to disrespect it. It's just I2068.922068.92 don't think there is much point in me going over it. It's the results are always sort2073.82073.8 of the same that DFA it. It's not there yet, but it's a good direction. Yeah, I hope this2081.122081.12 was informative. Let me know if you disagree about my assessment of DFA. I could be completely2087.842087.84 wrong or, you know, I guess, yeah, or this could be like well known to people already.2094.562094.56 So yeah, see you next time.2118.12\"}"}